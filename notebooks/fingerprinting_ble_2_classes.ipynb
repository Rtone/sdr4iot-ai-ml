{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import csv\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, metrics, optimizers, utils, initializers\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fingerprinting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are doing fingerprinting on BLE packets, meaning that we are trying to identify an emitter based on the traffic it emits. We are using data from Scenario 5, in which the emitter and the server are still, using 3 nRF52 dev kits used as BLE advertising devices, 4 USRPs used as receivers. We are using raw IQ data as input. We are using data solely from Scene 36 and Scene 37 in which only one emitter is advertising. In Scene 36 it is a nRF52 apuP22, and in Scene 37 it is nRF52 apuQ2. This means that we only have two classes. We are using data solely from one server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=('../data/processed/scenario5_scene36.csv','../data/processed/scenario5_scene37.csv')\n",
    "\n",
    "idata = pd.concat([pd.read_csv(f) for f in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Time  Len Packet  Central Frequency     X   Y  \\\n",
      "1426075  2020-11-12T16:07:36.897782Z        1520         2480000000  3009  94   \n",
      "1426076  2020-11-12T16:07:36.897782Z        1520         2480000000  3009  94   \n",
      "1426077  2020-11-12T16:07:36.897782Z        1520         2480000000  3009  94   \n",
      "1426078  2020-11-12T16:07:36.897782Z        1520         2480000000  3009  94   \n",
      "1426079  2020-11-12T16:07:36.897782Z        1520         2480000000  3009  94   \n",
      "\n",
      "                                      IQ      real        im  Server_id  \\\n",
      "1426075     (6.103679e-05+6.103679e-05j)  0.000061  0.000061         11   \n",
      "1426076                   0.00021362878j  0.000000  0.000214         11   \n",
      "1426077  (-0.00012207359+0.00021362878j) -0.000122  0.000214         11   \n",
      "1426078   (-0.00030518396-6.103679e-05j) -0.000305 -0.000061         11   \n",
      "1426079  (-0.00012207359+0.00021362878j) -0.000122  0.000214         11   \n",
      "\n",
      "         Robot_node  Scenario  Scene  \n",
      "1426075         NaN         5     37  \n",
      "1426076         NaN         5     37  \n",
      "1426077         NaN         5     37  \n",
      "1426078         NaN         5     37  \n",
      "1426079         NaN         5     37  \n"
     ]
    }
   ],
   "source": [
    "idata_five=idata[idata['Scenario']==5] \n",
    "print(idata_five.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3079000\n"
     ]
    }
   ],
   "source": [
    "idata_sc=idata_five[(idata_five['Scene']==36) | (idata_five['Scene']==37)]\n",
    "print(len(idata_sc.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36    1652920\n",
      "37    1426080\n",
      "Name: Scene, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(idata_sc['Scene'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1520    2932080\n",
      "1880      71440\n",
      "1360      53040\n",
      "1320      17160\n",
      "880        5280\n",
      "Name: Len Packet, dtype: int64\n",
      "9     877480\n",
      "11    856880\n",
      "15    708720\n",
      "12    635920\n",
      "Name: Server_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(idata_sc['Len Packet'].value_counts())\n",
    "\n",
    "print(idata_sc['Server_id'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are selecting the most represented packet length, in order to have a constant input length for the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkt_len=1520\n",
    "#using only one pkt length for the time being\n",
    "idata_red=idata_sc[idata_sc['Len Packet']==pkt_len]\n",
    "idata_red=idata_red[idata_red['Server_id']==9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are creating input for the CNN: X is composed of 1520 IQ samples, and Y is the scene number, meaning the emitter used in each of these scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating input for CNN: IQ values for a whole pkt     output: robot_node\n",
    "X=list()\n",
    "Y=list()\n",
    "\n",
    "i=0\n",
    "while i<len(idata_red.index):\n",
    "    data=idata_red.iloc[i:i+pkt_len]\n",
    "    if len(data['Time'].unique())==1:\n",
    "        data=np.array(data[['real','im']], dtype='float64')\n",
    "        x=data.reshape(pkt_len,2,1)\n",
    "        X.append(x)\n",
    "        Y.append(int(idata_red.iloc[i]['Scene']))  #in each scene a different emitter is used ~ robot_node in that case\n",
    "    else:\n",
    "        print('Missing!!')\n",
    "    i=i+pkt_len\n",
    "\n",
    "X=np.array(X)\n",
    "Y=np.array(Y, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 541 échantillons\n"
     ]
    }
   ],
   "source": [
    "print(\"Il y a \"+str(len(Y))+\" échantillons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36 37]\n",
      "[299 242]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(Y, return_counts=True)\n",
    "print(unique_elements)\n",
    "print(counts_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are manually balancing the classes to make classification easier and avoid bias due to class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242\n"
     ]
    }
   ],
   "source": [
    "#balance classes\n",
    "min_samples=min(counts_elements)\n",
    "print(min_samples)\n",
    "\n",
    "thirty_six_index=np.where(Y == 36)\n",
    "thirty_six_index=thirty_six_index[0][:min_samples]\n",
    "X_thirty_six=X[thirty_six_index]\n",
    "Y_thirty_six=Y[thirty_six_index]\n",
    "\n",
    "thirty_seven_index=np.where(Y == 37)\n",
    "thirty_seven_index=thirty_seven_index[0][:min_samples]\n",
    "X_thirty_seven=X[thirty_seven_index]\n",
    "Y_thirty_seven=Y[thirty_seven_index]\n",
    "\n",
    "X=np.concatenate((X_thirty_six, X_thirty_seven))\n",
    "Y=np.concatenate((Y_thirty_six, Y_thirty_seven))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36 37]\n",
      "[242 242]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(Y, return_counts=True)\n",
    "print(unique_elements)\n",
    "print(counts_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First try : CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  4\n",
      "{0: 1.0, 1: 1.0}\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential_2/conv2d_2/Conv2D (defined at <ipython-input-31-1311771fe8ea>:61) ]] [Op:__inference_train_function_2604]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-1311771fe8ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m history=model.fit(X_train, y_train, batch_size=batch_size, epochs=steps, verbose=1, validation_split=0.2,\n\u001b[0;32m---> 61\u001b[0;31m                  class_weight=class_weights)\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential_2/conv2d_2/Conv2D (defined at <ipython-input-31-1311771fe8ea>:61) ]] [Op:__inference_train_function_2604]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#using Thibaud network\n",
    "from tensorflow.keras.constraints import min_max_norm\n",
    "from tensorflow.keras import layers, models, metrics, optimizers, utils\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "batch_size=100\n",
    "steps=500\n",
    "\n",
    "Y_change=list()\n",
    "\n",
    "#36:0, 37:1\n",
    "for i in Y:\n",
    "    if i==36:\n",
    "        Y_change.append(0)\n",
    "    elif i==37:\n",
    "        Y_change.append(1)\n",
    "\n",
    "Y_change=np.array(Y_change)\n",
    "\n",
    "unique_elements, counts_elements = np.unique(Y_change, return_counts=True)\n",
    "nb_class=max(unique_elements)+1\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "Y_cat=utils.to_categorical(Y_change, num_classes=nb_class)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_cat, test_size=0.2, random_state=54)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                np.unique(Y_change),\n",
    "                                                 Y_change)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "print(class_weights)\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (6,2), padding='valid',activation='relu', kernel_constraint=min_max_norm(min_value=1, max_value=1.0),input_shape=(pkt_len,2, 1)))\n",
    "model.add(layers.MaxPooling2D((2,1), strides=2))\n",
    "#model.add(layers.Conv2D(32, (8,1), padding='valid',activation='relu',kernel_constraint=min_max_norm(min_value=1.0, max_value=1.0)))\n",
    "#model.add(layers.MaxPooling2D((4,1), strides=4))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dropout(0.7))\n",
    "model.add(layers.Dense(nb_class, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "history=model.fit(X_train, y_train, batch_size=batch_size, epochs=steps, verbose=1, validation_split=0.2,\n",
    "                 class_weight=class_weights)\n",
    "\n",
    "i = np.arange(steps)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(i, history.history['accuracy'], label='accuracy')\n",
    "plt.plot(i, history.history['val_accuracy'], label='val_accuaracy')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions= model.predict(X_test)\n",
    "\n",
    "predictions = [np.argmax(y, axis=0, out=None) for y in predictions]\n",
    "y_test_cat = [np.argmax(y, axis=0, out=None) for y in y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_score, recall_score, f1_score, accuracy_score, average_precision_score\n",
    "\n",
    "present_class_list=np.unique(y_test_cat)\n",
    "print(present_class_list)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat=confusion_matrix(y_test_cat, predictions, labels=present_class_list)\n",
    "print(\"Conf mat\")\n",
    "print(conf_mat)\n",
    "\n",
    "#F1-score\n",
    "print(\"f1_score for each class\")\n",
    "f1_each=f1_score(y_test_cat, predictions, average=None, labels=present_class_list)\n",
    "print(f1_each)\n",
    "\n",
    "#F1-score\n",
    "print(\"Global f1_score\")\n",
    "print(f1_score(y_test_cat, predictions, average='weighted'))\n",
    "\n",
    "#accyracy\n",
    "print(\"Global accuracy\")\n",
    "print(accuracy_score(y_test_cat,predictions))\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "cmap = sns.diverging_palette(200, 10, as_cmap=True)\n",
    "sns.heatmap(conf_mat, cmap=cmap, xticklabels=present_class_list, yticklabels=present_class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Accuracy and F1-score are good for a first try, but we need to tune the model to make it more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Keras Hypertuner on this network. This tool allows us to select a range of values for a given hyperparameter, and to try various combinations of these hyperparameters. After a given number of execution, the best combination according to a given metric is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, optimizers\n",
    "from kerastuner.tuners import RandomSearch, Hyperband\n",
    "\n",
    "input_shape=(1520,2,1)\n",
    "\n",
    "def build_model(h):\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(filters=h.Int('filters_1', 2, 200,10, default=32), kernel_size=(6, 2), padding='valid',activation=h.Choice(\n",
    "                'activation_1',\n",
    "                values=['relu', 'tanh', 'sigmoid'],\n",
    "                default='relu'), kernel_constraint=min_max_norm(min_value=1.0, max_value=1.0),input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 1), strides=2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(units=h.Int('units_dense', 2, 200,10, default=100), activation=h.Choice(\n",
    "                'activation_2',\n",
    "                values=['relu', 'tanh', 'sigmoid'],\n",
    "                default='relu')))\n",
    "    model.add(layers.Dropout(\n",
    "            h.Float(\n",
    "                'dropout',\n",
    "                min_value=0.5,\n",
    "                max_value=0.9,\n",
    "                default=0.7,\n",
    "                step=0.1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(layers.Dense(nb_class, activation='softmax'))\n",
    "\n",
    "    hp_learning_rate = h.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4])\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "{0: 1.0, 1: 1.0}\n"
     ]
    }
   ],
   "source": [
    "#trying first simple CNN to test if our data are ok\n",
    "#using Thibaud network\n",
    "from tensorflow.keras.constraints import min_max_norm\n",
    "from tensorflow.keras import layers, models, metrics, optimizers, utils\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "batch_size=100\n",
    "steps=300\n",
    "\n",
    "Y_change=list()\n",
    "\n",
    "#36:0, 37:1\n",
    "for i in Y:\n",
    "    if i==36:\n",
    "        Y_change.append(0)\n",
    "    elif i==37:\n",
    "        Y_change.append(1)\n",
    "\n",
    "Y_change=np.array(Y_change)\n",
    "\n",
    "unique_elements, counts_elements = np.unique(Y_change, return_counts=True)\n",
    "nb_class=max(unique_elements)+1\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "Y_cat=utils.to_categorical(Y_change, num_classes=nb_class)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_cat, test_size=0.2, random_state=54)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                np.unique(Y_change),\n",
    "                                                 Y_change)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 00m 51s]\n",
      "val_accuracy: 0.5256410241127014\n",
      "\n",
      "Best val_accuracy So Far: 0.9871794879436493\n",
      "Total elapsed time: 00h 17m 24s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in ./untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 52\n",
      "activation_1: tanh\n",
      "units_dense: 162\n",
      "activation_2: relu\n",
      "dropout: 0.7999999999999999\n",
      "learning_rate: 0.001\n",
      "Score: 0.9871794879436493\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 62\n",
      "activation_1: tanh\n",
      "units_dense: 182\n",
      "activation_2: tanh\n",
      "dropout: 0.7\n",
      "learning_rate: 0.001\n",
      "Score: 0.9743589758872986\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 162\n",
      "activation_1: relu\n",
      "units_dense: 52\n",
      "activation_2: sigmoid\n",
      "dropout: 0.6\n",
      "learning_rate: 0.001\n",
      "Score: 0.9743589758872986\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 12\n",
      "activation_1: tanh\n",
      "units_dense: 172\n",
      "activation_2: tanh\n",
      "dropout: 0.6\n",
      "learning_rate: 0.001\n",
      "Score: 0.9743589460849762\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 112\n",
      "activation_1: relu\n",
      "units_dense: 192\n",
      "activation_2: sigmoid\n",
      "dropout: 0.7\n",
      "learning_rate: 0.001\n",
      "Score: 0.9615384638309479\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 72\n",
      "activation_1: relu\n",
      "units_dense: 72\n",
      "activation_2: tanh\n",
      "dropout: 0.6\n",
      "learning_rate: 0.0001\n",
      "Score: 0.9551281929016113\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 42\n",
      "activation_1: relu\n",
      "units_dense: 82\n",
      "activation_2: tanh\n",
      "dropout: 0.7\n",
      "learning_rate: 0.0001\n",
      "Score: 0.9487179517745972\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 132\n",
      "activation_1: relu\n",
      "units_dense: 82\n",
      "activation_2: tanh\n",
      "dropout: 0.8999999999999999\n",
      "learning_rate: 0.001\n",
      "Score: 0.9423076808452606\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 112\n",
      "activation_1: relu\n",
      "units_dense: 102\n",
      "activation_2: relu\n",
      "dropout: 0.8999999999999999\n",
      "learning_rate: 0.0001\n",
      "Score: 0.9423076808452606\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 32\n",
      "activation_1: tanh\n",
      "units_dense: 182\n",
      "activation_2: relu\n",
      "dropout: 0.7\n",
      "learning_rate: 0.01\n",
      "Score: 0.5512820482254028\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "tuner_rs = RandomSearch(\n",
    "            build_model,\n",
    "    objective='val_accuracy',\n",
    "            seed=42,\n",
    "            max_trials=20,\n",
    "            executions_per_trial=2,\n",
    "            overwrite=True)\n",
    "\n",
    "tuner_rs.search_space_summary()\n",
    "\n",
    "\n",
    "\n",
    "tuner_rs.search(X_train, y_train, epochs=300, validation_split=0.2, verbose=1, class_weight=class_weights)\n",
    "\n",
    "tuner_rs.results_summary()\n",
    "\n",
    "best_model = tuner_rs.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test optimal CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  4\n",
      "{0: 1.0, 1: 1.0}\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential_1/conv2d_1/Conv2D (defined at <ipython-input-15-778c27fcf08f>:58) ]] [Op:__inference_train_function_1737]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-778c27fcf08f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m history=model.fit(X_train, y_train, batch_size=batch_size, epochs=steps, verbose=1, validation_split=0.2,\n\u001b[0;32m---> 58\u001b[0;31m                  class_weight=class_weights)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential_1/conv2d_1/Conv2D (defined at <ipython-input-15-778c27fcf08f>:58) ]] [Op:__inference_train_function_1737]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#using Thibaud network\n",
    "from tensorflow.keras.constraints import min_max_norm\n",
    "from tensorflow.keras import layers, models, metrics, optimizers, utils\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "\n",
    "batch_size=100\n",
    "steps=300\n",
    "\n",
    "Y_change=list()\n",
    "\n",
    "#36:0, 37:1\n",
    "for i in Y:\n",
    "    if i==36:\n",
    "        Y_change.append(0)\n",
    "    elif i==37:\n",
    "        Y_change.append(1)\n",
    "\n",
    "Y_change=np.array(Y_change)\n",
    "\n",
    "unique_elements, counts_elements = np.unique(Y_change, return_counts=True)\n",
    "nb_class=max(unique_elements)+1\n",
    "        \n",
    "Y_cat=utils.to_categorical(Y_change, num_classes=nb_class)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_cat, test_size=0.2, random_state=54)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                np.unique(Y_change),\n",
    "                                                 Y_change)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "print(class_weights)\n",
    "input_shape=(1520,2,1)\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=52, kernel_size=(6, 2), padding='valid',activation='tanh', kernel_constraint=min_max_norm(min_value=1.0, max_value=1.0),input_shape=input_shape))\n",
    "model.add(layers.MaxPooling2D((2, 1), strides=2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(units=162, activation='tanh'))\n",
    "model.add(layers.Dropout(0.8)\n",
    ")\n",
    "\n",
    "model.add(layers.Dense(nb_class, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate = 0.001),\n",
    "          loss=tf.keras.losses.categorical_crossentropy,\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "history=model.fit(X_train, y_train, batch_size=batch_size, epochs=steps, verbose=1, validation_split=0.2,\n",
    "                 class_weight=class_weights)\n",
    "\n",
    "i = np.arange(steps)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(i, history.history['accuracy'], label='accuracy')\n",
    "plt.plot(i, history.history['val_accuracy'], label='val_accuaracy')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions= model.predict(X_test)\n",
    "\n",
    "predictions = [np.argmax(y, axis=0, out=None) for y in predictions]\n",
    "y_test_cat = [np.argmax(y, axis=0, out=None) for y in y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "Conf mat\n",
      "[[43  3]\n",
      " [ 3 48]]\n",
      "f1_score for each class\n",
      "[0.93478261 0.94117647]\n",
      "Global f1_score\n",
      "0.9381443298969072\n",
      "Global accuracy\n",
      "0.9381443298969072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2c05ef0490>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_score, recall_score, f1_score, accuracy_score, average_precision_score\n",
    "\n",
    "present_class_list=np.unique(y_test_cat)\n",
    "print(present_class_list)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat=confusion_matrix(y_test_cat, predictions, labels=present_class_list)\n",
    "print(\"Conf mat\")\n",
    "print(conf_mat)\n",
    "\n",
    "#F1-score\n",
    "print(\"f1_score for each class\")\n",
    "f1_each=f1_score(y_test_cat, predictions, average=None, labels=present_class_list)\n",
    "print(f1_each)\n",
    "\n",
    "#F1-score\n",
    "print(\"Global f1_score\")\n",
    "print(f1_score(y_test_cat, predictions, average='weighted'))\n",
    "\n",
    "#accyracy\n",
    "print(\"Global accuracy\")\n",
    "print(accuracy_score(y_test_cat,predictions))\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "cmap = sns.diverging_palette(200, 10, as_cmap=True)\n",
    "sns.heatmap(conf_mat, cmap=cmap, xticklabels=present_class_list, yticklabels=present_class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating input for CNN: IQ values for a whole pkt     output: robot_node\n",
    "X=list()\n",
    "Y=list()\n",
    "\n",
    "i=0\n",
    "while i<len(idata_red.index):\n",
    "    data=idata_red.iloc[i:i+pkt_len]\n",
    "    if len(data['Time'].unique())==1:\n",
    "        data=np.array(data[['real','im']], dtype='float64')\n",
    "        x=data.reshape(pkt_len,2,1)\n",
    "        X.append(x)\n",
    "        Y.append(int(idata_red.iloc[i]['Scene']))  #in each scene a different emitter is used ~ robot_node in that case\n",
    "    else:\n",
    "        print('Missing!!')\n",
    "    i=i+pkt_len\n",
    "\n",
    "X=np.array(X)\n",
    "Y=np.array(Y, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36 37]\n",
      "[299 242]\n",
      "242\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(Y, return_counts=True)\n",
    "print(unique_elements)\n",
    "print(counts_elements)\n",
    "#balance classes\n",
    "min_samples=min(counts_elements)\n",
    "print(min_samples)\n",
    "\n",
    "thirty_six_index=np.where(Y == 36)\n",
    "thirty_six_index=thirty_six_index[0][:min_samples]\n",
    "X_thirty_six=X[thirty_six_index]\n",
    "Y_thirty_six=Y[thirty_six_index]\n",
    "\n",
    "thirty_seven_index=np.where(Y == 37)\n",
    "thirty_seven_index=thirty_seven_index[0][:min_samples]\n",
    "X_thirty_seven=X[thirty_seven_index]\n",
    "Y_thirty_seven=Y[thirty_seven_index]\n",
    "\n",
    "X=np.concatenate((X_thirty_six, X_thirty_seven))\n",
    "Y=np.concatenate((Y_thirty_six, Y_thirty_seven))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, optimizers\n",
    "from kerastuner.tuners import RandomSearch, Hyperband\n",
    "\n",
    "input_shape=(1520,2,1)\n",
    "\n",
    "def build_model(h):\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(filters=h.Int('filters_1', 2, 200,10, default=32), kernel_size=(6, 2), padding='valid',activation=h.Choice(\n",
    "                'activation_1',\n",
    "                values=['relu', 'tanh', 'sigmoid'],\n",
    "                default='relu'), kernel_constraint=min_max_norm(min_value=1.0, max_value=1.0),input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 1), strides=2))\n",
    "    model.add(layers.Conv2D(filters=h.Int('filters_2', 2, 200,10, default=32), kernel_size=(8,1), padding='valid',activation=h.Choice(\n",
    "                'activation_2',\n",
    "                values=['relu', 'tanh', 'sigmoid'],\n",
    "                default='relu'),kernel_constraint=min_max_norm(min_value=1.0, max_value=1.0)))\n",
    "    model.add(layers.MaxPooling2D((4,1), strides=4))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(units=h.Int('units_dense', 2, 200,10, default=100), activation=h.Choice(\n",
    "                'activation_3',\n",
    "                values=['relu', 'tanh', 'sigmoid'],\n",
    "                default='relu')))\n",
    "    model.add(layers.Dropout(\n",
    "            h.Float(\n",
    "                'dropout',\n",
    "                min_value=0.5,\n",
    "                max_value=0.9,\n",
    "                default=0.7,\n",
    "                step=0.1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(layers.Dense(nb_class, activation='softmax'))\n",
    "\n",
    "    hp_learning_rate = h.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4])\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "{0: 1.0, 1: 1.0}\n"
     ]
    }
   ],
   "source": [
    "#trying first simple CNN to test if our data are ok\n",
    "#using Thibaud network\n",
    "from tensorflow.keras.constraints import min_max_norm\n",
    "from tensorflow.keras import layers, models, metrics, optimizers, utils\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "batch_size=100\n",
    "steps=500\n",
    "\n",
    "Y_change=list()\n",
    "\n",
    "#36:0, 37:1\n",
    "for i in Y:\n",
    "    if i==36:\n",
    "        Y_change.append(0)\n",
    "    elif i==37:\n",
    "        Y_change.append(1)\n",
    "\n",
    "Y_change=np.array(Y_change)\n",
    "\n",
    "unique_elements, counts_elements = np.unique(Y_change, return_counts=True)\n",
    "nb_class=max(unique_elements)+1\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "Y_cat=utils.to_categorical(Y_change, num_classes=nb_class)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_cat, test_size=0.2, random_state=54)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                np.unique(Y_change),\n",
    "                                                 Y_change)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 01m 13s]\n",
      "val_accuracy: 0.5256410241127014\n",
      "\n",
      "Best val_accuracy So Far: 0.967948704957962\n",
      "Total elapsed time: 00h 21m 19s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in ./untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 112\n",
      "activation_1: relu\n",
      "filters_2: 62\n",
      "activation_2: tanh\n",
      "units_dense: 182\n",
      "activation_3: tanh\n",
      "dropout: 0.7\n",
      "learning_rate: 0.001\n",
      "Score: 0.967948704957962\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 172\n",
      "activation_1: tanh\n",
      "filters_2: 72\n",
      "activation_2: tanh\n",
      "units_dense: 162\n",
      "activation_3: relu\n",
      "dropout: 0.6\n",
      "learning_rate: 0.0001\n",
      "Score: 0.8717948496341705\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 102\n",
      "activation_1: relu\n",
      "filters_2: 192\n",
      "activation_2: relu\n",
      "units_dense: 112\n",
      "activation_3: relu\n",
      "dropout: 0.8999999999999999\n",
      "learning_rate: 0.0001\n",
      "Score: 0.7435897588729858\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 72\n",
      "activation_1: tanh\n",
      "filters_2: 162\n",
      "activation_2: tanh\n",
      "units_dense: 12\n",
      "activation_3: sigmoid\n",
      "dropout: 0.6\n",
      "learning_rate: 0.0001\n",
      "Score: 0.6153846383094788\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 132\n",
      "activation_1: relu\n",
      "filters_2: 82\n",
      "activation_2: tanh\n",
      "units_dense: 182\n",
      "activation_3: tanh\n",
      "dropout: 0.7\n",
      "learning_rate: 0.01\n",
      "Score: 0.5256410241127014\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 102\n",
      "activation_1: relu\n",
      "filters_2: 192\n",
      "activation_2: sigmoid\n",
      "units_dense: 182\n",
      "activation_3: relu\n",
      "dropout: 0.8999999999999999\n",
      "learning_rate: 0.01\n",
      "Score: 0.5256410241127014\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 12\n",
      "activation_1: relu\n",
      "filters_2: 152\n",
      "activation_2: sigmoid\n",
      "units_dense: 182\n",
      "activation_3: tanh\n",
      "dropout: 0.5\n",
      "learning_rate: 0.001\n",
      "Score: 0.5256410241127014\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 32\n",
      "activation_1: tanh\n",
      "filters_2: 142\n",
      "activation_2: relu\n",
      "units_dense: 162\n",
      "activation_3: relu\n",
      "dropout: 0.7\n",
      "learning_rate: 0.01\n",
      "Score: 0.5256410241127014\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 42\n",
      "activation_1: relu\n",
      "filters_2: 82\n",
      "activation_2: tanh\n",
      "units_dense: 112\n",
      "activation_3: sigmoid\n",
      "dropout: 0.6\n",
      "learning_rate: 0.01\n",
      "Score: 0.5256410241127014\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 72\n",
      "activation_1: tanh\n",
      "filters_2: 32\n",
      "activation_2: sigmoid\n",
      "units_dense: 32\n",
      "activation_3: sigmoid\n",
      "dropout: 0.8999999999999999\n",
      "learning_rate: 0.0001\n",
      "Score: 0.5256410241127014\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "tuner_rs = RandomSearch(\n",
    "            build_model,\n",
    "    objective='val_accuracy',\n",
    "            seed=42,\n",
    "            max_trials=20,\n",
    "            executions_per_trial=2,\n",
    "            overwrite=True)\n",
    "\n",
    "tuner_rs.search_space_summary()\n",
    "\n",
    "\n",
    "\n",
    "tuner_rs.search(X_train, y_train, epochs=300, validation_split=0.2, verbose=1, class_weight=class_weights)\n",
    "\n",
    "tuner_rs.results_summary()\n",
    "\n",
    "best_model = tuner_rs.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more layers doesn't seem to improve performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost doesnot support 2d inputs: lets reshape the initial matrix in a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(484, 3040)\n"
     ]
    }
   ],
   "source": [
    "X_vect=list()\n",
    "for x in X:\n",
    "    x_vect=x.reshape(len(x)*2)\n",
    "    X_vect.append(x_vect)\n",
    "X_vect=np.array(X_vect)\n",
    "print(X_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_change=list()\n",
    "\n",
    "#36:0, 37:1\n",
    "for i in Y:\n",
    "    if i==36:\n",
    "        Y_change.append(0)\n",
    "    elif i==37:\n",
    "        Y_change.append(1)\n",
    "\n",
    "Y_change=np.array(Y_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(387, 3040)\n",
      "[11:18:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vect, Y_change, test_size=0.2, random_state=56)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "boost = XGBClassifier(max_depth=50, use_label_encoder=False)\n",
    "boost.fit(np.array(X_train), y_train)\n",
    "predictions = boost.predict(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score for each class\n",
      "[0.84615385 0.82222222]\n",
      "Global f1_score\n",
      "0.8350515463917526\n",
      "Global accuracy\n",
      "0.8350515463917526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2d2a62d450>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "present_class_list=np.unique(y_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat=confusion_matrix(y_test, predictions, labels=present_class_list)\n",
    "\n",
    "#F1-score\n",
    "print(\"f1_score for each class\")\n",
    "f1_each=f1_score(y_test, predictions, average=None, labels=present_class_list)\n",
    "print(f1_each)\n",
    "\n",
    "#F1-score\n",
    "print(\"Global f1_score\")\n",
    "print(f1_score(y_test, predictions, average='weighted'))\n",
    "\n",
    "#accyracy\n",
    "print(\"Global accuracy\")\n",
    "print(accuracy_score(y_test,predictions))\n",
    "\n",
    "import seaborn as sns\n",
    "cmap = sns.diverging_palette(200, 10, as_cmap=True)\n",
    "sns.heatmap(conf_mat, cmap=cmap, xticklabels=present_class_list, yticklabels=present_class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGboost performs worst than CNN, let's try to tune it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] min_child_weight=0, max_depth=55, gamma=3, booster=gbtree .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:19:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=0, max_depth=55, gamma=3, booster=gbtree, total=  16.3s\n",
      "[CV] min_child_weight=0, max_depth=55, gamma=3, booster=gbtree .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   16.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:19:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=0, max_depth=55, gamma=3, booster=gbtree, total=  18.0s\n",
      "[CV] min_child_weight=0, max_depth=55, gamma=3, booster=gbtree .......\n",
      "[11:20:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=0, max_depth=55, gamma=3, booster=gbtree, total=  16.3s\n",
      "[CV] min_child_weight=0, max_depth=55, gamma=3, booster=gbtree .......\n",
      "[11:20:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=0, max_depth=55, gamma=3, booster=gbtree, total=  17.9s\n",
      "[CV] min_child_weight=0, max_depth=55, gamma=3, booster=gbtree .......\n",
      "[11:20:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=0, max_depth=55, gamma=3, booster=gbtree, total=  16.9s\n",
      "[CV] min_child_weight=6, max_depth=55, gamma=0, booster=gbtree .......\n",
      "[11:20:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=6, max_depth=55, gamma=0, booster=gbtree, total=   2.9s\n",
      "[CV] min_child_weight=6, max_depth=55, gamma=0, booster=gbtree .......\n",
      "[11:20:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=6, max_depth=55, gamma=0, booster=gbtree, total=   2.9s\n",
      "[CV] min_child_weight=6, max_depth=55, gamma=0, booster=gbtree .......\n",
      "[11:21:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=6, max_depth=55, gamma=0, booster=gbtree, total=   2.9s\n",
      "[CV] min_child_weight=6, max_depth=55, gamma=0, booster=gbtree .......\n",
      "[11:21:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=6, max_depth=55, gamma=0, booster=gbtree, total=   3.0s\n",
      "[CV] min_child_weight=6, max_depth=55, gamma=0, booster=gbtree .......\n",
      "[11:21:06] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=6, max_depth=55, gamma=0, booster=gbtree, total=   2.9s\n",
      "[CV] min_child_weight=3, max_depth=85, gamma=0, booster=gblinear .....\n",
      "[11:21:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:21:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=85, gamma=0, booster=gblinear, total=   0.5s\n",
      "[CV] min_child_weight=3, max_depth=85, gamma=0, booster=gblinear .....\n",
      "[11:21:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:21:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=85, gamma=0, booster=gblinear, total=   0.5s\n",
      "[CV] min_child_weight=3, max_depth=85, gamma=0, booster=gblinear .....\n",
      "[11:21:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:21:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=85, gamma=0, booster=gblinear, total=   0.6s\n",
      "[CV] min_child_weight=3, max_depth=85, gamma=0, booster=gblinear .....\n",
      "[11:21:11] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:21:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=85, gamma=0, booster=gblinear, total=   0.5s\n",
      "[CV] min_child_weight=3, max_depth=85, gamma=0, booster=gblinear .....\n",
      "[11:21:11] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:21:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=85, gamma=0, booster=gblinear, total=   0.5s\n",
      "[CV] min_child_weight=3, max_depth=70, gamma=3, booster=gblinear .....\n",
      "[11:21:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:21:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=70, gamma=3, booster=gblinear, total=   0.5s\n",
      "[CV] min_child_weight=3, max_depth=70, gamma=3, booster=gblinear .....\n",
      "[11:21:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:21:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=70, gamma=3, booster=gblinear, total=   0.6s\n",
      "[CV] min_child_weight=3, max_depth=70, gamma=3, booster=gblinear .....\n",
      "[11:21:13] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:21:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=70, gamma=3, booster=gblinear, total=   0.5s\n",
      "[CV] min_child_weight=3, max_depth=70, gamma=3, booster=gblinear .....\n",
      "[11:21:13] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:21:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=70, gamma=3, booster=gblinear, total=   0.5s\n",
      "[CV] min_child_weight=3, max_depth=70, gamma=3, booster=gblinear .....\n",
      "[11:21:14] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:21:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=70, gamma=3, booster=gblinear, total=   0.5s\n",
      "[CV] min_child_weight=0, max_depth=55, gamma=3, booster=gblinear .....\n",
      "[11:21:14] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:21:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=0, max_depth=55, gamma=3, booster=gblinear, total=   0.5s\n",
      "[CV] min_child_weight=0, max_depth=55, gamma=3, booster=gblinear .....\n",
      "[11:21:15] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:21:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=0, max_depth=55, gamma=3, booster=gblinear, total=   0.5s\n",
      "[CV] min_child_weight=0, max_depth=55, gamma=3, booster=gblinear .....\n",
      "[11:21:15] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:21:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=0, max_depth=55, gamma=3, booster=gblinear, total=   0.5s\n",
      "[CV] min_child_weight=0, max_depth=55, gamma=3, booster=gblinear .....\n",
      "[11:21:16] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:21:16] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=0, max_depth=55, gamma=3, booster=gblinear, total=   0.5s\n",
      "[CV] min_child_weight=0, max_depth=55, gamma=3, booster=gblinear .....\n",
      "[11:21:17] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:21:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=0, max_depth=55, gamma=3, booster=gblinear, total=   0.5s\n",
      "[CV] min_child_weight=0, max_depth=85, gamma=0, booster=dart .........\n",
      "[11:21:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=0, max_depth=85, gamma=0, booster=dart, total=  25.1s\n",
      "[CV] min_child_weight=0, max_depth=85, gamma=0, booster=dart .........\n",
      "[11:21:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=0, max_depth=85, gamma=0, booster=dart, total=  25.0s\n",
      "[CV] min_child_weight=0, max_depth=85, gamma=0, booster=dart .........\n",
      "[11:22:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=0, max_depth=85, gamma=0, booster=dart, total=  25.0s\n",
      "[CV] min_child_weight=0, max_depth=85, gamma=0, booster=dart .........\n",
      "[11:22:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=0, max_depth=85, gamma=0, booster=dart, total=  24.7s\n",
      "[CV] min_child_weight=0, max_depth=85, gamma=0, booster=dart .........\n",
      "[11:22:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=0, max_depth=85, gamma=0, booster=dart, total=  26.1s\n",
      "[CV] min_child_weight=3, max_depth=85, gamma=10, booster=dart ........\n",
      "[11:23:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=85, gamma=10, booster=dart, total=   8.5s\n",
      "[CV] min_child_weight=3, max_depth=85, gamma=10, booster=dart ........\n",
      "[11:23:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=85, gamma=10, booster=dart, total=   8.3s\n",
      "[CV] min_child_weight=3, max_depth=85, gamma=10, booster=dart ........\n",
      "[11:23:40] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=85, gamma=10, booster=dart, total=   8.7s\n",
      "[CV] min_child_weight=3, max_depth=85, gamma=10, booster=dart ........\n",
      "[11:23:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=85, gamma=10, booster=dart, total=   7.8s\n",
      "[CV] min_child_weight=3, max_depth=85, gamma=10, booster=dart ........\n",
      "[11:23:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=85, gamma=10, booster=dart, total=   8.6s\n",
      "[CV] min_child_weight=6, max_depth=55, gamma=0, booster=gblinear .....\n",
      "[11:24:05] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:24:05] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=6, max_depth=55, gamma=0, booster=gblinear, total=   0.6s\n",
      "[CV] min_child_weight=6, max_depth=55, gamma=0, booster=gblinear .....\n",
      "[11:24:06] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:24:06] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=6, max_depth=55, gamma=0, booster=gblinear, total=   0.5s\n",
      "[CV] min_child_weight=6, max_depth=55, gamma=0, booster=gblinear .....\n",
      "[11:24:06] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:24:06] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=6, max_depth=55, gamma=0, booster=gblinear, total=   0.6s\n",
      "[CV] min_child_weight=6, max_depth=55, gamma=0, booster=gblinear .....\n",
      "[11:24:07] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:24:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=6, max_depth=55, gamma=0, booster=gblinear, total=   0.5s\n",
      "[CV] min_child_weight=6, max_depth=55, gamma=0, booster=gblinear .....\n",
      "[11:24:07] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:24:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=6, max_depth=55, gamma=0, booster=gblinear, total=   0.5s\n",
      "[CV] min_child_weight=3, max_depth=70, gamma=0, booster=gbtree .......\n",
      "[11:24:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=70, gamma=0, booster=gbtree, total=   3.4s\n",
      "[CV] min_child_weight=3, max_depth=70, gamma=0, booster=gbtree .......\n",
      "[11:24:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=70, gamma=0, booster=gbtree, total=   3.4s\n",
      "[CV] min_child_weight=3, max_depth=70, gamma=0, booster=gbtree .......\n",
      "[11:24:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=70, gamma=0, booster=gbtree, total=   3.5s\n",
      "[CV] min_child_weight=3, max_depth=70, gamma=0, booster=gbtree .......\n",
      "[11:24:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=70, gamma=0, booster=gbtree, total=   3.5s\n",
      "[CV] min_child_weight=3, max_depth=70, gamma=0, booster=gbtree .......\n",
      "[11:24:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=70, gamma=0, booster=gbtree, total=   3.4s\n",
      "[CV] min_child_weight=10, max_depth=40, gamma=10, booster=gblinear ...\n",
      "[11:24:25] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:24:25] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=10, max_depth=40, gamma=10, booster=gblinear, total=   0.5s\n",
      "[CV] min_child_weight=10, max_depth=40, gamma=10, booster=gblinear ...\n",
      "[11:24:25] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:24:25] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=10, max_depth=40, gamma=10, booster=gblinear, total=   0.5s\n",
      "[CV] min_child_weight=10, max_depth=40, gamma=10, booster=gblinear ...\n",
      "[11:24:26] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:24:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=10, max_depth=40, gamma=10, booster=gblinear, total=   0.5s\n",
      "[CV] min_child_weight=10, max_depth=40, gamma=10, booster=gblinear ...\n",
      "[11:24:26] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:24:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=10, max_depth=40, gamma=10, booster=gblinear, total=   0.5s\n",
      "[CV] min_child_weight=10, max_depth=40, gamma=10, booster=gblinear ...\n",
      "[11:24:27] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:24:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=10, max_depth=40, gamma=10, booster=gblinear, total=   0.5s\n",
      "[11:24:28] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { gamma, max_depth, min_child_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:24:28] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score=nan,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n...\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           use_label_encoder=False,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   iid='deprecated', n_iter=10, n_jobs=-1,\n",
       "                   param_distributions={'booster': ['gbtree', 'gblinear',\n",
       "                                                    'dart'],\n",
       "                                        'gamma': [0, 3, 6, 10],\n",
       "                                        'max_depth': [40, 55, 70, 85, 100],\n",
       "                                        'min_child_weight': [0, 3, 6, 10]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vect, Y_change, test_size=0.2, random_state=56)\n",
    "\n",
    "booster=['gbtree','gblinear', 'dart']\n",
    "max_depth = [int(x) for x in np.linspace(40, 100, num = 5)]\n",
    "gamma=[int(x) for x in np.linspace(0, 10, num = 4)]\n",
    "min_child_weight=[int(x) for x in np.linspace(0, 10, num = 4)]\n",
    "\n",
    "\n",
    "random_grid = {'booster': booster,\n",
    "               'max_depth': max_depth,\n",
    "               'gamma': gamma,\n",
    "               'min_child_weight': min_child_weight}\n",
    "\n",
    "xg = XGBClassifier(use_label_encoder=False)\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "xg_random = RandomizedSearchCV(estimator = xg, param_distributions = random_grid, n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = -1)# Fit the random search model\n",
    "xg_random.fit(np.array(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_child_weight': 3, 'max_depth': 85, 'gamma': 0, 'booster': 'gblinear'}\n"
     ]
    }
   ],
   "source": [
    "clf = xg_random.best_estimator_\n",
    "\n",
    "print(xg_random.best_params_)\n",
    "\n",
    "predictions=clf.predict(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score for each class\n",
      "[0.90566038 0.88636364]\n",
      "Global f1_score\n",
      "0.8967082810206716\n",
      "Global accuracy\n",
      "0.8969072164948454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2d2a788910>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "present_class_list=np.unique(y_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat=confusion_matrix(y_test, predictions, labels=present_class_list)\n",
    "\n",
    "#F1-score\n",
    "print(\"f1_score for each class\")\n",
    "f1_each=f1_score(y_test, predictions, average=None, labels=present_class_list)\n",
    "print(f1_each)\n",
    "\n",
    "#F1-score\n",
    "print(\"Global f1_score\")\n",
    "print(f1_score(y_test, predictions, average='weighted'))\n",
    "\n",
    "#accyracy\n",
    "print(\"Global accuracy\")\n",
    "print(accuracy_score(y_test,predictions))\n",
    "\n",
    "import seaborn as sns\n",
    "cmap = sns.diverging_palette(200, 10, as_cmap=True)\n",
    "sns.heatmap(conf_mat, cmap=cmap, xticklabels=present_class_list, yticklabels=present_class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even tuned, XGBoost performs worst than the CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using only payload portions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now, we have been using whole BLE packet, meaning that the source adress was in the input data, making it fairly easy to classify. Now, let's try to use input without this piece of information: we will only be using the packet payload. \\\\\n",
    "After studying the pcap, we saw that the packet's payload starts at byte 23, for a packet of 47 bytes. Knowing that one packet is represented here by 1520 samples, we will drop the first 744 samples at the beginning of the packet for the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating input for CNN: IQ values for a whole pkt     output: robot_node\n",
    "X=list()\n",
    "Y=list()\n",
    "\n",
    "i=0\n",
    "while i<len(idata_red.index):\n",
    "    data=idata_red.iloc[i+744:i+pkt_len]\n",
    "    if len(data['Time'].unique())==1:\n",
    "        data=np.array(data[['real','im']], dtype='float64')\n",
    "        x=data.reshape((pkt_len-744),2,1)\n",
    "        X.append(x)\n",
    "        Y.append(int(idata_red.iloc[i]['Scene']))  #in each scene a different emitter is used ~ robot_node in that case\n",
    "    else:\n",
    "        print('Missing!!')\n",
    "    i=i+pkt_len\n",
    "\n",
    "X=np.array(X)\n",
    "Y=np.array(Y, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36 37]\n",
      "[299 242]\n",
      "242\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(Y, return_counts=True)\n",
    "print(unique_elements)\n",
    "print(counts_elements)\n",
    "#balance classes\n",
    "min_samples=min(counts_elements)\n",
    "print(min_samples)\n",
    "\n",
    "thirty_six_index=np.where(Y == 36)\n",
    "thirty_six_index=thirty_six_index[0][:min_samples]\n",
    "X_thirty_six=X[thirty_six_index]\n",
    "Y_thirty_six=Y[thirty_six_index]\n",
    "\n",
    "thirty_seven_index=np.where(Y == 37)\n",
    "thirty_seven_index=thirty_seven_index[0][:min_samples]\n",
    "X_thirty_seven=X[thirty_seven_index]\n",
    "Y_thirty_seven=Y[thirty_seven_index]\n",
    "\n",
    "X=np.concatenate((X_thirty_six, X_thirty_seven))\n",
    "Y=np.concatenate((Y_thirty_six, Y_thirty_seven))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "{0: 1.0, 1: 1.0}\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 4s 982ms/step - loss: 0.6937 - accuracy: 0.4757 - val_loss: 0.6941 - val_accuracy: 0.4744\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6861 - accuracy: 0.4951 - val_loss: 0.6940 - val_accuracy: 0.4872\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6768 - accuracy: 0.5502 - val_loss: 0.6985 - val_accuracy: 0.4744\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6771 - accuracy: 0.5890 - val_loss: 0.6929 - val_accuracy: 0.4872\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6812 - accuracy: 0.5307 - val_loss: 0.6909 - val_accuracy: 0.5128\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6766 - accuracy: 0.5534 - val_loss: 0.6887 - val_accuracy: 0.5385\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6675 - accuracy: 0.5761 - val_loss: 0.6894 - val_accuracy: 0.5256\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6683 - accuracy: 0.5340 - val_loss: 0.6888 - val_accuracy: 0.5256\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6815 - accuracy: 0.5113 - val_loss: 0.6877 - val_accuracy: 0.5256\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6739 - accuracy: 0.5049 - val_loss: 0.6885 - val_accuracy: 0.4872\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6510 - accuracy: 0.6084 - val_loss: 0.6928 - val_accuracy: 0.4744\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6541 - accuracy: 0.5825 - val_loss: 0.6843 - val_accuracy: 0.4872\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6494 - accuracy: 0.6440 - val_loss: 0.6818 - val_accuracy: 0.5513\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6590 - accuracy: 0.5858 - val_loss: 0.6805 - val_accuracy: 0.5769\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6534 - accuracy: 0.6214 - val_loss: 0.6865 - val_accuracy: 0.4872\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6509 - accuracy: 0.5696 - val_loss: 0.6916 - val_accuracy: 0.4744\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6459 - accuracy: 0.6149 - val_loss: 0.6866 - val_accuracy: 0.4744\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6395 - accuracy: 0.6117 - val_loss: 0.6797 - val_accuracy: 0.7308\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6567 - accuracy: 0.5890 - val_loss: 0.6775 - val_accuracy: 0.5641\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6427 - accuracy: 0.6278 - val_loss: 0.6868 - val_accuracy: 0.4615\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6538 - accuracy: 0.5987 - val_loss: 0.6913 - val_accuracy: 0.4615\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6445 - accuracy: 0.6246 - val_loss: 0.6866 - val_accuracy: 0.4615\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6403 - accuracy: 0.5922 - val_loss: 0.6966 - val_accuracy: 0.4744\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6472 - accuracy: 0.6375 - val_loss: 0.6946 - val_accuracy: 0.4744\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6463 - accuracy: 0.5793 - val_loss: 0.6773 - val_accuracy: 0.6282\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6176 - accuracy: 0.7087 - val_loss: 0.6741 - val_accuracy: 0.5897\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6268 - accuracy: 0.6667 - val_loss: 0.6741 - val_accuracy: 0.6923\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6329 - accuracy: 0.6311 - val_loss: 0.6730 - val_accuracy: 0.6795\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6198 - accuracy: 0.6311 - val_loss: 0.6723 - val_accuracy: 0.5769\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6322 - accuracy: 0.5955 - val_loss: 0.6789 - val_accuracy: 0.5513\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6431 - accuracy: 0.5890 - val_loss: 0.6794 - val_accuracy: 0.5513\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6209 - accuracy: 0.5890 - val_loss: 0.6735 - val_accuracy: 0.5769\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6189 - accuracy: 0.6408 - val_loss: 0.6971 - val_accuracy: 0.4744\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6124 - accuracy: 0.6246 - val_loss: 0.6673 - val_accuracy: 0.5769\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6186 - accuracy: 0.6375 - val_loss: 0.6760 - val_accuracy: 0.5513\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6197 - accuracy: 0.6084 - val_loss: 0.6672 - val_accuracy: 0.8333\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5934 - accuracy: 0.7087 - val_loss: 0.7094 - val_accuracy: 0.4744\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6408 - accuracy: 0.6149 - val_loss: 0.6642 - val_accuracy: 0.6538\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6066 - accuracy: 0.6602 - val_loss: 0.6649 - val_accuracy: 0.5897\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6169 - accuracy: 0.6278 - val_loss: 0.6715 - val_accuracy: 0.4744\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5993 - accuracy: 0.7282 - val_loss: 0.6652 - val_accuracy: 0.6795\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5850 - accuracy: 0.7702 - val_loss: 0.6678 - val_accuracy: 0.8205\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5723 - accuracy: 0.7443 - val_loss: 0.6673 - val_accuracy: 0.8333\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5944 - accuracy: 0.6634 - val_loss: 0.6650 - val_accuracy: 0.5769\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6140 - accuracy: 0.6246 - val_loss: 0.6632 - val_accuracy: 0.5769\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5748 - accuracy: 0.7184 - val_loss: 0.6656 - val_accuracy: 0.5897\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5735 - accuracy: 0.7379 - val_loss: 0.6580 - val_accuracy: 0.7564\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5910 - accuracy: 0.6958 - val_loss: 0.6559 - val_accuracy: 0.5897\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5715 - accuracy: 0.7087 - val_loss: 0.6557 - val_accuracy: 0.8462\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5861 - accuracy: 0.6958 - val_loss: 0.6538 - val_accuracy: 0.5897\n",
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5652 - accuracy: 0.6926 - val_loss: 0.6620 - val_accuracy: 0.4744\n",
      "Epoch 52/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5864 - accuracy: 0.6990 - val_loss: 0.6799 - val_accuracy: 0.4615\n",
      "Epoch 53/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5970 - accuracy: 0.6667 - val_loss: 0.6524 - val_accuracy: 0.8718\n",
      "Epoch 54/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5428 - accuracy: 0.7314 - val_loss: 0.6465 - val_accuracy: 0.5769\n",
      "Epoch 55/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5644 - accuracy: 0.6861 - val_loss: 0.7049 - val_accuracy: 0.4744\n",
      "Epoch 56/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6114 - accuracy: 0.6472 - val_loss: 0.6554 - val_accuracy: 0.5256\n",
      "Epoch 57/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5523 - accuracy: 0.7411 - val_loss: 0.6431 - val_accuracy: 0.5897\n",
      "Epoch 58/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5529 - accuracy: 0.7055 - val_loss: 0.6557 - val_accuracy: 0.4872\n",
      "Epoch 59/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5653 - accuracy: 0.6731 - val_loss: 0.6519 - val_accuracy: 0.5256\n",
      "Epoch 60/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5384 - accuracy: 0.7508 - val_loss: 0.6371 - val_accuracy: 0.6795\n",
      "Epoch 61/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5306 - accuracy: 0.7508 - val_loss: 0.6415 - val_accuracy: 0.8846\n",
      "Epoch 62/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5858 - accuracy: 0.6667 - val_loss: 0.6392 - val_accuracy: 0.5769\n",
      "Epoch 63/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6012 - accuracy: 0.6634 - val_loss: 0.6333 - val_accuracy: 0.6538\n",
      "Epoch 64/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5347 - accuracy: 0.7476 - val_loss: 0.6619 - val_accuracy: 0.4615\n",
      "Epoch 65/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5177 - accuracy: 0.7411 - val_loss: 0.6291 - val_accuracy: 0.6538\n",
      "Epoch 66/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5382 - accuracy: 0.7508 - val_loss: 0.6283 - val_accuracy: 0.6282\n",
      "Epoch 67/300\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.5278 - accuracy: 0.7702 - val_loss: 0.6329 - val_accuracy: 0.8974\n",
      "Epoch 68/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5304 - accuracy: 0.7314 - val_loss: 0.6320 - val_accuracy: 0.8846\n",
      "Epoch 69/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5199 - accuracy: 0.7605 - val_loss: 0.6250 - val_accuracy: 0.8974\n",
      "Epoch 70/300\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.5078 - accuracy: 0.7929 - val_loss: 0.6186 - val_accuracy: 0.7308\n",
      "Epoch 71/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5071 - accuracy: 0.7994 - val_loss: 0.6213 - val_accuracy: 0.8974\n",
      "Epoch 72/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5187 - accuracy: 0.7702 - val_loss: 0.6222 - val_accuracy: 0.8718\n",
      "Epoch 73/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4950 - accuracy: 0.8188 - val_loss: 0.6102 - val_accuracy: 0.6538\n",
      "Epoch 74/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5068 - accuracy: 0.7702 - val_loss: 0.6092 - val_accuracy: 0.8205\n",
      "Epoch 75/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4891 - accuracy: 0.8091 - val_loss: 0.6473 - val_accuracy: 0.4744\n",
      "Epoch 76/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5399 - accuracy: 0.7087 - val_loss: 0.6075 - val_accuracy: 0.8846\n",
      "Epoch 77/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4793 - accuracy: 0.8058 - val_loss: 0.6020 - val_accuracy: 0.6667\n",
      "Epoch 78/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4783 - accuracy: 0.8285 - val_loss: 0.6471 - val_accuracy: 0.4744\n",
      "Epoch 79/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5043 - accuracy: 0.7411 - val_loss: 0.6156 - val_accuracy: 0.5769\n",
      "Epoch 80/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5362 - accuracy: 0.6893 - val_loss: 0.6011 - val_accuracy: 0.6282\n",
      "Epoch 81/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4759 - accuracy: 0.7994 - val_loss: 0.6540 - val_accuracy: 0.4744\n",
      "Epoch 82/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4936 - accuracy: 0.7508 - val_loss: 0.6055 - val_accuracy: 0.8846\n",
      "Epoch 83/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4672 - accuracy: 0.7929 - val_loss: 0.5978 - val_accuracy: 0.8846\n",
      "Epoch 84/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4875 - accuracy: 0.7832 - val_loss: 0.5935 - val_accuracy: 0.8462\n",
      "Epoch 85/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4666 - accuracy: 0.8252 - val_loss: 0.5906 - val_accuracy: 0.8590\n",
      "Epoch 86/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4566 - accuracy: 0.8414 - val_loss: 0.5978 - val_accuracy: 0.8846\n",
      "Epoch 87/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4523 - accuracy: 0.8317 - val_loss: 0.5937 - val_accuracy: 0.8846\n",
      "Epoch 88/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4677 - accuracy: 0.8091 - val_loss: 0.5802 - val_accuracy: 0.8590\n",
      "Epoch 89/300\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.4591 - accuracy: 0.8155 - val_loss: 0.5752 - val_accuracy: 0.8077\n",
      "Epoch 90/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4278 - accuracy: 0.8964 - val_loss: 0.6321 - val_accuracy: 0.4744\n",
      "Epoch 91/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4776 - accuracy: 0.7605 - val_loss: 0.5710 - val_accuracy: 0.8846\n",
      "Epoch 92/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4622 - accuracy: 0.8188 - val_loss: 0.5726 - val_accuracy: 0.6667\n",
      "Epoch 93/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4436 - accuracy: 0.8350 - val_loss: 0.5642 - val_accuracy: 0.8974\n",
      "Epoch 94/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4408 - accuracy: 0.8285 - val_loss: 0.5581 - val_accuracy: 0.8205\n",
      "Epoch 95/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4250 - accuracy: 0.8673 - val_loss: 0.5578 - val_accuracy: 0.8846\n",
      "Epoch 96/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4197 - accuracy: 0.8673 - val_loss: 0.5896 - val_accuracy: 0.6154\n",
      "Epoch 97/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4179 - accuracy: 0.8155 - val_loss: 0.5526 - val_accuracy: 0.7692\n",
      "Epoch 98/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4799 - accuracy: 0.7476 - val_loss: 0.5461 - val_accuracy: 0.8846\n",
      "Epoch 99/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4424 - accuracy: 0.8350 - val_loss: 0.5666 - val_accuracy: 0.8718\n",
      "Epoch 100/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4219 - accuracy: 0.8285 - val_loss: 0.5397 - val_accuracy: 0.7949\n",
      "Epoch 101/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4174 - accuracy: 0.8220 - val_loss: 0.5574 - val_accuracy: 0.8718\n",
      "Epoch 102/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4047 - accuracy: 0.8317 - val_loss: 0.5368 - val_accuracy: 0.8974\n",
      "Epoch 103/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4087 - accuracy: 0.8511 - val_loss: 0.5276 - val_accuracy: 0.8590\n",
      "Epoch 104/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3852 - accuracy: 0.8706 - val_loss: 0.5337 - val_accuracy: 0.8974\n",
      "Epoch 105/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4135 - accuracy: 0.8350 - val_loss: 0.5341 - val_accuracy: 0.8846\n",
      "Epoch 106/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3788 - accuracy: 0.8673 - val_loss: 0.5291 - val_accuracy: 0.8974\n",
      "Epoch 107/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4379 - accuracy: 0.8058 - val_loss: 0.5301 - val_accuracy: 0.8974\n",
      "Epoch 108/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3958 - accuracy: 0.8544 - val_loss: 0.5098 - val_accuracy: 0.8077\n",
      "Epoch 109/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3889 - accuracy: 0.8447 - val_loss: 0.5042 - val_accuracy: 0.8333\n",
      "Epoch 110/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3670 - accuracy: 0.8803 - val_loss: 0.5009 - val_accuracy: 0.8974\n",
      "Epoch 111/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3734 - accuracy: 0.8835 - val_loss: 0.4976 - val_accuracy: 0.8974\n",
      "Epoch 112/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3457 - accuracy: 0.9126 - val_loss: 0.4930 - val_accuracy: 0.8846\n",
      "Epoch 113/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3665 - accuracy: 0.9126 - val_loss: 0.5013 - val_accuracy: 0.8974\n",
      "Epoch 114/300\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.3579 - accuracy: 0.8673 - val_loss: 0.5144 - val_accuracy: 0.8590\n",
      "Epoch 115/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3633 - accuracy: 0.8641 - val_loss: 0.4871 - val_accuracy: 0.9103\n",
      "Epoch 116/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3532 - accuracy: 0.8932 - val_loss: 0.4806 - val_accuracy: 0.9231\n",
      "Epoch 117/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3415 - accuracy: 0.9094 - val_loss: 0.4724 - val_accuracy: 0.8974\n",
      "Epoch 118/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3331 - accuracy: 0.9191 - val_loss: 0.4857 - val_accuracy: 0.8974\n",
      "Epoch 119/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3482 - accuracy: 0.8964 - val_loss: 0.4757 - val_accuracy: 0.9103\n",
      "Epoch 120/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3233 - accuracy: 0.9223 - val_loss: 0.4717 - val_accuracy: 0.9103\n",
      "Epoch 121/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3308 - accuracy: 0.9191 - val_loss: 0.4526 - val_accuracy: 0.8974\n",
      "Epoch 122/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3121 - accuracy: 0.9482 - val_loss: 0.4500 - val_accuracy: 0.9103\n",
      "Epoch 123/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3231 - accuracy: 0.9126 - val_loss: 0.4631 - val_accuracy: 0.9359\n",
      "Epoch 124/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3315 - accuracy: 0.8997 - val_loss: 0.4545 - val_accuracy: 0.9359\n",
      "Epoch 125/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3133 - accuracy: 0.9353 - val_loss: 0.4364 - val_accuracy: 0.8974\n",
      "Epoch 126/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3104 - accuracy: 0.9288 - val_loss: 0.4370 - val_accuracy: 0.9231\n",
      "Epoch 127/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2988 - accuracy: 0.9450 - val_loss: 0.4286 - val_accuracy: 0.8846\n",
      "Epoch 128/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3155 - accuracy: 0.9029 - val_loss: 0.4275 - val_accuracy: 0.9231\n",
      "Epoch 129/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2934 - accuracy: 0.9482 - val_loss: 0.4387 - val_accuracy: 0.9231\n",
      "Epoch 130/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2798 - accuracy: 0.9547 - val_loss: 0.4175 - val_accuracy: 0.8974\n",
      "Epoch 131/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2867 - accuracy: 0.9482 - val_loss: 0.4613 - val_accuracy: 0.8974\n",
      "Epoch 132/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3275 - accuracy: 0.8738 - val_loss: 0.4168 - val_accuracy: 0.9359\n",
      "Epoch 133/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2558 - accuracy: 0.9838 - val_loss: 0.4036 - val_accuracy: 0.9359\n",
      "Epoch 134/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2572 - accuracy: 0.9676 - val_loss: 0.4109 - val_accuracy: 0.9359\n",
      "Epoch 135/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2711 - accuracy: 0.9417 - val_loss: 0.3946 - val_accuracy: 0.9487\n",
      "Epoch 136/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2777 - accuracy: 0.9288 - val_loss: 0.4045 - val_accuracy: 0.9359\n",
      "Epoch 137/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2650 - accuracy: 0.9644 - val_loss: 0.3884 - val_accuracy: 0.9487\n",
      "Epoch 138/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2962 - accuracy: 0.9320 - val_loss: 0.3809 - val_accuracy: 0.9103\n",
      "Epoch 139/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2437 - accuracy: 0.9741 - val_loss: 0.3958 - val_accuracy: 0.9359\n",
      "Epoch 140/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2420 - accuracy: 0.9709 - val_loss: 0.3873 - val_accuracy: 0.9231\n",
      "Epoch 141/300\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2349 - accuracy: 0.9773 - val_loss: 0.3707 - val_accuracy: 0.9359\n",
      "Epoch 142/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2157 - accuracy: 0.9871 - val_loss: 0.3733 - val_accuracy: 0.9231\n",
      "Epoch 143/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2468 - accuracy: 0.9547 - val_loss: 0.3629 - val_accuracy: 0.9359\n",
      "Epoch 144/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2224 - accuracy: 0.9806 - val_loss: 0.3635 - val_accuracy: 0.9359\n",
      "Epoch 145/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2292 - accuracy: 0.9676 - val_loss: 0.3537 - val_accuracy: 0.9487\n",
      "Epoch 146/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2407 - accuracy: 0.9353 - val_loss: 0.3654 - val_accuracy: 0.9359\n",
      "Epoch 147/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2184 - accuracy: 0.9709 - val_loss: 0.3453 - val_accuracy: 0.9359\n",
      "Epoch 148/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2255 - accuracy: 0.9612 - val_loss: 0.3410 - val_accuracy: 0.9359\n",
      "Epoch 149/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2087 - accuracy: 0.9871 - val_loss: 0.3438 - val_accuracy: 0.9359\n",
      "Epoch 150/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1992 - accuracy: 0.9806 - val_loss: 0.3336 - val_accuracy: 0.9359\n",
      "Epoch 151/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2023 - accuracy: 0.9871 - val_loss: 0.3387 - val_accuracy: 0.9359\n",
      "Epoch 152/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.1944 - accuracy: 0.9903 - val_loss: 0.3251 - val_accuracy: 0.9487\n",
      "Epoch 153/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1963 - accuracy: 0.9903 - val_loss: 0.3259 - val_accuracy: 0.9487\n",
      "Epoch 154/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2002 - accuracy: 0.9903 - val_loss: 0.3207 - val_accuracy: 0.9487\n",
      "Epoch 155/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1818 - accuracy: 0.9903 - val_loss: 0.3203 - val_accuracy: 0.9359\n",
      "Epoch 156/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1813 - accuracy: 0.9935 - val_loss: 0.3194 - val_accuracy: 0.9359\n",
      "Epoch 157/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1779 - accuracy: 0.9903 - val_loss: 0.3099 - val_accuracy: 0.9487\n",
      "Epoch 158/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1664 - accuracy: 0.9968 - val_loss: 0.3027 - val_accuracy: 0.9359\n",
      "Epoch 159/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1823 - accuracy: 0.9773 - val_loss: 0.3004 - val_accuracy: 0.9487\n",
      "Epoch 160/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.1642 - accuracy: 0.9935 - val_loss: 0.3256 - val_accuracy: 0.9359\n",
      "Epoch 161/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.1717 - accuracy: 0.9871 - val_loss: 0.2954 - val_accuracy: 0.9487\n",
      "Epoch 162/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1667 - accuracy: 0.9935 - val_loss: 0.2874 - val_accuracy: 0.9615\n",
      "Epoch 163/300\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.1577 - accuracy: 0.9935 - val_loss: 0.2870 - val_accuracy: 0.9487\n",
      "Epoch 164/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.1536 - accuracy: 0.9935 - val_loss: 0.2845 - val_accuracy: 0.9487\n",
      "Epoch 165/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.1584 - accuracy: 0.9968 - val_loss: 0.2826 - val_accuracy: 0.9487\n",
      "Epoch 166/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.1588 - accuracy: 0.9935 - val_loss: 0.2756 - val_accuracy: 0.9615\n",
      "Epoch 167/300\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.1693 - accuracy: 0.9903 - val_loss: 0.2709 - val_accuracy: 0.9487\n",
      "Epoch 168/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1457 - accuracy: 0.9968 - val_loss: 0.2822 - val_accuracy: 0.9487\n",
      "Epoch 169/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.1543 - accuracy: 0.9903 - val_loss: 0.2830 - val_accuracy: 0.9487\n",
      "Epoch 170/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.1532 - accuracy: 0.9935 - val_loss: 0.2621 - val_accuracy: 0.9487\n",
      "Epoch 171/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1544 - accuracy: 0.9838 - val_loss: 0.2647 - val_accuracy: 0.9487\n",
      "Epoch 172/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1348 - accuracy: 0.9935 - val_loss: 0.2777 - val_accuracy: 0.9359\n",
      "Epoch 173/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1290 - accuracy: 0.9968 - val_loss: 0.2628 - val_accuracy: 0.9487\n",
      "Epoch 174/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1324 - accuracy: 0.9935 - val_loss: 0.2523 - val_accuracy: 0.9615\n",
      "Epoch 175/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1297 - accuracy: 0.9968 - val_loss: 0.2580 - val_accuracy: 0.9487\n",
      "Epoch 176/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.1239 - accuracy: 0.9935 - val_loss: 0.2467 - val_accuracy: 0.9615\n",
      "Epoch 177/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.1409 - accuracy: 0.9935 - val_loss: 0.2473 - val_accuracy: 0.9487\n",
      "Epoch 178/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1285 - accuracy: 0.9935 - val_loss: 0.2439 - val_accuracy: 0.9615\n",
      "Epoch 179/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.1335 - accuracy: 0.9968 - val_loss: 0.2378 - val_accuracy: 0.9615\n",
      "Epoch 180/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1194 - accuracy: 0.9935 - val_loss: 0.2366 - val_accuracy: 0.9615\n",
      "Epoch 181/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1217 - accuracy: 0.9935 - val_loss: 0.2574 - val_accuracy: 0.9359\n",
      "Epoch 182/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1237 - accuracy: 0.9935 - val_loss: 0.2438 - val_accuracy: 0.9487\n",
      "Epoch 183/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1112 - accuracy: 0.9903 - val_loss: 0.2299 - val_accuracy: 0.9615\n",
      "Epoch 184/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.1170 - accuracy: 0.9935 - val_loss: 0.2492 - val_accuracy: 0.9359\n",
      "Epoch 185/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1283 - accuracy: 0.9903 - val_loss: 0.2314 - val_accuracy: 0.9487\n",
      "Epoch 186/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1159 - accuracy: 0.9903 - val_loss: 0.2238 - val_accuracy: 0.9615\n",
      "Epoch 187/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.1201 - accuracy: 0.9968 - val_loss: 0.2473 - val_accuracy: 0.9487\n",
      "Epoch 188/300\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.1492 - accuracy: 0.9773 - val_loss: 0.2185 - val_accuracy: 0.9615\n",
      "Epoch 189/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1189 - accuracy: 0.9871 - val_loss: 0.2210 - val_accuracy: 0.9615\n",
      "Epoch 190/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1117 - accuracy: 0.9935 - val_loss: 0.2219 - val_accuracy: 0.9487\n",
      "Epoch 191/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1095 - accuracy: 0.9935 - val_loss: 0.2098 - val_accuracy: 0.9744\n",
      "Epoch 192/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1068 - accuracy: 0.9935 - val_loss: 0.2082 - val_accuracy: 0.9615\n",
      "Epoch 193/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1036 - accuracy: 0.9968 - val_loss: 0.2180 - val_accuracy: 0.9487\n",
      "Epoch 194/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0975 - accuracy: 0.9968 - val_loss: 0.2103 - val_accuracy: 0.9615\n",
      "Epoch 195/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0932 - accuracy: 0.9935 - val_loss: 0.2088 - val_accuracy: 0.9615\n",
      "Epoch 196/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0961 - accuracy: 0.9968 - val_loss: 0.2087 - val_accuracy: 0.9487\n",
      "Epoch 197/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0861 - accuracy: 0.9968 - val_loss: 0.2094 - val_accuracy: 0.9487\n",
      "Epoch 198/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0851 - accuracy: 0.9968 - val_loss: 0.2026 - val_accuracy: 0.9615\n",
      "Epoch 199/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0855 - accuracy: 0.9968 - val_loss: 0.1945 - val_accuracy: 0.9615\n",
      "Epoch 200/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0880 - accuracy: 0.9968 - val_loss: 0.1942 - val_accuracy: 0.9615\n",
      "Epoch 201/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0863 - accuracy: 0.9968 - val_loss: 0.1964 - val_accuracy: 0.9615\n",
      "Epoch 202/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0881 - accuracy: 0.9968 - val_loss: 0.1901 - val_accuracy: 0.9615\n",
      "Epoch 203/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0849 - accuracy: 0.9968 - val_loss: 0.1875 - val_accuracy: 0.9615\n",
      "Epoch 204/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0815 - accuracy: 0.9968 - val_loss: 0.1906 - val_accuracy: 0.9615\n",
      "Epoch 205/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0804 - accuracy: 0.9968 - val_loss: 0.1930 - val_accuracy: 0.9615\n",
      "Epoch 206/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0820 - accuracy: 0.9968 - val_loss: 0.1830 - val_accuracy: 0.9615\n",
      "Epoch 207/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0809 - accuracy: 0.9968 - val_loss: 0.1801 - val_accuracy: 0.9615\n",
      "Epoch 208/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0788 - accuracy: 0.9968 - val_loss: 0.1859 - val_accuracy: 0.9615\n",
      "Epoch 209/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0735 - accuracy: 0.9968 - val_loss: 0.1908 - val_accuracy: 0.9487\n",
      "Epoch 210/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0772 - accuracy: 0.9968 - val_loss: 0.1845 - val_accuracy: 0.9615\n",
      "Epoch 211/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0755 - accuracy: 0.9968 - val_loss: 0.1765 - val_accuracy: 0.9615\n",
      "Epoch 212/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0712 - accuracy: 0.9968 - val_loss: 0.1761 - val_accuracy: 0.9615\n",
      "Epoch 213/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0776 - accuracy: 0.9968 - val_loss: 0.1812 - val_accuracy: 0.9615\n",
      "Epoch 214/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0767 - accuracy: 0.9968 - val_loss: 0.1721 - val_accuracy: 0.9615\n",
      "Epoch 215/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0795 - accuracy: 0.9968 - val_loss: 0.1653 - val_accuracy: 0.9615\n",
      "Epoch 216/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0831 - accuracy: 0.9968 - val_loss: 0.1715 - val_accuracy: 0.9615\n",
      "Epoch 217/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0764 - accuracy: 0.9968 - val_loss: 0.1642 - val_accuracy: 0.9615\n",
      "Epoch 218/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0709 - accuracy: 0.9968 - val_loss: 0.1599 - val_accuracy: 0.9615\n",
      "Epoch 219/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0681 - accuracy: 0.9968 - val_loss: 0.1601 - val_accuracy: 0.9615\n",
      "Epoch 220/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0735 - accuracy: 0.9968 - val_loss: 0.1717 - val_accuracy: 0.9615\n",
      "Epoch 221/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0689 - accuracy: 0.9968 - val_loss: 0.1575 - val_accuracy: 0.9615\n",
      "Epoch 222/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0633 - accuracy: 0.9935 - val_loss: 0.1550 - val_accuracy: 0.9615\n",
      "Epoch 223/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0674 - accuracy: 0.9968 - val_loss: 0.1682 - val_accuracy: 0.9615\n",
      "Epoch 224/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0628 - accuracy: 1.0000 - val_loss: 0.1550 - val_accuracy: 0.9615\n",
      "Epoch 225/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0569 - accuracy: 0.9968 - val_loss: 0.1505 - val_accuracy: 0.9744\n",
      "Epoch 226/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0669 - accuracy: 0.9968 - val_loss: 0.1497 - val_accuracy: 0.9615\n",
      "Epoch 227/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0578 - accuracy: 0.9968 - val_loss: 0.1626 - val_accuracy: 0.9615\n",
      "Epoch 228/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0613 - accuracy: 0.9968 - val_loss: 0.1674 - val_accuracy: 0.9615\n",
      "Epoch 229/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0558 - accuracy: 1.0000 - val_loss: 0.1500 - val_accuracy: 0.9615\n",
      "Epoch 230/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0561 - accuracy: 0.9968 - val_loss: 0.1444 - val_accuracy: 0.9615\n",
      "Epoch 231/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0622 - accuracy: 0.9968 - val_loss: 0.1463 - val_accuracy: 0.9615\n",
      "Epoch 232/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0536 - accuracy: 0.9968 - val_loss: 0.1612 - val_accuracy: 0.9615\n",
      "Epoch 233/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0629 - accuracy: 0.9968 - val_loss: 0.1529 - val_accuracy: 0.9615\n",
      "Epoch 234/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0530 - accuracy: 0.9968 - val_loss: 0.1432 - val_accuracy: 0.9615\n",
      "Epoch 235/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0541 - accuracy: 0.9968 - val_loss: 0.1430 - val_accuracy: 0.9615\n",
      "Epoch 236/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0519 - accuracy: 1.0000 - val_loss: 0.1524 - val_accuracy: 0.9615\n",
      "Epoch 237/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.0496 - accuracy: 0.9968 - val_loss: 0.1533 - val_accuracy: 0.9615\n",
      "Epoch 238/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.0510 - accuracy: 0.9968 - val_loss: 0.1523 - val_accuracy: 0.9615\n",
      "Epoch 239/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0505 - accuracy: 1.0000 - val_loss: 0.1504 - val_accuracy: 0.9615\n",
      "Epoch 240/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0543 - accuracy: 0.9968 - val_loss: 0.1385 - val_accuracy: 0.9615\n",
      "Epoch 241/300\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0500 - accuracy: 0.9968 - val_loss: 0.1335 - val_accuracy: 0.9615\n",
      "Epoch 242/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0493 - accuracy: 0.9968 - val_loss: 0.1369 - val_accuracy: 0.9615\n",
      "Epoch 243/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0481 - accuracy: 0.9968 - val_loss: 0.1320 - val_accuracy: 0.9615\n",
      "Epoch 244/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0506 - accuracy: 0.9968 - val_loss: 0.1322 - val_accuracy: 0.9615\n",
      "Epoch 245/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0487 - accuracy: 0.9968 - val_loss: 0.1406 - val_accuracy: 0.9615\n",
      "Epoch 246/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0489 - accuracy: 0.9968 - val_loss: 0.1294 - val_accuracy: 0.9615\n",
      "Epoch 247/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0447 - accuracy: 0.9968 - val_loss: 0.1270 - val_accuracy: 0.9615\n",
      "Epoch 248/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0485 - accuracy: 0.9968 - val_loss: 0.1316 - val_accuracy: 0.9615\n",
      "Epoch 249/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0439 - accuracy: 0.9968 - val_loss: 0.1418 - val_accuracy: 0.9615\n",
      "Epoch 250/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0435 - accuracy: 0.9968 - val_loss: 0.1390 - val_accuracy: 0.9615\n",
      "Epoch 251/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0441 - accuracy: 0.9968 - val_loss: 0.1263 - val_accuracy: 0.9615\n",
      "Epoch 252/300\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0453 - accuracy: 0.9968 - val_loss: 0.1249 - val_accuracy: 0.9615\n",
      "Epoch 253/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0449 - accuracy: 0.9968 - val_loss: 0.1267 - val_accuracy: 0.9615\n",
      "Epoch 254/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.0372 - accuracy: 0.9968 - val_loss: 0.1339 - val_accuracy: 0.9615\n",
      "Epoch 255/300\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0403 - accuracy: 0.9968 - val_loss: 0.1341 - val_accuracy: 0.9615\n",
      "Epoch 256/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.0435 - accuracy: 0.9968 - val_loss: 0.1245 - val_accuracy: 0.9615\n",
      "Epoch 257/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.0432 - accuracy: 0.9968 - val_loss: 0.1223 - val_accuracy: 0.9615\n",
      "Epoch 258/300\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0384 - accuracy: 0.9968 - val_loss: 0.1257 - val_accuracy: 0.9615\n",
      "Epoch 259/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0472 - accuracy: 0.9968 - val_loss: 0.1448 - val_accuracy: 0.9615\n",
      "Epoch 260/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0630 - accuracy: 0.9903 - val_loss: 0.1196 - val_accuracy: 0.9872\n",
      "Epoch 261/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.0762 - accuracy: 0.9838 - val_loss: 0.1361 - val_accuracy: 0.9615\n",
      "Epoch 262/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.0484 - accuracy: 1.0000 - val_loss: 0.1530 - val_accuracy: 0.9615\n",
      "Epoch 263/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0427 - accuracy: 0.9968 - val_loss: 0.1167 - val_accuracy: 0.9615\n",
      "Epoch 264/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0473 - accuracy: 0.9968 - val_loss: 0.1207 - val_accuracy: 0.9615\n",
      "Epoch 265/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0365 - accuracy: 0.9968 - val_loss: 0.1500 - val_accuracy: 0.9615\n",
      "Epoch 266/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0425 - accuracy: 0.9968 - val_loss: 0.1381 - val_accuracy: 0.9615\n",
      "Epoch 267/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0362 - accuracy: 0.9968 - val_loss: 0.1186 - val_accuracy: 0.9615\n",
      "Epoch 268/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0376 - accuracy: 0.9968 - val_loss: 0.1175 - val_accuracy: 0.9615\n",
      "Epoch 269/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0385 - accuracy: 0.9968 - val_loss: 0.1239 - val_accuracy: 0.9615\n",
      "Epoch 270/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0357 - accuracy: 0.9968 - val_loss: 0.1233 - val_accuracy: 0.9615\n",
      "Epoch 271/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0333 - accuracy: 0.9968 - val_loss: 0.1196 - val_accuracy: 0.9615\n",
      "Epoch 272/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0324 - accuracy: 0.9968 - val_loss: 0.1214 - val_accuracy: 0.9615\n",
      "Epoch 273/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0346 - accuracy: 0.9968 - val_loss: 0.1270 - val_accuracy: 0.9615\n",
      "Epoch 274/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0328 - accuracy: 0.9968 - val_loss: 0.1264 - val_accuracy: 0.9615\n",
      "Epoch 275/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0315 - accuracy: 0.9968 - val_loss: 0.1213 - val_accuracy: 0.9615\n",
      "Epoch 276/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0327 - accuracy: 0.9968 - val_loss: 0.1133 - val_accuracy: 0.9615\n",
      "Epoch 277/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0335 - accuracy: 0.9968 - val_loss: 0.1072 - val_accuracy: 0.9615\n",
      "Epoch 278/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0332 - accuracy: 0.9968 - val_loss: 0.1058 - val_accuracy: 0.9615\n",
      "Epoch 279/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0373 - accuracy: 0.9968 - val_loss: 0.1188 - val_accuracy: 0.9615\n",
      "Epoch 280/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0371 - accuracy: 0.9968 - val_loss: 0.1341 - val_accuracy: 0.9615\n",
      "Epoch 281/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0405 - accuracy: 0.9968 - val_loss: 0.1073 - val_accuracy: 0.9872\n",
      "Epoch 282/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0718 - accuracy: 0.9871 - val_loss: 0.1268 - val_accuracy: 0.9615\n",
      "Epoch 283/300\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0394 - accuracy: 1.0000 - val_loss: 0.1669 - val_accuracy: 0.9487\n",
      "Epoch 284/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.0361 - accuracy: 0.9968 - val_loss: 0.1119 - val_accuracy: 0.9615\n",
      "Epoch 285/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0395 - accuracy: 0.9935 - val_loss: 0.1076 - val_accuracy: 0.9615\n",
      "Epoch 286/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0350 - accuracy: 0.9968 - val_loss: 0.1376 - val_accuracy: 0.9615\n",
      "Epoch 287/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0347 - accuracy: 0.9968 - val_loss: 0.1296 - val_accuracy: 0.9615\n",
      "Epoch 288/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0314 - accuracy: 1.0000 - val_loss: 0.1064 - val_accuracy: 0.9615\n",
      "Epoch 289/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0288 - accuracy: 1.0000 - val_loss: 0.1042 - val_accuracy: 0.9615\n",
      "Epoch 290/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0323 - accuracy: 0.9968 - val_loss: 0.1083 - val_accuracy: 0.9615\n",
      "Epoch 291/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0277 - accuracy: 1.0000 - val_loss: 0.1180 - val_accuracy: 0.9615\n",
      "Epoch 292/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0277 - accuracy: 1.0000 - val_loss: 0.1262 - val_accuracy: 0.9615\n",
      "Epoch 293/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0276 - accuracy: 1.0000 - val_loss: 0.1163 - val_accuracy: 0.9615\n",
      "Epoch 294/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.1073 - val_accuracy: 0.9615\n",
      "Epoch 295/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0316 - accuracy: 0.9968 - val_loss: 0.1042 - val_accuracy: 0.9615\n",
      "Epoch 296/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0306 - accuracy: 0.9968 - val_loss: 0.1065 - val_accuracy: 0.9615\n",
      "Epoch 297/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0263 - accuracy: 1.0000 - val_loss: 0.1083 - val_accuracy: 0.9615\n",
      "Epoch 298/300\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0262 - accuracy: 0.9968 - val_loss: 0.1041 - val_accuracy: 0.9615\n",
      "Epoch 299/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0267 - accuracy: 0.9968 - val_loss: 0.1012 - val_accuracy: 0.9615\n",
      "Epoch 300/300\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0262 - accuracy: 0.9968 - val_loss: 0.1058 - val_accuracy: 0.9615\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trying optimal CNN\n",
    "\n",
    "from tensorflow.keras.constraints import min_max_norm\n",
    "from tensorflow.keras import layers, models, metrics, optimizers, utils\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "\n",
    "batch_size=100\n",
    "steps=300\n",
    "\n",
    "Y_change=list()\n",
    "\n",
    "#36:0, 37:1\n",
    "for i in Y:\n",
    "    if i==36:\n",
    "        Y_change.append(0)\n",
    "    elif i==37:\n",
    "        Y_change.append(1)\n",
    "\n",
    "Y_change=np.array(Y_change)\n",
    "\n",
    "unique_elements, counts_elements = np.unique(Y_change, return_counts=True)\n",
    "nb_class=max(unique_elements)+1\n",
    "        \n",
    "Y_cat=utils.to_categorical(Y_change, num_classes=nb_class)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_cat, test_size=0.2, random_state=54)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                np.unique(Y_change),\n",
    "                                                 Y_change)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "print(class_weights)\n",
    "input_shape=((1520-744),2,1)\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=52, kernel_size=(6, 2), padding='valid',activation='tanh', kernel_constraint=min_max_norm(min_value=1.0, max_value=1.0),input_shape=input_shape))\n",
    "model.add(layers.MaxPooling2D((2, 1), strides=2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(units=162, activation='tanh'))\n",
    "model.add(layers.Dropout(0.8)\n",
    ")\n",
    "\n",
    "model.add(layers.Dense(nb_class, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate = 0.001),\n",
    "          loss=tf.keras.losses.categorical_crossentropy,\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "history=model.fit(X_train, y_train, batch_size=batch_size, epochs=steps, verbose=1, validation_split=0.2,\n",
    "                 class_weight=class_weights)\n",
    "\n",
    "i = np.arange(steps)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(i, history.history['accuracy'], label='accuracy')\n",
    "plt.plot(i, history.history['val_accuracy'], label='val_accuaracy')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions= model.predict(X_test)\n",
    "\n",
    "predictions = [np.argmax(y, axis=0, out=None) for y in predictions]\n",
    "y_test_cat = [np.argmax(y, axis=0, out=None) for y in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score for each class\n",
      "[0.93617021 0.94      ]\n",
      "Global f1_score\n",
      "0.9381838122395263\n",
      "Global accuracy\n",
      "0.9381443298969072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2be80fb9d0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD4CAYAAACt8i4nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAPFElEQVR4nO3df4ylVX3H8fdnRiqmaoQomy3Y0Cq2WlOXlFBb+gcVU1c0Aokk0oDbdtPhj5JAYmLBPxRjmphGxH8MyaDETbWYTSuBbKyWrFJCqsiiFJesDUYpIpvd1h9Rk8aWmW//mAd7XYb73Dtzz8zdZ9+v5OTe+9znnudMsvnk7HnOc06qCklSOwvb3QBJGjqDVpIaM2glqTGDVpIaM2glqbEXtL7Ad9/5bqc16Dn+7Hdfu91N0Bz6l/fflM3W8fgfvWXizDnvgS9u+nqTsEcrSY0179FK0pbK/PUfDVpJg5JFg1aS2rJHK0mNZUvub03FoJU0LAvzF7Tz18eWpE1IMnGZsL7FJN9IcqD7fHOS7yd5pCuX9tVhj1bSsCzMvP94PXAEeOnIsVur6iMTN2nWLZKkbbWwMHnpkeQc4G3AJzbVpM38WJLmTRYWJi/JUpJDI2XphOo+BrwXWD3h+HVJHk1yR5Iz+tpk0Eoalil6tFW1XFUXjJTlZ6tJ8nbgeFU9fMIVbgNeBewCjgK39DXJMVpJwzK76V0XAe/obnadDrw0yaer6ur/v1RuBw70VWSPVtKgzGrWQVXdVFXnVNW5wLuAL1XV1Ul2jpx2BXC4r032aCUNy+Ji6yv8bZJdQAFPANf2/cCglTQsDR5YqKr7gPu699dM+3uDVtKgTPogwlYyaCUNi4vKSFJjc7jWgUEraVAy+0dwN82glTQsBq0kNWbQSlJbzjqQpNa8GSZJjTm9S5LachdcSWrNMVpJasxZB5LUlg8sSFJrDh1IUmNzGLTz18eWpE3I4uLEZaL6ksUk30hyoPt8ZpJ7kzzevbo5o6RTTDJ5mcz1wJGRzzcCB6vqPOBg93ksg1bSsCxk8tIjyTnA24BPjBy+DNjXvd8HXN7bpA38GZI0v7IwcUmylOTQSFk6obaPAe8FVkeO7aiqowDd61l9TfJmmKRByRRrHVTVMrC8bj3J24HjVfVwkos30yaDVtKwzG4e7UXAO5JcCpwOvDTJp4FjSXZW1dFu6/HjvU2aVYskaR5kYWHiMk5V3VRV51TVucC7gC9V1dXAPcCe7rQ9wN19bbJHK2lY2j8Z9mFgf5K9wJPAlX0/MGglDUuDBxaq6j7gvu79D4BLpvm9QStpWObwyTCDVtKguKiMJLXmwt+S1JhDB5LUlkMHktSamzNKUmNuNy5JbcUxWklqbMIFvbeSQStpWOzRSlJb0yyTuFUMWknD4qwDSWrsZBw6SPLbrO2RczZQwNPAPVV1ZOwPJWk7zOHQwdg+dpK/Bj4LBPga8FD3/s4kvTs/StJWy+ILJi5bpe9Ke4Hfqar/HT2Y5KPAY6wtgPsc3QZnSwB/c/7vc9VvvmYGTZWkCcyoR5vkdOB+4IWsZeU/VNUHktwM/CXwn92p76uqz4+rqy9oV4FfA/7jhOM7+eVdIX/J6IZn333nu6vnGpI0MzN8YOHnwJuq6mdJTgMeSPJP3Xe3VtVHJq2oL2hvAA4meRz4Xnfs14FXA9dN2WhJam9GQVtVBfys+3haVzbUcRwbtFX1hSSvAS5k7WZYgKeAh6pqZSMXlKSmpli9a3SYs7Pc/Y/82e8XgYdZ61x+vKoeTPJW4Lok7wYOAe+pqh+Nu07vaHBVrQJfnbjlkrSNplkmcXSY83m+XwF2JXkZcFeS1wO3AR9irXf7IeAW4C/GXWf+ZvZK0mYsLExeJlRVP2Ztc8bdVXWsqla6TujtrP2Pf3yTNvq3SNJcSiYvY6vJK7qeLEleBLwZ+FaSnSOnXQEc7muST4ZJGpbZ7bCwE9jXjdMuAPur6kCSv0uyi7WhgyeAa/sqMmglDcqsFpWpqkeB89c5fs20dRm0koblZFzrQJJOJnHhb0lqzB6tJDXmerSS1NgcLpNo0EoaFHfBlaTW7NFKUmNbuKD3pOavRZK0CQ4dSFJrDh1IUmP2aCWpMefRSlJbWTRoJamt2S2TODMGraRBmcdZB/MX/ZK0GTPayibJ6Um+luTfkjyW5IPd8TOT3Jvk8e71jN4mzehPk6T5MKOtbICfA2+qqjcAu4DdSd4I3AgcrKrzgIPd57EMWknDspDJyxi15mfdx9O6UsBlwL7u+D7g8r4mOUYraVCyMLuFv7v9wh4GXg18vKoeTLKjqo4CVNXRJGf11WOPVtKwTNGjTbKU5NBIWRqtqttWfBdwDnBhktdvpEn2aCUNyxQPLFTVMrA8wXk/TnIfsBs4lmRn15vdCRzv+709WkmDkoVMXMbWk7wiycu69y8C3gx8C7gH2NOdtge4u69N9mglDcvs5tHuBPZ147QLwP6qOpDkK8D+JHuBJ4Er+yoyaCUNyqx2wa2qR4Hz1zn+A+CSaeoyaCUNi4/gSlJjc/gIrkEraVhc+FuS2orr0UpSYw4dSFJjLvwtSW05dCBJrXkzTJIacx6tJLU1j1vZGLSShsUerSQ1ZtBKUlt9yx9uB4NW0rA4vUuSGvNmmCQ1NodDB/PXx5akTcji4sRlbD3JK5N8OcmRJI8lub47fnOS7yd5pCuX9rXJHq2kQfnv01848bkvGf/1M8B7qurrSV4CPJzk3u67W6vqI5Nex6CVpHVU1VHgaPf+p0mOAGdvpC6HDiSdspIsJTk0Upae57xzWds/7MHu0HVJHk1yR5Iz+q5j0Eo6ZVXVclVdMFKWTzwnyYuBfwRuqKqfALcBrwJ2sdbjvaXvOs2HDm74g99rfQmdhD5x1+e3uwmaR++/abtb8EuSnMZayH6mqj4HUFXHRr6/HTjQV489WklaR9ZWp/kkcKSqPjpyfOfIaVcAh/vq8maYJK3vIuAa4JtJHumOvQ+4KskuoIAngGv7KjJoJWkdVfUAsN7TD1OPezl0IEmN2aOVNCjPvOC07W7Ccxi0kgZltWq7m/AcBq2kQVmp1e1uwnMYtJIGpezRSlJbc5izBq2kYXGMVpIac+hAkhpbWfVmmCQ1ZY9WkhpzjFaSGjNoJakxhw4kqTGDVpIam8dZBy6TKGlQVqsmLuMkeWWSLyc5kuSxJNd3x89Mcm+Sx7tXN2eUdGqpmrz0eAZ4T1W9Fngj8FdJXgfcCBysqvOAg93nsQxaSYNSVROXnnqOVtXXu/c/BY4AZwOXAfu60/YBl/e1yaCVNCjTDB0kWUpyaKQsrVdnknOB84EHgR1VdRTWwhg4q69N3gyTNCirU6xHW1XLwPK4c5K8mLUtx2+oqp+sbY47HYNW0qCsrM5ueleS01gL2c9U1ee6w8eS7Kyqo93W48f76nHoQNKgzGqMNmtd108CR6rqoyNf3QPs6d7vAe7ua5M9WkmDMsMHFi4CrgG+meSR7tj7gA8D+5PsBZ4EruyryKCVNCizWuugqh4Anm9A9pJp6jJoJQ1K4SO4ktTUPD6Ca9BKGhQXlZGkxlZnOL1rVgxaSYPiwt+S1Jg3wySpMcdoJamxWT6COysGraRBcYxWkhpz6ECSGjNoJakxhw4kqTEfwZWkxuzRSlJj8zhG6w4LkgZlVjssACS5I8nxJIdHjt2c5PtJHunKpX31GLSSBmWaXXAn8Clg9zrHb62qXV35fF8lDh1IGpRZjhxU1f3dVuObYo9W0qCsrK5OXJIsJTk0UpYmvMx1SR7thhbO6DvZoJU0KNOM0VbVclVdMFKWJ7jEbcCrgF3AUeCWvh84dCBpUFpP76qqY8++T3I7cKDvNwatpEFpHbRJdlbV0e7jFcDhceeDQStpYGY5jzbJncDFwMuTPAV8ALg4yS6ggCeAa/vqMWglDcosg7aqrlrn8CenrceglTQornUgSY2tzuGeYRue3pXkz8d894u5aU989V83eglJmtosH8Gdlc3Mo/3g830xOjft3Df+4SYuIUnTmcegHTt0kOTR5/sK2DH75kjS5szh3oy9Y7Q7gLcAPzrheADHBCTNnZWVle1uwnP0Be0B4MVV9ciJXyS5r0mLJGkTag5vho0N2qraO+a7P519cyRpc07GoQNJOqnM4w4LBq2kQXHPMElqzB6tJDXmI7iS1Jg9WklqzDFaSWpsHoPWPcMkDcos1zroNl88nuTwyLEzk9yb5PHu1c0ZJZ1aZryozKeA3SccuxE4WFXnAQe7z2MZtJIGZZrtxvtU1f3AD084fBmwr3u/D7i8rx6DVtKgVE1eRtfO7srSBJfY8ezmjN3rWX0/8GaYpEGZ5mZYVS0Dy+1as8aglTQoWzCP9tizW44n2Qkc7/uBQweSBmW1auKyQfcAe7r3e4C7+35gj1bSoMzyEdwkdwIXAy9P8hTwAeDDwP4ke4EngSv76jFoJQ3KLIcOquqq5/nqkmnqMWglDco8Phlm0EoaFBeVkaTG7NFKUmMn3eaMknSyWZnD3RkNWkmD4hitJDXmGK0kNbbqnmGS1Nb89WcNWkkD4xitJDXmduOS1Jg9WklqzFkHktSYPVpJaswerSQ1NuOFv58AfgqsAM9U1QUbqceglTQoDYYO/riq/mszFRi0kgZlHocO3JxR0qBU1cQlyVKSQyNl6cTqgH9O8vA6303MHq2kQZlmlcSqWgaWx5xyUVU9neQs4N4k36qq+6dtkz1aSYMyTY92grqe7l6PA3cBF26kTQatpEFZWV2duIyT5FeTvOTZ98CfAIc30iaHDiQNygxnHewA7koCa1n591X1hY1UZNBKGpRZ7RlWVd8B3jCLugxaSYPinmGS1JhrHUhSYwatJDXmwt+S1NisbobNkkEraVDmca0Dg1bSoMxhzhq0kobFHq0kNbY6hzfDMo9TIYYqyVK3WpD0C/67GD4XldlaG17PUoPmv4uBM2glqTGDVpIaM2i3luNwWo//LgbOm2GS1Jg9WklqzKCVpMYM2i2SZHeSf0/y7SQ3bnd7tP2S3JHkeJIN7UOlk4dBuwWSLAIfB94KvA64KsnrtrdVmgOfAnZvdyPUnkG7NS4Evl1V36mq/wE+C1y2zW3SNquq+4Efbnc71J5BuzXOBr438vmp7pikU4BBuzWyzjHn1UmnCIN2azwFvHLk8znA09vUFklbzKDdGg8B5yX5jSS/ArwLuGeb2yRpixi0W6CqngGuA74IHAH2V9Vj29sqbbckdwJfAX4ryVNJ9m53m9SGj+BKUmP2aCWpMYNWkhozaCWpMYNWkhozaCWpMYNWkhozaCWpsf8DWdV7MPgCFuAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "present_class_list=np.unique(y_test_cat)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat=confusion_matrix(y_test_cat, predictions, labels=present_class_list)\n",
    "\n",
    "#F1-score\n",
    "print(\"f1_score for each class\")\n",
    "f1_each=f1_score(y_test_cat, predictions, average=None, labels=present_class_list)\n",
    "print(f1_each)\n",
    "\n",
    "#F1-score\n",
    "print(\"Global f1_score\")\n",
    "print(f1_score(y_test_cat, predictions, average='weighted'))\n",
    "\n",
    "#accyracy\n",
    "print(\"Global accuracy\")\n",
    "print(accuracy_score(y_test_cat,predictions))\n",
    "\n",
    "import seaborn as sns\n",
    "cmap = sns.diverging_palette(200, 10, as_cmap=True)\n",
    "sns.heatmap(conf_mat, cmap=cmap, xticklabels=present_class_list, yticklabels=present_class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only the payload works just as well as with the full packet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Not surprising since it turns out that the 2 devices had the same address?).//\n",
    "We have to keep in mind that since these data come from two different scenes, we cannot tell wether the classification is actually done on the device, or simply on the situation it was in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6433600\n"
     ]
    }
   ],
   "source": [
    "idata_sc=idata_five[(idata_five['Scene']==36) | (idata_five['Scene']==37) | (idata_five['Scene']==35)]\n",
    "print(len(idata_sc.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35    3354600\n",
      "36    1652920\n",
      "37    1426080\n",
      "Name: Scene, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(idata_sc['Scene'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1520    5976640\n",
      "1880     246280\n",
      "1360     161840\n",
      "880       28160\n",
      "1320      17160\n",
      "1760       3520\n",
      "Name: Len Packet, dtype: int64\n",
      "11    1770600\n",
      "9     1760720\n",
      "15    1515080\n",
      "12    1387200\n",
      "Name: Server_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(idata_sc['Len Packet'].value_counts())\n",
    "\n",
    "print(idata_sc['Server_id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkt_len=1520\n",
    "#using only one pkt length for the time being\n",
    "idata_red=idata_sc[idata_sc['Len Packet']==pkt_len]\n",
    "idata_red=idata_red[idata_red['Server_id']==9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only the payload as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating input for CNN: IQ values for a whole pkt     output: robot_node\n",
    "X=list()\n",
    "Y=list()\n",
    "\n",
    "i=0\n",
    "while i<len(idata_red.index):\n",
    "    data=idata_red.iloc[i+744:i+pkt_len]\n",
    "    if len(data['Time'].unique())==1:\n",
    "        data=np.array(data[['real','im']], dtype='float64')\n",
    "        x=data.reshape((pkt_len-744),2,1)\n",
    "        X.append(x)\n",
    "        Y.append(int(idata_red.iloc[i]['Scene']))  #in each scene a different emitter is used ~ robot_node in that case\n",
    "    else:\n",
    "        print('Missing!!')\n",
    "    i=i+pkt_len\n",
    "\n",
    "X=np.array(X)\n",
    "Y=np.array(Y, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35 36 37]\n",
      "[522 299 242]\n",
      "242\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(Y, return_counts=True)\n",
    "print(unique_elements)\n",
    "print(counts_elements)\n",
    "#balance classes\n",
    "min_samples=min(counts_elements)\n",
    "print(min_samples)\n",
    "\n",
    "thirty_five_index=np.where(Y == 35)\n",
    "thirty_five_index=thirty_five_index[0][:min_samples]\n",
    "X_thirty_five=X[thirty_five_index]\n",
    "Y_thirty_five=Y[thirty_five_index]\n",
    "\n",
    "thirty_six_index=np.where(Y == 36)\n",
    "thirty_six_index=thirty_six_index[0][:min_samples]\n",
    "X_thirty_six=X[thirty_six_index]\n",
    "Y_thirty_six=Y[thirty_six_index]\n",
    "\n",
    "thirty_seven_index=np.where(Y == 37)\n",
    "thirty_seven_index=thirty_seven_index[0][:min_samples]\n",
    "X_thirty_seven=X[thirty_seven_index]\n",
    "Y_thirty_seven=Y[thirty_seven_index]\n",
    "\n",
    "X=np.concatenate((X_thirty_five, X_thirty_six, X_thirty_seven))\n",
    "Y=np.concatenate((Y_thirty_five,Y_thirty_six, Y_thirty_seven))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "{0: 1.0, 1: 1.0, 2: 1.0}\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 4s 820ms/step - loss: 1.0997 - accuracy: 0.3491 - val_loss: 1.0967 - val_accuracy: 0.4741\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.0884 - accuracy: 0.3858 - val_loss: 1.0994 - val_accuracy: 0.2931\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.0849 - accuracy: 0.4030 - val_loss: 1.1012 - val_accuracy: 0.3276\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.0774 - accuracy: 0.3966 - val_loss: 1.0902 - val_accuracy: 0.3793\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.0739 - accuracy: 0.4267 - val_loss: 1.0968 - val_accuracy: 0.3103\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.0634 - accuracy: 0.4547 - val_loss: 1.0973 - val_accuracy: 0.2931\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.0687 - accuracy: 0.4073 - val_loss: 1.0914 - val_accuracy: 0.3017\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.0562 - accuracy: 0.4332 - val_loss: 1.0967 - val_accuracy: 0.3190\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.0567 - accuracy: 0.4246 - val_loss: 1.0947 - val_accuracy: 0.3103\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.0550 - accuracy: 0.4591 - val_loss: 1.0936 - val_accuracy: 0.3362\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.0373 - accuracy: 0.4914 - val_loss: 1.0902 - val_accuracy: 0.3534\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.0413 - accuracy: 0.4375 - val_loss: 1.0921 - val_accuracy: 0.4483\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.0300 - accuracy: 0.4655 - val_loss: 1.0978 - val_accuracy: 0.3190\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.0244 - accuracy: 0.4720 - val_loss: 1.0984 - val_accuracy: 0.3534\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.0223 - accuracy: 0.5151 - val_loss: 1.0889 - val_accuracy: 0.3793\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.0174 - accuracy: 0.5280 - val_loss: 1.0905 - val_accuracy: 0.4828\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.0087 - accuracy: 0.5625 - val_loss: 1.0854 - val_accuracy: 0.3879\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.0006 - accuracy: 0.5409 - val_loss: 1.1052 - val_accuracy: 0.3103\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.0033 - accuracy: 0.5172 - val_loss: 1.0940 - val_accuracy: 0.4310\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.9968 - accuracy: 0.5000 - val_loss: 1.0937 - val_accuracy: 0.5345\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.9902 - accuracy: 0.5582 - val_loss: 1.0946 - val_accuracy: 0.3534\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.9799 - accuracy: 0.5474 - val_loss: 1.0881 - val_accuracy: 0.3448\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.9686 - accuracy: 0.5539 - val_loss: 1.0907 - val_accuracy: 0.4655\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.9625 - accuracy: 0.5431 - val_loss: 1.0916 - val_accuracy: 0.3362\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.9540 - accuracy: 0.6013 - val_loss: 1.0910 - val_accuracy: 0.4569\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.9559 - accuracy: 0.5496 - val_loss: 1.0899 - val_accuracy: 0.4397\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.9559 - accuracy: 0.5216 - val_loss: 1.0890 - val_accuracy: 0.3448\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.9493 - accuracy: 0.5409 - val_loss: 1.0841 - val_accuracy: 0.4914\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.9293 - accuracy: 0.5862 - val_loss: 1.0919 - val_accuracy: 0.4224\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.9319 - accuracy: 0.5733 - val_loss: 1.0827 - val_accuracy: 0.4138\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.9370 - accuracy: 0.5776 - val_loss: 1.0912 - val_accuracy: 0.5000\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.9137 - accuracy: 0.6142 - val_loss: 1.0906 - val_accuracy: 0.4138\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.9102 - accuracy: 0.5991 - val_loss: 1.1001 - val_accuracy: 0.4828\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.9029 - accuracy: 0.6315 - val_loss: 1.0997 - val_accuracy: 0.4828\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.8830 - accuracy: 0.6530 - val_loss: 1.0926 - val_accuracy: 0.4741\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.8753 - accuracy: 0.6746 - val_loss: 1.0917 - val_accuracy: 0.4741\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.8790 - accuracy: 0.6509 - val_loss: 1.0878 - val_accuracy: 0.6034\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.8843 - accuracy: 0.6358 - val_loss: 1.0920 - val_accuracy: 0.4741\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.8635 - accuracy: 0.6164 - val_loss: 1.0882 - val_accuracy: 0.5259\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.8539 - accuracy: 0.6530 - val_loss: 1.0836 - val_accuracy: 0.4224\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.8399 - accuracy: 0.6552 - val_loss: 1.0838 - val_accuracy: 0.4914\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.8326 - accuracy: 0.6595 - val_loss: 1.0798 - val_accuracy: 0.5345\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.8237 - accuracy: 0.7004 - val_loss: 1.1011 - val_accuracy: 0.5172\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.8202 - accuracy: 0.6659 - val_loss: 1.0892 - val_accuracy: 0.4483\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.7976 - accuracy: 0.6983 - val_loss: 1.1106 - val_accuracy: 0.3276\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.8229 - accuracy: 0.6595 - val_loss: 1.0872 - val_accuracy: 0.4741\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.7977 - accuracy: 0.7091 - val_loss: 1.0907 - val_accuracy: 0.5000\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.7915 - accuracy: 0.6961 - val_loss: 1.0815 - val_accuracy: 0.5345\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.7716 - accuracy: 0.7500 - val_loss: 1.0830 - val_accuracy: 0.5259\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.7560 - accuracy: 0.7349 - val_loss: 1.0897 - val_accuracy: 0.4655\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.7595 - accuracy: 0.7392 - val_loss: 1.0769 - val_accuracy: 0.5603\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.7459 - accuracy: 0.7284 - val_loss: 1.0919 - val_accuracy: 0.5603\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7376 - accuracy: 0.7198 - val_loss: 1.0952 - val_accuracy: 0.4397\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.7232 - accuracy: 0.7284 - val_loss: 1.0870 - val_accuracy: 0.5431\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.7174 - accuracy: 0.7263 - val_loss: 1.0949 - val_accuracy: 0.3966\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.7239 - accuracy: 0.7134 - val_loss: 1.0891 - val_accuracy: 0.5000\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.7017 - accuracy: 0.7522 - val_loss: 1.0770 - val_accuracy: 0.4914\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.7036 - accuracy: 0.7478 - val_loss: 1.0769 - val_accuracy: 0.5172\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6791 - accuracy: 0.8297 - val_loss: 1.0615 - val_accuracy: 0.5517\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6640 - accuracy: 0.8017 - val_loss: 1.0662 - val_accuracy: 0.5172\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6603 - accuracy: 0.8147 - val_loss: 1.0610 - val_accuracy: 0.5345\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6493 - accuracy: 0.8190 - val_loss: 1.0671 - val_accuracy: 0.5086\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6392 - accuracy: 0.8103 - val_loss: 1.0637 - val_accuracy: 0.5431\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6380 - accuracy: 0.8039 - val_loss: 1.0592 - val_accuracy: 0.5603\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6240 - accuracy: 0.8297 - val_loss: 1.0566 - val_accuracy: 0.5259\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6048 - accuracy: 0.8448 - val_loss: 1.0524 - val_accuracy: 0.5259\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6045 - accuracy: 0.7888 - val_loss: 1.0615 - val_accuracy: 0.5259\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5945 - accuracy: 0.8039 - val_loss: 1.0518 - val_accuracy: 0.5345\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5820 - accuracy: 0.8082 - val_loss: 1.0588 - val_accuracy: 0.5517\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5873 - accuracy: 0.7909 - val_loss: 1.0576 - val_accuracy: 0.5431\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5744 - accuracy: 0.8147 - val_loss: 1.0536 - val_accuracy: 0.5603\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5791 - accuracy: 0.8017 - val_loss: 1.0472 - val_accuracy: 0.5000\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5610 - accuracy: 0.8297 - val_loss: 1.0386 - val_accuracy: 0.5690\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5412 - accuracy: 0.8427 - val_loss: 1.0495 - val_accuracy: 0.5345\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5392 - accuracy: 0.8491 - val_loss: 1.0405 - val_accuracy: 0.6121\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5333 - accuracy: 0.8427 - val_loss: 1.0397 - val_accuracy: 0.5345\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5263 - accuracy: 0.8470 - val_loss: 1.0372 - val_accuracy: 0.5517\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5157 - accuracy: 0.8578 - val_loss: 1.0412 - val_accuracy: 0.5172\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5052 - accuracy: 0.8470 - val_loss: 1.0561 - val_accuracy: 0.6121\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5161 - accuracy: 0.8405 - val_loss: 1.0530 - val_accuracy: 0.5259\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4967 - accuracy: 0.8728 - val_loss: 1.0635 - val_accuracy: 0.5345\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4877 - accuracy: 0.8534 - val_loss: 1.0424 - val_accuracy: 0.5603\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4792 - accuracy: 0.8815 - val_loss: 1.0407 - val_accuracy: 0.5517\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4837 - accuracy: 0.8642 - val_loss: 1.0292 - val_accuracy: 0.5776\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4631 - accuracy: 0.8922 - val_loss: 1.0317 - val_accuracy: 0.5517\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4601 - accuracy: 0.8664 - val_loss: 1.0292 - val_accuracy: 0.5862\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4535 - accuracy: 0.8879 - val_loss: 1.0423 - val_accuracy: 0.5603\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4466 - accuracy: 0.8793 - val_loss: 1.0245 - val_accuracy: 0.5690\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4435 - accuracy: 0.8642 - val_loss: 1.0248 - val_accuracy: 0.5603\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4410 - accuracy: 0.8815 - val_loss: 1.0253 - val_accuracy: 0.5690\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4554 - accuracy: 0.8470 - val_loss: 1.0275 - val_accuracy: 0.5776\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4258 - accuracy: 0.8793 - val_loss: 1.0417 - val_accuracy: 0.5517\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4254 - accuracy: 0.8491 - val_loss: 1.0295 - val_accuracy: 0.5690\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4193 - accuracy: 0.8728 - val_loss: 1.0421 - val_accuracy: 0.5776\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4212 - accuracy: 0.8599 - val_loss: 1.0405 - val_accuracy: 0.5603\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4212 - accuracy: 0.8491 - val_loss: 1.0396 - val_accuracy: 0.6379\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4056 - accuracy: 0.8858 - val_loss: 1.0367 - val_accuracy: 0.5603\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3987 - accuracy: 0.8858 - val_loss: 1.0409 - val_accuracy: 0.5517\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4085 - accuracy: 0.8793 - val_loss: 1.0399 - val_accuracy: 0.6034\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3916 - accuracy: 0.8793 - val_loss: 1.0434 - val_accuracy: 0.5776\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3936 - accuracy: 0.8836 - val_loss: 1.0363 - val_accuracy: 0.5690\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3804 - accuracy: 0.8728 - val_loss: 1.0430 - val_accuracy: 0.5603\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3940 - accuracy: 0.8793 - val_loss: 1.0373 - val_accuracy: 0.5603\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3744 - accuracy: 0.9095 - val_loss: 1.0447 - val_accuracy: 0.5517\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3619 - accuracy: 0.9009 - val_loss: 1.0396 - val_accuracy: 0.5948\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3785 - accuracy: 0.8728 - val_loss: 1.0431 - val_accuracy: 0.5603\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3646 - accuracy: 0.8944 - val_loss: 1.0765 - val_accuracy: 0.5431\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3599 - accuracy: 0.9116 - val_loss: 1.0541 - val_accuracy: 0.5776\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3487 - accuracy: 0.9095 - val_loss: 1.0718 - val_accuracy: 0.5517\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3619 - accuracy: 0.8922 - val_loss: 1.0610 - val_accuracy: 0.5948\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3587 - accuracy: 0.8901 - val_loss: 1.0694 - val_accuracy: 0.5776\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3622 - accuracy: 0.8750 - val_loss: 1.0656 - val_accuracy: 0.5690\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3462 - accuracy: 0.8858 - val_loss: 1.0728 - val_accuracy: 0.6379\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3501 - accuracy: 0.8772 - val_loss: 1.0746 - val_accuracy: 0.5345\n",
      "Epoch 115/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3469 - accuracy: 0.8858 - val_loss: 1.0734 - val_accuracy: 0.6034\n",
      "Epoch 116/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3496 - accuracy: 0.8750 - val_loss: 1.1023 - val_accuracy: 0.5172\n",
      "Epoch 117/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3359 - accuracy: 0.8815 - val_loss: 1.1136 - val_accuracy: 0.6207\n",
      "Epoch 118/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3668 - accuracy: 0.8534 - val_loss: 1.1348 - val_accuracy: 0.5172\n",
      "Epoch 119/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3443 - accuracy: 0.8642 - val_loss: 1.1017 - val_accuracy: 0.6034\n",
      "Epoch 120/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3448 - accuracy: 0.8858 - val_loss: 1.1011 - val_accuracy: 0.5603\n",
      "Epoch 121/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3170 - accuracy: 0.9159 - val_loss: 1.0918 - val_accuracy: 0.6034\n",
      "Epoch 122/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3187 - accuracy: 0.9009 - val_loss: 1.0981 - val_accuracy: 0.5517\n",
      "Epoch 123/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3220 - accuracy: 0.9052 - val_loss: 1.0936 - val_accuracy: 0.5776\n",
      "Epoch 124/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3157 - accuracy: 0.9073 - val_loss: 1.0903 - val_accuracy: 0.6121\n",
      "Epoch 125/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3076 - accuracy: 0.9332 - val_loss: 1.1090 - val_accuracy: 0.5517\n",
      "Epoch 126/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3198 - accuracy: 0.8750 - val_loss: 1.1233 - val_accuracy: 0.6379\n",
      "Epoch 127/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3170 - accuracy: 0.9030 - val_loss: 1.1469 - val_accuracy: 0.5603\n",
      "Epoch 128/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3099 - accuracy: 0.8922 - val_loss: 1.1496 - val_accuracy: 0.5948\n",
      "Epoch 129/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3031 - accuracy: 0.9030 - val_loss: 1.1971 - val_accuracy: 0.5517\n",
      "Epoch 130/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3116 - accuracy: 0.8944 - val_loss: 1.1476 - val_accuracy: 0.5948\n",
      "Epoch 131/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2956 - accuracy: 0.9310 - val_loss: 1.1449 - val_accuracy: 0.5431\n",
      "Epoch 132/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2875 - accuracy: 0.9310 - val_loss: 1.1336 - val_accuracy: 0.5517\n",
      "Epoch 133/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2855 - accuracy: 0.9461 - val_loss: 1.1289 - val_accuracy: 0.5776\n",
      "Epoch 134/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2979 - accuracy: 0.9138 - val_loss: 1.1394 - val_accuracy: 0.6121\n",
      "Epoch 135/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2824 - accuracy: 0.9353 - val_loss: 1.1375 - val_accuracy: 0.5776\n",
      "Epoch 136/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2778 - accuracy: 0.9332 - val_loss: 1.1485 - val_accuracy: 0.6121\n",
      "Epoch 137/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2971 - accuracy: 0.9030 - val_loss: 1.1434 - val_accuracy: 0.6034\n",
      "Epoch 138/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2920 - accuracy: 0.9138 - val_loss: 1.1716 - val_accuracy: 0.5517\n",
      "Epoch 139/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2712 - accuracy: 0.9203 - val_loss: 1.1439 - val_accuracy: 0.6034\n",
      "Epoch 140/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2941 - accuracy: 0.9052 - val_loss: 1.1557 - val_accuracy: 0.5431\n",
      "Epoch 141/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2736 - accuracy: 0.9397 - val_loss: 1.1532 - val_accuracy: 0.5862\n",
      "Epoch 142/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2600 - accuracy: 0.9483 - val_loss: 1.1552 - val_accuracy: 0.5948\n",
      "Epoch 143/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.2679 - accuracy: 0.9397 - val_loss: 1.1600 - val_accuracy: 0.5517\n",
      "Epoch 144/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2633 - accuracy: 0.9397 - val_loss: 1.1634 - val_accuracy: 0.5862\n",
      "Epoch 145/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2689 - accuracy: 0.9095 - val_loss: 1.1673 - val_accuracy: 0.6034\n",
      "Epoch 146/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2706 - accuracy: 0.9267 - val_loss: 1.1900 - val_accuracy: 0.5431\n",
      "Epoch 147/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2592 - accuracy: 0.9418 - val_loss: 1.1678 - val_accuracy: 0.6121\n",
      "Epoch 148/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2663 - accuracy: 0.9267 - val_loss: 1.1971 - val_accuracy: 0.5431\n",
      "Epoch 149/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2673 - accuracy: 0.9181 - val_loss: 1.1969 - val_accuracy: 0.5517\n",
      "Epoch 150/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2582 - accuracy: 0.9461 - val_loss: 1.2017 - val_accuracy: 0.6034\n",
      "Epoch 151/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2421 - accuracy: 0.9397 - val_loss: 1.2175 - val_accuracy: 0.5603\n",
      "Epoch 152/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2502 - accuracy: 0.9375 - val_loss: 1.2062 - val_accuracy: 0.6207\n",
      "Epoch 153/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2387 - accuracy: 0.9461 - val_loss: 1.2125 - val_accuracy: 0.5603\n",
      "Epoch 154/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2379 - accuracy: 0.9569 - val_loss: 1.2118 - val_accuracy: 0.5690\n",
      "Epoch 155/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2481 - accuracy: 0.9418 - val_loss: 1.2224 - val_accuracy: 0.5517\n",
      "Epoch 156/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2321 - accuracy: 0.9591 - val_loss: 1.2187 - val_accuracy: 0.6121\n",
      "Epoch 157/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2563 - accuracy: 0.9246 - val_loss: 1.2236 - val_accuracy: 0.5690\n",
      "Epoch 158/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2294 - accuracy: 0.9461 - val_loss: 1.2342 - val_accuracy: 0.5776\n",
      "Epoch 159/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2415 - accuracy: 0.9332 - val_loss: 1.2289 - val_accuracy: 0.6034\n",
      "Epoch 160/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2346 - accuracy: 0.9504 - val_loss: 1.2248 - val_accuracy: 0.5776\n",
      "Epoch 161/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2231 - accuracy: 0.9440 - val_loss: 1.2340 - val_accuracy: 0.5948\n",
      "Epoch 162/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2380 - accuracy: 0.9310 - val_loss: 1.2465 - val_accuracy: 0.5431\n",
      "Epoch 163/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2235 - accuracy: 0.9547 - val_loss: 1.2425 - val_accuracy: 0.6034\n",
      "Epoch 164/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.2351 - accuracy: 0.9353 - val_loss: 1.2385 - val_accuracy: 0.5862\n",
      "Epoch 165/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2356 - accuracy: 0.9375 - val_loss: 1.2399 - val_accuracy: 0.6121\n",
      "Epoch 166/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2271 - accuracy: 0.9547 - val_loss: 1.2597 - val_accuracy: 0.5603\n",
      "Epoch 167/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2222 - accuracy: 0.9526 - val_loss: 1.2543 - val_accuracy: 0.5690\n",
      "Epoch 168/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2224 - accuracy: 0.9483 - val_loss: 1.2531 - val_accuracy: 0.6034\n",
      "Epoch 169/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2182 - accuracy: 0.9655 - val_loss: 1.2606 - val_accuracy: 0.5690\n",
      "Epoch 170/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2040 - accuracy: 0.9612 - val_loss: 1.2792 - val_accuracy: 0.6034\n",
      "Epoch 171/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2128 - accuracy: 0.9720 - val_loss: 1.2942 - val_accuracy: 0.5776\n",
      "Epoch 172/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2124 - accuracy: 0.9569 - val_loss: 1.2965 - val_accuracy: 0.5948\n",
      "Epoch 173/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2118 - accuracy: 0.9698 - val_loss: 1.3202 - val_accuracy: 0.5603\n",
      "Epoch 174/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2157 - accuracy: 0.9634 - val_loss: 1.3020 - val_accuracy: 0.5690\n",
      "Epoch 175/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2040 - accuracy: 0.9698 - val_loss: 1.3078 - val_accuracy: 0.5948\n",
      "Epoch 176/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2083 - accuracy: 0.9547 - val_loss: 1.2969 - val_accuracy: 0.5517\n",
      "Epoch 177/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2102 - accuracy: 0.9591 - val_loss: 1.2851 - val_accuracy: 0.5690\n",
      "Epoch 178/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1968 - accuracy: 0.9634 - val_loss: 1.3072 - val_accuracy: 0.5431\n",
      "Epoch 179/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1955 - accuracy: 0.9677 - val_loss: 1.2686 - val_accuracy: 0.6121\n",
      "Epoch 180/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1958 - accuracy: 0.9655 - val_loss: 1.2893 - val_accuracy: 0.5776\n",
      "Epoch 181/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1920 - accuracy: 0.9655 - val_loss: 1.2927 - val_accuracy: 0.5948\n",
      "Epoch 182/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1868 - accuracy: 0.9784 - val_loss: 1.2982 - val_accuracy: 0.5603\n",
      "Epoch 183/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1812 - accuracy: 0.9634 - val_loss: 1.3061 - val_accuracy: 0.6121\n",
      "Epoch 184/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1855 - accuracy: 0.9720 - val_loss: 1.3142 - val_accuracy: 0.5690\n",
      "Epoch 185/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1881 - accuracy: 0.9784 - val_loss: 1.3161 - val_accuracy: 0.5948\n",
      "Epoch 186/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1914 - accuracy: 0.9784 - val_loss: 1.3269 - val_accuracy: 0.5948\n",
      "Epoch 187/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1806 - accuracy: 0.9871 - val_loss: 1.3169 - val_accuracy: 0.5862\n",
      "Epoch 188/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1782 - accuracy: 0.9871 - val_loss: 1.3349 - val_accuracy: 0.5862\n",
      "Epoch 189/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1835 - accuracy: 0.9655 - val_loss: 1.3343 - val_accuracy: 0.5776\n",
      "Epoch 190/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1838 - accuracy: 0.9763 - val_loss: 1.3305 - val_accuracy: 0.6293\n",
      "Epoch 191/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1796 - accuracy: 0.9677 - val_loss: 1.3563 - val_accuracy: 0.5603\n",
      "Epoch 192/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1701 - accuracy: 0.9806 - val_loss: 1.3547 - val_accuracy: 0.5948\n",
      "Epoch 193/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1629 - accuracy: 0.9892 - val_loss: 1.3449 - val_accuracy: 0.5690\n",
      "Epoch 194/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1702 - accuracy: 0.9720 - val_loss: 1.3544 - val_accuracy: 0.5603\n",
      "Epoch 195/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1611 - accuracy: 0.9784 - val_loss: 1.3539 - val_accuracy: 0.6121\n",
      "Epoch 196/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1722 - accuracy: 0.9763 - val_loss: 1.3485 - val_accuracy: 0.5776\n",
      "Epoch 197/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1666 - accuracy: 0.9634 - val_loss: 1.3613 - val_accuracy: 0.5690\n",
      "Epoch 198/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1636 - accuracy: 0.9828 - val_loss: 1.3710 - val_accuracy: 0.5776\n",
      "Epoch 199/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1739 - accuracy: 0.9720 - val_loss: 1.3575 - val_accuracy: 0.6207\n",
      "Epoch 200/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1547 - accuracy: 0.9849 - val_loss: 1.3742 - val_accuracy: 0.5690\n",
      "Epoch 201/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1584 - accuracy: 0.9677 - val_loss: 1.3762 - val_accuracy: 0.6293\n",
      "Epoch 202/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1613 - accuracy: 0.9677 - val_loss: 1.3980 - val_accuracy: 0.5603\n",
      "Epoch 203/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1748 - accuracy: 0.9504 - val_loss: 1.3965 - val_accuracy: 0.5948\n",
      "Epoch 204/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1535 - accuracy: 0.9806 - val_loss: 1.4056 - val_accuracy: 0.5603\n",
      "Epoch 205/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1722 - accuracy: 0.9634 - val_loss: 1.4072 - val_accuracy: 0.5603\n",
      "Epoch 206/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1623 - accuracy: 0.9612 - val_loss: 1.4090 - val_accuracy: 0.5776\n",
      "Epoch 207/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1528 - accuracy: 0.9892 - val_loss: 1.3953 - val_accuracy: 0.5776\n",
      "Epoch 208/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1489 - accuracy: 0.9806 - val_loss: 1.4234 - val_accuracy: 0.5948\n",
      "Epoch 209/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1639 - accuracy: 0.9634 - val_loss: 1.4462 - val_accuracy: 0.5517\n",
      "Epoch 210/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1585 - accuracy: 0.9763 - val_loss: 1.4079 - val_accuracy: 0.6207\n",
      "Epoch 211/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1581 - accuracy: 0.9720 - val_loss: 1.4422 - val_accuracy: 0.6034\n",
      "Epoch 212/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1514 - accuracy: 0.9720 - val_loss: 1.4466 - val_accuracy: 0.5603\n",
      "Epoch 213/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1527 - accuracy: 0.9806 - val_loss: 1.4484 - val_accuracy: 0.6034\n",
      "Epoch 214/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1381 - accuracy: 0.9978 - val_loss: 1.4599 - val_accuracy: 0.5690\n",
      "Epoch 215/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1427 - accuracy: 0.9784 - val_loss: 1.4479 - val_accuracy: 0.6207\n",
      "Epoch 216/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1353 - accuracy: 0.9828 - val_loss: 1.4684 - val_accuracy: 0.5776\n",
      "Epoch 217/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1378 - accuracy: 0.9892 - val_loss: 1.4668 - val_accuracy: 0.6293\n",
      "Epoch 218/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1290 - accuracy: 0.9871 - val_loss: 1.4711 - val_accuracy: 0.5862\n",
      "Epoch 219/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1362 - accuracy: 0.9871 - val_loss: 1.4687 - val_accuracy: 0.5948\n",
      "Epoch 220/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1309 - accuracy: 0.9828 - val_loss: 1.4679 - val_accuracy: 0.5862\n",
      "Epoch 221/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1309 - accuracy: 0.9935 - val_loss: 1.4811 - val_accuracy: 0.5690\n",
      "Epoch 222/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1420 - accuracy: 0.9763 - val_loss: 1.4742 - val_accuracy: 0.5862\n",
      "Epoch 223/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1355 - accuracy: 0.9849 - val_loss: 1.4873 - val_accuracy: 0.6207\n",
      "Epoch 224/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1342 - accuracy: 0.9828 - val_loss: 1.4831 - val_accuracy: 0.5862\n",
      "Epoch 225/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1259 - accuracy: 0.9871 - val_loss: 1.4853 - val_accuracy: 0.5862\n",
      "Epoch 226/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1323 - accuracy: 0.9849 - val_loss: 1.5127 - val_accuracy: 0.5776\n",
      "Epoch 227/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1204 - accuracy: 0.9914 - val_loss: 1.4902 - val_accuracy: 0.5776\n",
      "Epoch 228/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1300 - accuracy: 0.9871 - val_loss: 1.4958 - val_accuracy: 0.5862\n",
      "Epoch 229/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.1406 - accuracy: 0.9871 - val_loss: 1.5152 - val_accuracy: 0.6207\n",
      "Epoch 230/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1169 - accuracy: 0.9957 - val_loss: 1.5080 - val_accuracy: 0.5862\n",
      "Epoch 231/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1347 - accuracy: 0.9914 - val_loss: 1.4848 - val_accuracy: 0.5862\n",
      "Epoch 232/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1172 - accuracy: 0.9892 - val_loss: 1.4464 - val_accuracy: 0.5862\n",
      "Epoch 233/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1260 - accuracy: 0.9784 - val_loss: 1.4390 - val_accuracy: 0.5862\n",
      "Epoch 234/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1178 - accuracy: 0.9935 - val_loss: 1.4532 - val_accuracy: 0.5776\n",
      "Epoch 235/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1146 - accuracy: 0.9978 - val_loss: 1.4391 - val_accuracy: 0.5776\n",
      "Epoch 236/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1229 - accuracy: 0.9849 - val_loss: 1.4445 - val_accuracy: 0.5776\n",
      "Epoch 237/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1061 - accuracy: 0.9957 - val_loss: 1.4514 - val_accuracy: 0.6207\n",
      "Epoch 238/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1236 - accuracy: 0.9914 - val_loss: 1.4594 - val_accuracy: 0.5776\n",
      "Epoch 239/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1045 - accuracy: 0.9935 - val_loss: 1.4640 - val_accuracy: 0.6121\n",
      "Epoch 240/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1091 - accuracy: 0.9892 - val_loss: 1.4842 - val_accuracy: 0.5776\n",
      "Epoch 241/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1082 - accuracy: 0.9957 - val_loss: 1.4724 - val_accuracy: 0.5776\n",
      "Epoch 242/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1088 - accuracy: 0.9892 - val_loss: 1.4756 - val_accuracy: 0.5862\n",
      "Epoch 243/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1045 - accuracy: 0.9957 - val_loss: 1.4906 - val_accuracy: 0.5776\n",
      "Epoch 244/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1174 - accuracy: 0.9828 - val_loss: 1.4963 - val_accuracy: 0.5862\n",
      "Epoch 245/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0995 - accuracy: 1.0000 - val_loss: 1.5036 - val_accuracy: 0.6293\n",
      "Epoch 246/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1027 - accuracy: 0.9978 - val_loss: 1.5105 - val_accuracy: 0.5776\n",
      "Epoch 247/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.1013 - accuracy: 0.9978 - val_loss: 1.5151 - val_accuracy: 0.6034\n",
      "Epoch 248/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1091 - accuracy: 0.9978 - val_loss: 1.5232 - val_accuracy: 0.5776\n",
      "Epoch 249/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1034 - accuracy: 0.9914 - val_loss: 1.5288 - val_accuracy: 0.6034\n",
      "Epoch 250/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0936 - accuracy: 0.9978 - val_loss: 1.5375 - val_accuracy: 0.5690\n",
      "Epoch 251/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1067 - accuracy: 0.9957 - val_loss: 1.5471 - val_accuracy: 0.5862\n",
      "Epoch 252/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0991 - accuracy: 0.9978 - val_loss: 1.5502 - val_accuracy: 0.6207\n",
      "Epoch 253/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0973 - accuracy: 0.9935 - val_loss: 1.5646 - val_accuracy: 0.5862\n",
      "Epoch 254/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0994 - accuracy: 0.9957 - val_loss: 1.5494 - val_accuracy: 0.5862\n",
      "Epoch 255/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0997 - accuracy: 0.9914 - val_loss: 1.5597 - val_accuracy: 0.5776\n",
      "Epoch 256/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0947 - accuracy: 0.9914 - val_loss: 1.5650 - val_accuracy: 0.6293\n",
      "Epoch 257/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0932 - accuracy: 0.9935 - val_loss: 1.5647 - val_accuracy: 0.5948\n",
      "Epoch 258/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0945 - accuracy: 0.9978 - val_loss: 1.5801 - val_accuracy: 0.5948\n",
      "Epoch 259/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0951 - accuracy: 0.9935 - val_loss: 1.5796 - val_accuracy: 0.6034\n",
      "Epoch 260/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0875 - accuracy: 0.9978 - val_loss: 1.5810 - val_accuracy: 0.5776\n",
      "Epoch 261/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0869 - accuracy: 0.9978 - val_loss: 1.5857 - val_accuracy: 0.6121\n",
      "Epoch 262/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0856 - accuracy: 0.9978 - val_loss: 1.6002 - val_accuracy: 0.5690\n",
      "Epoch 263/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0898 - accuracy: 0.9935 - val_loss: 1.5954 - val_accuracy: 0.5862\n",
      "Epoch 264/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0891 - accuracy: 0.9935 - val_loss: 1.6012 - val_accuracy: 0.5776\n",
      "Epoch 265/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0877 - accuracy: 0.9935 - val_loss: 1.6261 - val_accuracy: 0.5948\n",
      "Epoch 266/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0931 - accuracy: 0.9957 - val_loss: 1.6160 - val_accuracy: 0.6034\n",
      "Epoch 267/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0843 - accuracy: 1.0000 - val_loss: 1.6143 - val_accuracy: 0.5862\n",
      "Epoch 268/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0936 - accuracy: 0.9935 - val_loss: 1.6191 - val_accuracy: 0.6034\n",
      "Epoch 269/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0855 - accuracy: 1.0000 - val_loss: 1.6338 - val_accuracy: 0.5690\n",
      "Epoch 270/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0818 - accuracy: 0.9957 - val_loss: 1.6381 - val_accuracy: 0.6293\n",
      "Epoch 271/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0857 - accuracy: 1.0000 - val_loss: 1.6432 - val_accuracy: 0.5776\n",
      "Epoch 272/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0827 - accuracy: 0.9978 - val_loss: 1.6508 - val_accuracy: 0.5862\n",
      "Epoch 273/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0760 - accuracy: 1.0000 - val_loss: 1.6525 - val_accuracy: 0.5776\n",
      "Epoch 274/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0874 - accuracy: 0.9935 - val_loss: 1.6487 - val_accuracy: 0.5948\n",
      "Epoch 275/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0742 - accuracy: 1.0000 - val_loss: 1.6578 - val_accuracy: 0.5776\n",
      "Epoch 276/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0785 - accuracy: 1.0000 - val_loss: 1.6510 - val_accuracy: 0.5862\n",
      "Epoch 277/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0742 - accuracy: 1.0000 - val_loss: 1.6602 - val_accuracy: 0.6034\n",
      "Epoch 278/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0686 - accuracy: 1.0000 - val_loss: 1.6762 - val_accuracy: 0.5862\n",
      "Epoch 279/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0741 - accuracy: 0.9978 - val_loss: 1.6637 - val_accuracy: 0.6293\n",
      "Epoch 280/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0804 - accuracy: 1.0000 - val_loss: 1.6723 - val_accuracy: 0.5776\n",
      "Epoch 281/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0706 - accuracy: 1.0000 - val_loss: 1.6735 - val_accuracy: 0.6293\n",
      "Epoch 282/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0745 - accuracy: 1.0000 - val_loss: 1.7160 - val_accuracy: 0.5690\n",
      "Epoch 283/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0758 - accuracy: 1.0000 - val_loss: 1.6923 - val_accuracy: 0.6207\n",
      "Epoch 284/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0702 - accuracy: 0.9978 - val_loss: 1.6958 - val_accuracy: 0.5862\n",
      "Epoch 285/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0668 - accuracy: 0.9978 - val_loss: 1.7001 - val_accuracy: 0.5948\n",
      "Epoch 286/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0699 - accuracy: 0.9978 - val_loss: 1.7037 - val_accuracy: 0.5948\n",
      "Epoch 287/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0677 - accuracy: 0.9978 - val_loss: 1.7007 - val_accuracy: 0.5948\n",
      "Epoch 288/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0673 - accuracy: 1.0000 - val_loss: 1.6949 - val_accuracy: 0.5948\n",
      "Epoch 289/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0687 - accuracy: 1.0000 - val_loss: 1.7085 - val_accuracy: 0.6121\n",
      "Epoch 290/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0653 - accuracy: 0.9978 - val_loss: 1.7287 - val_accuracy: 0.5776\n",
      "Epoch 291/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0672 - accuracy: 1.0000 - val_loss: 1.7090 - val_accuracy: 0.6121\n",
      "Epoch 292/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0689 - accuracy: 0.9978 - val_loss: 1.7240 - val_accuracy: 0.5776\n",
      "Epoch 293/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0682 - accuracy: 0.9978 - val_loss: 1.7192 - val_accuracy: 0.6207\n",
      "Epoch 294/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0669 - accuracy: 0.9978 - val_loss: 1.7327 - val_accuracy: 0.5776\n",
      "Epoch 295/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0673 - accuracy: 1.0000 - val_loss: 1.7515 - val_accuracy: 0.5690\n",
      "Epoch 296/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0641 - accuracy: 1.0000 - val_loss: 1.7485 - val_accuracy: 0.5862\n",
      "Epoch 297/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0633 - accuracy: 0.9978 - val_loss: 1.7476 - val_accuracy: 0.6034\n",
      "Epoch 298/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0617 - accuracy: 0.9978 - val_loss: 1.7515 - val_accuracy: 0.5948\n",
      "Epoch 299/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0620 - accuracy: 1.0000 - val_loss: 1.7548 - val_accuracy: 0.5948\n",
      "Epoch 300/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0584 - accuracy: 1.0000 - val_loss: 1.7679 - val_accuracy: 0.5776\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEeCAYAAACHXhKxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOydd3hb1dnAf0eSl7yHvO04y06cEGeREAKEESCsMsLHLhsKhbZAS8sHLaVfdxktAcooe5XSsGdYCStkk72HE+89ZFu21v3+OLpalhybRCS2z+95/Ei699x7z72S3/e847xHaJqGQqFQKIYvhkPdAYVCoVAcWpQiUCgUimGOUgQKhUIxzFGKQKFQKIY5ShEoFArFMEcpAoVCoRjmmA51BwbK6tWrM00m05PARJQiUygUiv3hBjY6nc5rp02bVh+qwaBTBCaT6cns7OzxFoulxWAwqEkQCoVC0Qdut1s0NDSU1tbWPgn8IFSbwTiinmixWNqVElAoFIr9YzAYNIvF0ob0ooRu8z3252BhUEpAoVAo+o9HZoaV94NRESgUCoXiIKIUwWGMw+E41F1QKIYUZrN5yqHuw+GIUgTfkblz546eMGHC+DFjxky47777MgAWLlyYVFpaOr6kpKR01qxZxQBtbW2G888/v6i4uLi0uLi49Nlnn02BwB/kM888kzp//vwigPnz5xdde+21+TNnziz+8Y9/nL948WLzlClTxo0fP750ypQp49atWxcD4HQ6uf766/P18/7xj3/MfOuttxJPPvnk0fp533jjjaRTTjllNAqFYlBwqAZ/gy5r6HDhpZdeKs/KynJ1dHSIKVOmlF544YWtN998c9GSJUu2jhs3zl5XV2cEuOOOO3KSkpJc27dv3wzQ0NBg3N+5d+3aFfv1119vN5lMNDc3G1asWLE1KiqKN998M/GXv/xl/qJFi3bdf//9lr1798Zs2rRpc1RUFHV1dUaLxeK65ZZbCqurq025ubnOp59+Ov3KK69sjPSzUCgOFTfeeGPeiBEj7HfccUcDwG233ZYrhNCWLl2a2NbWZnQ6neLuu++uvuyyy1r3d662tjbDvHnzxoQ67uGHH05fsGBBlhCC8ePH29588809FRUVpquvvnrEvn37Yjxt9hYWFjrOPPPMsTt27NgEcPfdd2d1dHQYH3jgger7778/45lnnrE4HA5RVFTUs3Dhwj2JiYnu+fPnF6Wmpjo3bNhgnjRpUtcll1zSfNtttxV2d3cbYmNj3c8+++yesrKyHqfTyY9//OP8JUuWJAFcccUVjRMnTrQ9/PDDmR9//PEukIO/Rx991PLRRx/tGshzHNSK4PaF6wq211rNB/OcxdmJXfeeX1axv3Z//etfs957770UgNra2qgFCxZYZsyYYR03bpwdICsrywXwxRdfJL3yyiu79eMsFotrf+c+77zzWkwm+dU0NzcbL7zwwpHl5eWxQgjN4XAIgM8++yzphhtuaIiKisL/ehdccEHTv/71r7Sbbrqpac2aNQmvv/76noE+A4XiO/HmTQXUbz6o/49klnZxziNh/x8vu+yy5ltuuaVQVwRvvfVW6ocffrjjrrvuqktLS3PX1NSYZs6cOe6SSy5pNRj6doCYzWb3e++9tzP4uDVr1sTed999Od98883WnJwcpz7Iu+GGGwqPPfZY6913373L6XTS1tZmbGxsDDvQu/TSS1t+/vOfNwL89Kc/zV2wYEHGXXfdVQ+HfvA3qBXBoeLdd99N/PzzzxNXrVq1NTEx0T1jxoySyZMnd23fvj02uK2maQghep3Df5vNZgtokJCQ4Nbf/+pXv8qbM2eO9eOPP961bdu26BNPPLHE77y9sqduvPHGpjPOOGNMbGysdtZZZ7XoikKhGIrMnj3b1tTUZCovL4+qqakxJScnuwoLCx3XXXddwbJlyxIMBgP19fXRlZWVpsLCQmdf53K73eKWW27JDz5u0aJFSWeddVZLTk6OE3yDrqVLlyYuXLhwD4DJZCI9Pd3VlyJYvXp13N13351ntVqNnZ2dxjlz5rTp+w714G9QK4L+jNwjQWtrqzE5OdmVmJjo/vbbb2PXrVsX39PTY1i+fHni1q1bo3XXUFZWluv4449vf+CBBzKffvrpCpCuIYvF4kpPT3esWbMmtqysrPutt95KTUhICGkptLe3G/Pz8+0Ajz/+eIa+fe7cue2PPfaY5YwzzrDqo4OsrCxXUVGRIysry3H//ffnfPDBB9u/nyeiUEBfI/dIctZZZ7W8+OKLqbW1tVHz589vfvzxx9OamppMGzZs2BITE6Pl5eUdYbPZ9hsPDXdcuEFXKEwmk+Z2e8dxdHd3e697/fXXj1y4cOHOWbNm2RYsWJD++eefJ+r7DvXgTwWLvwPz589vczqdori4uPTOO+/MLSsr68zMzHQuWLCg/Nxzzx1TUlJSeu65544C+POf/1zT2tpqHDt27ISSkpLS999/PxHgd7/7XdXZZ589ZtasWSVZWVlhI0S/+tWvau+55578qVOnjnO5fLri1ltvbcjPz7ePGzduQklJSelTTz2Vpu+76KKLmnJycuzTpk3rjuBjUCgOC374wx82v/baa2nvvvtu6mWXXdbS1tZmzMjIcMTExGjvvPNOYnV1dXR/zhPuuHnz5rW//fbbabW1tUYA3TU0e/Zs67333msBmbzR3NxsyM/PdzY3N5tqa2uNNptNLFq0KFk/f1dXl6GwsNDR09MjXnnllbRQfYD9D/70gLLeD//B33XXXfedYoJisC1VuW7duvKysjIVAO2Dyy+/vHDKlCldt956q3pOimFBcXFxaWpqqnP58uXba2pqTKeddtoYp9MpJkyY0LVy5cqEDz74YEdJSYndbDZP6erq+jbUOfo67qGHHkpfsGBBtsFg0CZOnNj12muvlVdUVJiuvPLKERUVFTEGg4GHH35479y5czv/8Ic/ZD7xxBOZ+fn5PTk5OY4RI0bYH3jggeq//vWvlgULFmTn5eXZx48f39XR0WF87bXXyufPn1905plntl111VUtAJ988kn8tddeOzItLc157LHHti9cuDC9qqpqg8Ph4MYbb8xfvHhxsslk0q644oqGO++8swHgiSeeSH3kkUey1q1btzXcM1q3bl1GWVlZUah9ShEMMSZMmDA+Li7O/eWXX26Pi4sbXF+uQqH4TvRn8NeXIhjUMQJFbzZt2rTlUPdBoVB8f+iDv8cff/w7x2iUIlAoFMOKFStWxF1++eUj/bdFR0e7169fH9atcjhzMAZ/ShEoFIphxYwZM2xbt27dfKj7cTgxGLOG3G63u3divkKhUChC4pGZ7nD7B6Mi2NjQ0JCslIFCoVDsH8/CNMnAxnBtBp1ryOl0XltbW/tkbW2tWqpSoVAo9o93qcpwDQZd+qhCoVAoDi5qRK1QKBTDHKUIFAqFYpgz6GIEGRkZWlFR0aHuhkKhUAwqVq9e3ahpmiXUvkGnCIqKili1atWh7oZCoVAMKoQQe8PtU64hhUKhGOYoRaBQKBTDHKUIFAqFYpijFIFCoVAMcyKmCIQQTwsh6oUQIac1C8kCIcROIcR6IcTUSPVFoVAoFOGJpEXwLDCvj/2nAWM9f9cDj0awLwqFQqEIQ8QUgaZpXwDNfTQ5G3hekywDUoQQOZHqj0KhUChCcyjnEeQB/ivqVHq21Rya7igUisHMp1vqSE+IYXJBykE97+6GDnbWd1Cam8TailbK8lP4ckcjE/OS2FnfwfElmfx7xT56HC4AEmOjOGZsBp9srsPhkpWfY6ONXDC9gIyEGABaOu38Z1UFDqebuaVZrK9sZebIdIoy4umyO/nPygpauxycdkQ2IzPi+e+qSurbu5lelMZxxSHnhB0Qh1IRhCojHbICnhDieqT7iMLCwkj2SaFQfEc+3FjLf1bu4+krj0SI71YlfkedldGWBAwGefzuhg5++sq3/P7siSTHRVGQZibKKB0Z5Y2d5KXGsbepkxRzNNc8JyeaZiTEYBBw0wljWLGnGYNB0NzZw7jsJC6fNYK4KCOZSbG43Rp3v72RLruLlLho3llfDcDUwhQWXDyFGJORypYuLnxiGQ3WHorSzZQ3dZEUa6K92+nt86iMeHY3dqLfsn8dT/9tD36yg6S4KACs3Q66HVJJ3P/xdgBMBkFqfDRdPU467VKpPPTZDuKjTVh7nAgBN8wZPeQUQSVQ4Pc5H6gO1VDTtCeAJwCmT5+uyqUqFIchb6+rYvG2BipbbBSkmQP27W3q5Jmvy7nrjPFeQa7jcLmJMhr4ZHMd1z6/iuuOHcldZ5Ris7u4/OkVVLbY+PMHW1lV3syJ4zKxJMZQlp/CXW9u5OjR6Xyzq4lJ+cmAFOIl2Ulsqm7jnnc2oWlgEBBjMvL1ziae/noP2UmxPHPVkby0bB8vLtvn7cfc8ZkkxkbxxrdVzH90KZmJsazc04wGpMVHU97URVG6mZq2bn5/9gQ67S5eXVXB7oZOfn5yMT85aSwAG6vaWLKtnvOm5pObEgfAroYOXly21yv8Y0wGLp5RSGKsidfXVFJWkMLXO5toszkwGuDsyXmMzUzg6a/20NRpZ97EbI4de/AVgE5Ey1ALIYqAdzVNmxhi3xnAzcDpwExggaZpM/Z3zunTp2uqxIRCcWB8vr2B/NQ4RlsSBnTcjjorde09HDM2o9e+2X/5jKpWG4//cBpTClJYvK2esyfnERtl5DdvbuSFZXt57cajmTYi1XvM4m313PTSGu49v4yXV+xl6a4mNA3OKsvltInZ/PilNQPq36pfzyUjIYaqVhsn3reE0ZYE/n39UUQbDdy7aBudPU7eWV9Nl2fEfc0xI0lPiKaty8Edp41DCMEL35Tzn1XSa12YZuYXp5Swp7GTT7fW87sfTMDa7SQtPhqA1XubeXHZPv507hHERRsH1NfvGyHEak3TpofcFylFIIT4N3A8kAHUAb8FogA0TXtMSNvxYWRmURdwlaZp+5XwShEoFP3D4XJz6ZPLqWzu4tdnlnL6ETIXo7Gjh6P//BknjLPw+A97y4V6azcGIbz+bH+m/+FjGjvsrL37ZFLM0QHHzPjjpwD87KSxrK9sZfG2BhJjTYyyJFDZ3EVTp51fnzGeo0al02Zz8Kf3t9DcaaemrRuTQeB0a9x2cjFOt8aCT3eQkRBNa5eD208t4c8fbGXmyDRumDOa+BgTty9cx+Wzinh1ZQVx0UbWVrSSnxrHV7860dunTdVtWBJjyEyMDbiHrbXtrN7bwsTcZMoOcjzhcKYvRRAx15CmaRfvZ78G3BSp6ysUw52PNtWxYk+z532tVxG8smIfdpebdRVtAe3/u6qC+BgTj3+xm/hoIy9fd5R3n6ZpCCFo6XIA8OqqCq4/brR3/3rPuUwGwT+X7MTh0rhwegEGg+CdddV09Eif+j8+2UFHzxYMAqKMBnqcbu77nzK21LTjcmtccXQRyXFRfLy5ji017UwpTOH0I3K4d9E2zpuaxwnjMgFY8ovjEUJw9ewidtZ3cPLfv+gl1CfkJod8LuOykxiXnfSdn+tQZNBVH1UoFL3RNI1319dw3FgLyeYo3t9Qw0Of7aAgLY6i9Hi213V42/17RQVGg6C2vZs3vq2kINXM9KI07vtoGw6XRnOnndgoA06XG5PRwG/e3Mi+5i6eu3oG5mgj1m4nr6wMVAQrypsxGgSTC1JYtbeF4qwE/u+cCcSYjJw5KYf/rKygvdvBkm0NFKaZKcqI54/nTMRgEOR5/Oj+nDkphy017cwYmUZBmpkvf3UC2Um+kb0ejBZCMCYzgf+Zls8Zk1T2+XdFKQKFYgiwpcbKT/79LbfOLWZyYYrXt/77cyZS0dzFs0vLueGF1YxIN1PVauOC6fm8uqqSW/+zDoDrjxtFXXuP93zdDjfb62TK5LcVLWyrtVJv7cba7SQjIZrdDZ088cUuVuxp5r7/KWPh6krmjs9k7vgsypu6eOqKI4kxSZ/57DEZzB6TwaNLdrFkWwM/P6WYsyfn9Xk/Z0/O5ZmvyzmlNBuAnOTeykJHCMG9/1N2QM9vuKMUgUJxmNLcaeehz3bwi1NKiI0yYjQIXG4No6F3aubn2xsAWLa7ifc31DAi3cyHPzuOuGgj/11Vgd3p5sNNtd72Pz5+DK+uqgSgrCCFf32527svxiRdNusqWynNTaKqxYbDpfHRpjoAzpyUy7NLy/nT+1sBOGPBVzR32rl8VhGzx2Rw/rT8kOmj86fl0e1wcdrE/Y/c81PNrPr13AE8LcWBoBSBQnGY8srKfTzzdTmxUUae+XoPPz1pLA99upOXr5vJlMJUXG6NN7+t4ugx6Xy+vR6Ab3Y3AfDgRZO9WSzFWYkB5y3JSqQoI55rjhnJKEs8mgbrKloBuGp2EXkpcTy8eCevra6kMM3sjQu8s05md59+RA7PfVOOpkFeShyJsSbOmJTD0aPTAcLOIchMjOXWk4sP7kNSHBSUIlAoDjN21lu547UN1Fulq+apL/dgd7n524fbAFi8tZ4phan86f0tPPXVHmJMBhwut3diU2ZijDcwDDAmU6aIHjs2gwZrDz+YnAvAb84sBaCiuQuA3ORYfnvWBAA2V7fz+rdVXPucL0NvuSfwPC4nkbGZCWyv6+Cfl04dVpk3QxWlCBSKw4wPN9ayam8LICdD2V1u72zWuCgjK8qbeWn5Xp76ag8XTM8nLspIl93FJTMLufDxZVw+a0TApK34GBP3nFXK9KI0Jub1zqQpSDMzLjuR0Zm+OQX3X1BGfpqZBZ/uAOTIv6rVBkBSbBQzRqbRbnNyRIjzKQYfShEoFBGgormLyhYbm6rbWLi6kg9vOa7fx26sagfkTNf8VDPPLi3numNHceGRBTz+xW6e/6acleUtnFBi4c/nTQqIGXz68zne2az+XDl7ZJ/XfPm6o4gy+s4jhAiY+PWvy6fz0vK9JHtKJNx5+nh+dlKxtxSEYnCjFqZRKCLAPW9v4upnV7JoUy1ba63YPDNZ/Vmzr4Vrnl2J3elma207Fz+xjI8317G2opWzJ+fy5BVHcuGRBSTGmpg3MZvMpFhmjEzD4dIYY0lgwcVTegWOC9LMIYPJ+yMtPprE2KiAbWWesg0mg6AkO5E/nnsEv5w3DgBztAlLYu8JZ4rBibIIFIqDTFuXgy92NOBwaawsly6e2vZuRmbEs73OygcbavnJiWP4aFMdn26tZ1utlQ831fDN7iZvsLcsX/rdx+ckseGeU73nnlNs4dpjRnLVMSN7Ce6DTYo5mqJ0My4tdKaSYuigFIFi2LNsdxOjLPG9ShH488OnlpOVFMt9YfLV9zR20txpZ9qIVBZtrsXhCizdsrGqjepWG49/sZsvtjcwIt3M7gY5yWtzTRvrK9soyUqk2+lib1NX2ABsbJSRX3uCvN8HP5xVhLXb8b1dT3FoUIpAMazp6HFy2ZPLufLoorACdl1FK1/uaMRoENx2crHXB9/e7cDt1kgxR3PF0yvY19zFsv89iYWetMsoo2BXQycAv393M40dPeSlymP/9uFWok3SM7uxqp11Fa2cWZbL/542jk+31DO18PDIxLnmmL5jC4qhgYoRKIY1a/a24HRrVLbYwrZ5/pu9xEUZ0TSNm19ew0eeiVm3vrKWq59dCchCbgA/+fcaVuxp5tKZhRw71uINrtZbe3BrUNFsozDNTHVbN+VNMm3zg401tHc7mZyfQmJsFOdMyfvO9fwViu+CUgSKYY1elK2mzcaafS3srO/o3aa8iRPHZ3LF0UVsqbHyyJJdOFxulu5qYm1FK+1+rpOV5S3EmAxceGQBv5xXwgc/O5aEmEDD++YTx3h97kmxJho77AAqH19xyFCKQDGk6Xa4uP75VWyrtfbaV9nSxRc7ZGmGqlYb5/1zKXMf+Nw7gxZkkba6th7yU+L47VkTOHtyLlUtNjZWtWFzuHBr8PGmOrrsLu4+s5Q3b5rNM1cdSYo5GnO0idyUOLKSZHaNLvyPG2thikfoX3bUCIwGwS1zx1KcNbC1ARSKg4WKESiGNCv2NPPR5jo6epwBZZWdLjen/v0LOu0uYqMM3lE5yAldZ5XJ2bfNnXbsLjdZnsqXuSlxNHb08OWORkAK94WrZc2ewjRzyPVys5Nj2dXQKWvtuzSyk2M969S28aPjRnP7qSXKFaQ4pChFoBjSNHdKAZ9iDky1bOq002l3cc0xI8lPjeN372wGIDHGRGWrL15Q294NSGEOeEsmv7W2ilEZ8SSbo7wpn8HLM+roSuScKXne4685ZiQnl2aRbI5sCqhC0R+Ua0gxpKlskQHZ5LjogO11HgE/c2Qao/yWazyuxEKVJ3Dc0mmnvFEerwtzPetnV0MnU0ekcuakXO+x+amhSyVPykumMM1MbrIvPTXKaBjwMpEKRaRQikAxpNEzc5wud8D2ek/t/aykWK+AzkuJoyQrkcaOHpo77Zz24Jfc9LKs658TZBGADO6ePy3f+zk+JrSBfeXskXx++/HK/aM4bFGKQDGkaLD2cPSfP/WWVd7bJPP424MmRdVZpUWQmRRDjke4F2cleAX93W9t9LqFhMBbTiE7ORZ9ku3k/BSS46L4xSnFXDW7qM9+KSWgOJyJqCIQQswTQmwTQuwUQtwRYn+qEOINIcR6IcQKIcTESPZHcXjjdLm9Pn2QpZCve34VK8ub+32OzTXtVLd1s8iT669bBO02Z0C7+vYehICMhBgSYkyMyohn5qh0r+vn3fU1JHpG+JqGt5pnlNFAdlIs0SYDJdmyzv/NJ471lm9WKAYjEVMEQggj8AhwGlAKXCyECJ66eSewVtO0ScDlwIOR6o/i8GfBpzuY+vuPvZOzXl9Tyceb67joiWXUe0bn+0P376/Y00xVq40GT03/YIug3tpNeny0V8B/fNscfnTcqADXz8/mjg15jdGZCUwuSPHODFYoBjuR/CXPAHZqmrZb0zQ78ApwdlCbUuBTAE3TtgJFQoisCPZJcRizdJfMvvnPygoA1le2YRDgcmus2dfSr3NUtUoLYNXeFo6/dzHRJgOjLfG0dztYtrsJm93F8t1N7GroxOJXW8hoEAghvNlBAJfPKgp5jfsvKOPhi6d8l1tUKA5LIqkI8oAKv8+Vnm3+rAPOAxBCzABGAPkohiWxUXJpxReX7cXhcrOhqo2LZhQSZRSsrWjr1zmq/EpFOFwaH996HMeMyaCmtZuLnljGmQ99yYVPLGPFnmbvRC9/oowGEmJMzB2fSbTJwMvXzuTDW44NaJOZGEtmUvgCdQrFYCOS8whCRce0oM9/AR4UQqwFNgDfAs7gg4QQ1wPXAxQWFh7kbioOF/QVsGrauvlqRyM2h4sZRWlsqmrzBn/7c46y/GRKshO57thRjEiPJykuCqdb/vT0InAAcR7FE8zau0/2BnePHpNxILekUAwKImkRVAIFfp/zgWr/BpqmtWuadpWmaZORMQILsCf4RJqmPaFp2nRN06ZbLJYIdlkxUHbWd1DTFr5gW39xuzWqWm2M8wRg311fA8Ck/GTKClJYX9mKIygFNBRVLTZGWxL42/lljPUs2p4UVLc/05MBpAUPSzyYjAZVf18xrIikIlgJjBVCjBRCRAMXAW/7NxBCpHj2AVwLfKFpWnsE+6Q4yPz4pdXc8/YmbHYX3Y7eq3D1l6ZOO3anm6NGpQPw2dY64qKMFKXHM3tMBp12F+f9c2nIlb50HC43te3d3swfnaQ4n+EbbTTw05PG8o8LJ/PbH3x/df0VisOZiCkCTdOcwM3AImAL8KqmaZuEEDcIIW7wNBsPbBJCbEVmF/0sUv1RHHxcbo3yxi521ndw2oNfcOJ9S/Z7zIbKNu55exNvra0K2K67hWaMTEMIaOlyUJyVgMEgOKU0i9+fM5ENVW18tLmWxz/fRUunncc+30WP06cYatu6cWuBk77AZxFkJcWw9rcnc+nMQs6ZkkdOcuiZwArFcCOitYY0TXsfeD9o22N+778BQufoKQ5rdtZ30NnjxO5yU9Fsw+5x2+yosxIXbUTTQtfeefKr3by1tppnl8LIjHi+2N7A0l1NXDxDxn6K0uPJToqlpq3b69oRQnD+1Hx+9/Ym7n5rE202B2+urWZLTTuZiTFkJ8cya1Q6ez1zBoKvm+RZE6Ag1Yw5WpXXUiiCUf8Viu/E1Z5F1wGvEgB4YdlettS0IxBcelQhrV0Orji6yLu/rr2bcdmyjMPVz67yzhnQc/LzUuMoSDVT09YdUJY5LtrIpPxk1uyTQeMtNdKDeNur6wB4/uoZ3rUEij0KREe3CMIVhVMohjtqRoxiwPQ4XVS0dHlLMOhYEmP4eHMd6yrbqGjp4oVv9vLkV7sD2tS39zDaksD9F0ymJDuBy44qJDbKwJJtDZTmJJEcF+UV2GODBPqRI9MAKM1JAuTC7jqVLTa211lJNUeRkRBYYE6PEShFoFCERlkEigFx00tryEqKDZlxc9nMEfz9k+2AXJoRZO2fhz/bwa6GTv5+4WTqrT3MKYlhTrGFOcUyA6y50877G2q564zxgKzrD1ASpAjOOCKHz7bU88xVR1Le2ElqfDTXPb+KvU1dVLR0sb3OytisxF51fbKSYpmUn8wxKhVUoQiJUgSKfuN2a3y0uRbhN0UkOymW5i47aeZo5pZmehWBy61R0yYthheX7aPe2s0vTi2ho8dJZmLgZKxfnFLC8cWZzPYI6otmFJCTEutdJF5nUn4KH982B/CVhf789hM4/t7F7GvuYkddB+dMCZ6zKCeqvX3zMQfpKSgUQw+lCBT9prGzB4dLw39eYGGamdT4aHKSYxmXnURirAlrd+CcQN2F9LpnJa/gGb2jLAkBawJkJcVywfQC+ktBmplV5c1Ye5xquUeF4jugFIGi39S0+mIC0UYD8yZmMy4nkdMn5hATJSdhzZ+az+aadu+i8P68ulpWHAm2CA6U/FSzd+nIiXnJB/XcCsVwQCkCxX7564dbOWZMBla/Cp55qXEsCFF47Z4fTKCuvZuZf/o0YLslMYaKZjlXIFSNnwNBjykkxZooy++9ZrBCoegblTWk6JOd9VYeXbKLBz/dQbWfRdBXBk5GQoy3RIOeFvqj40Z59x9si0Bfj3hSfgoGVRpCoRgwShEoQrKroYMep8tb82dlecaLI7gAACAASURBVDPrKluJMRk4pTSL48aGz8AxGgSWhBhiowwUZyUQG2Xgkpm+YoH+JR8OBseOzSA9PppfzRt3UM+rUAwXlGtoGGJ3uvtcVGVbrZXTF3zJvInZbKlppzDNzL7mLt5aW01RupknLp++32tkJ8dijjYyPjsJc5QJc7SJ0pwkNte0H/RlG/NTzaz+zckH9ZwKxXBCKYJhxtbadub940uevnI6J44LvQbQH9/fgsut8Z7HGnj00qk8+dUeVu9tIdkcHfKYYC6ZUUin3cnFMwpxeyYdvHHT0Z6sI4VCcTihFMEwQxfuy/c0h1QEFc1dfLG9gZtOGM2SbQ2cOiGb047IITMplvmPLqWnnxVGLziyd/pnjMlIjPrFKRSHHerfcpixpcYKQFqYkf2S7Q0AzJ+az+2n+nzu00ak8rf5kyjNTQp5nEKhGLwoRTDM2Fwtl3wMXsxd5/NtDRSkxTEyI77XvlCjfIVCMfhRWUNDkPLGTubcu5jatsCicFWtNqo929psvRWB0+Xmm12NzCm2HPSArkKhOHxRimAI8u8V+9jb1MVrayoDtm+vs3rft9l6LQ1NVauNTruLSXlqUpZCMZxQimAIku4pw6zX+tfZ2ygXbs9LiaM9hEWgz/xV5ZoViuGFUgRDkLgoIwBNHXbWV7Zy4ePfUN7YSXlTF/HRRkZZ4kO6hipa9BW+1BKOCsVwQgWLhyBdngXed9R3cO1zq6i39vCvL3dT3WpjRHo8SXFRVLXYvO3dbo2/LdpGRXMXJoNQa/kqFMOMiCoCIcQ84EHACDypadpfgvYnAy8ChZ6+3Kdp2jOR7NNwQFcE+nKO00ak8vqaKuJjjMwYmUZyXFRA1tCepk4e+3wXIAu4GVW9HoViWBEx15AQwgg8ApwGlAIXCyFKg5rdBGzWNK0MOB64XwjRv6mrirB02X2B4PzUOP7v7AnYHC4aO+yMSI8nOS6KNpuDypYuLnjsG173Cyort5BCMfyIpEUwA9ipadpuACHEK8DZwGa/NhqQKGSuYgLQDPROZ1EMCN0iAJhTbGFCbjLTR6Syam8LWYkx2BxuHC6NCx77huq2blbva/G2L0hVgWKFYrgRyWBxHlDh97nSs82fh4HxQDWwAfiZpmnuCPZpWGALUgQACy6ewrFjMzhpfBbJcbJssz6nwOXWvEXoRqT3nkimUCiGNpFUBKEczcEVx04F1gK5wGTgYSFErxoGQojrhRCrhBCrGhoaDn5PhxiddidjMhN49NKpzB0v6wnlpsTxwjUzKUgzexUBwDmTcwGYXJDCs1cdGVAuWqFQDA8iqQgqAf+aBPnIkb8/VwGva5KdwB6gV1F5TdOe0DRtuqZp0y0WS8Q6PFTosrtIiDFx2hE5IRdq8V8P4Nix8nmOtsRzfElmgJJQKBTDg0gqgpXAWCHESE8A+CLg7aA2+4CTAIQQWUAJsDuCfRoWdNldxMcYw+43R0tFUJAWx+RCOYt4TGbi99I3hUJx+BGxYLGmaU4hxM3AImT66NOapm0SQtzg2f8Y8HvgWSHEBqQr6VeapjVGqk/DhS67i9Q+1g3Q1/j91bxxjLYk8K/LpzNrdPr31T2FQnGYEdF5BJqmvQ+8H7TtMb/31cApkezDcMRmd2KODm8RWBJjKP/LGd7PJ5eGXqBGoVAMD1SJiSFI535cQwqFQuGPUgRDEJvdRVyUqh6iUCj6h1IEQwxN0+jaj2tIoVAo/FGKYIjR43Tj1sCsXEMKhaKfKEUwxNDLS5ijlCJQKBT9QymCIUZnjyzVpM8VUCgUiv2hFMEQw+bwWATKNaRQKPqJUgRDCE3T2ONZjlIFixUKRX9RimAQUdHcxXNLy9G04Np9ks+3N/CjF1YDyjWkUCj6j5IWg4if/3cdK/Y0M6fYQlFG73LRO+s7vO8TYtRXq1Ao+oeyCAYRUUZZSXRleXPI/a1dcvnJP5wzkfE5vap5KxQKRUiUIhhEWBJiAPjv6kpeWLa3l4uowdpDZmIMlx01Qq07rFAo+o3yHwwi2rtlauiKPc2s2NPMhNwkphamevc3dPRgSYw5VN1TKBSDFGURDCLabA7v+2ijgXfX1QTsb7AqRaBQKAaOUgSDiDabg9OPyGbL/81jTomF9zfUUG/t5pHFO2ns6JGKIEEpAoVCMTCUa2gQ0drlIDkuirhoIz8oy+XjzXVc/tQKttZa+deXu2ntciiLQKFQDBhlEQwSNE2j3eYgOU6uPHbqhGwyEmLYWmvliLxkb8aQUgQKhWKgKEUwSOh2uLG73N7F5aNNBi6dWQjA78+Z6G2nFIFCoRgoyjU0SGi12QG8igDgxyeMZk6JhckFKWQmxlBv7SEtPvxaxQqFQhGKiFoEQoh5QohtQoidQog7Quy/XQix1vO3UQjhEkKkRbJPgxG3W6OmrRuAFLNPEcSYjN700Z/NHQv4FqZXKBSK/hIxi0AIYQQeAU4GKoGVQoi3NU3brLfRNO1e4F5P+7OAWzVNCz1tdhhz70fbeHTJLiDQIvDn0pkjOHtyniotoVAoBkwkLYIZwE5N03ZrmmYHXgHO7qP9xcC/I9ifQct7633zBcIpAlD1hRQKxXcjkoogD6jw+1zp2dYLIYQZmAe8FsH+DBqW7mzkoU934HJr9DhdlGQnevf1pQgUCoXiuxDJIWSoYjeh6yfDWcDX4dxCQojrgesBCgsLD07vDmMWrqnk3XU1RJkMvLR8L0XpvkqjyWalCBQKxcElkhZBJVDg9zkfqA7T9iL6cAtpmvaEpmnTNU2bbrFYDmIXD0+aOuzYXW6+2tFIRbON6lYbAKMs8SSodQYUCsVBJpKKYCUwVggxUggRjRT2bwc3EkIkA3OAtyLYl0FFY0cPAOsqWgEob+ri3Cl5fPbz4zGoqqIKheIgE7HhpaZpTiHEzcAiwAg8rWnaJiHEDZ79j3mangt8pGlaZ6T6MtjQFYHVsxC9y60FpI0qFArFwSSifgZN094H3g/a9ljQ52eBZyPZj8GE263R1GHvtT3VrCaKKRSKyKBKTBxmtNkcON29Y+qpyiJQKBQRol+KQAgRL4QweN4XCyF+IIRQkukA2FjVxqoQS07qbqFgUpRFoFAoIkR/LYIvgFghRB7wKXAVyp1zQPzfO5v55Wvre21vDOEWAlSMQKFQRIz+KgKhaVoXcB7wkKZp5wKlkevW0EbTNLbVWdnX1IXD5Q7Yp1sEI9JlzaDc5FhAxQgUCkXk6LciEELMAi4F3vNsUwnt35F6a483FrCvuStgn64IJhekkBhjYmyWnFWsLAKFQhEp+qsIbgH+F3jDkwI6ClgcuW4NbbbXWb3vdzcEZs02dvRgNAjuOmM8L147k0zP+gLKIlAoFJGiX6N6TdM+Bz4H8ASNGzVN+2kkOzaU2V7X4X2/u6EDyAKgo8fJJ5vryU2JJTNR/i3d1URGQjTmaOMh6q1CoRjq9Ddr6GUhRJIQIh7YDGwTQtwe2a4NXXbUWUmLjyYjIZpdDVIpbK+zcv6jS9nZ0MEfzjnC2/aaY0by0a1zEELNKFYoFJGhv66hUk3T2oFzkBPECoEfRqxXQ5xtdVbGZiYwypLALo9r6KHPdlLVYuPxy6Yxp9hXTynaZFCrjikUiojSX0UQ5Zk3cA7wlqZpDsJXElX0gaZp7KzroDgrkYm5yWysasPudFPbZqM0N4m5pVmHuosKhWKY0V9F8DhQDsQDXwghRgDtkerUUKamrRtrj5PirARmjEyjx+lmQ1Ur9dYeMpNiD3X3FArFMKRfikDTtAWapuVpmna6JtkLnBDhvg1J9Iyh4qxEjiyS6w0v291MXXs3WZ4MoSFLjxU66g/sHJoGzbsHflzLXnA5D+zaCsUQpb/B4mQhxANCiFWev/uR1oFigPgrgvSEGMZkJvDZ1nq6HW4yk4a4Ivj09/DCeQd2jt2LYcFUKdj7i60VHp4Om944sGsrFEOU/rqGngaswAWev3bgmUh1aiizva6DjIQYUj0B4GmFqaze2wJA1lB3DVmr5d+B0F4NaNDZ2P9jbM3gsh/4tRWKIUp/FcFoTdN+61mIfremab8DRkWyY0OVPY2djLb4jKkJeUne95ah7hqyd8q/A6HHMwfDaRvAdbt81x/OvPMz2Pr+/tsBLH0Yvl7w3a+1bxm8ejm4Xd/9HAPlrZth24f9a/v1g7D0oYFfY9dieP36vts07oQX50P9FnjhXOhuG/h1vmf6qwhsQohj9A9CiNnAAP4TFTo1rTbyUuO8n0tzfIpgyFsE9k5wdh+Yr77HMyvb0d3/YxxKEeB2w5rnYefH/Wu/6XXY/OZ3v96uz2DzWwOz3A4ElwO+fQF2fNS/9htfg/X/Gfh1dn4ij3P0If7Kv5Dtvn1RPof6LQO/zvdMfxXBDcAjQohyIUQ58DDwo4j1aojicmvUWXvITfYpgnF+iiBzOFgEAI4DEMg9nmQ13SJwu2HLO/I1HF5F0BG+zcGidR9UrY78dQaK3Qqa26dIw1G3CRq2Q3f7/tu2VcG+5YHbOptgzxdga/F8bgjc7+wJbZW43bD57b6/xx0fh1fmXU3yVb9uMNVroWmX77OtBdprwl8rHPrvL9x1wKf89KQGW6u8N60fGfc7PvZZvd8j/c0aWqdpWhkwCZikadoU4MSI9mwIUm/txuXWyE72jfwTYkwUpZuJizKSEDPE6/jpgvhARub6OXSLoGIZ/OcyqFjexzHfo0Ww5K/w3ysjf52Boguu/QmZt38CH94h3Rn7a/vs6fD0KYEW3sonpTvEWis/dwVZBJvehFcuhoZtgdv3fgWv/hD2fB76Wu3V8NL54UfxuvANJ6Bfvw4+vtv32dYq++YMvf5HWHQ3z0AUwZZ35L3tW9b3udtr+r7HCDKgFco0TWv3zDAGuC0C/RmyLNpUy7+X7wMgNyXQBTS1MJWijPihX0ZCF8QHIpD1UapuEXR7fo49fUxr+T5dQ12N0NV7waFDjlcR7GeUb62Vo/ieflgELeXytcHP9dFZD24nNO30fA5SBG37PK8VgdtbK0Jv9++X/2swuuURSkBrmjy/fm6Xw/d7CXe+cHT3xyLw9KV5j3xt2iFfw92bTsd+7jGCHMhSlfuVWkKIeUKIbUKInUKIO8K0OV4IsVYIsUkIEWY4MPj50QurWfCZ/OfI8XMNAdxz9gSeu/rIQ9Gt0Gga/PNo2LDQt+2Z02HFvw7svF5FcACmb0+QRaArhFA+W1srPDBB+mv3d91/nSRHs48dC+/2c4yz6C5488eB27rb5XX0UfKqp+HJuX2fZ/cS2c/9Cd5gbK1w/3ife8bRDQumwPZFIdp6BJc96BqvXAqL/yzfa55srI46GcuxW/t21aSPla/+rjD9OrobJtg1pLtjgt0yekZXOHeN7vppq4QHJ8O2D0LvDyWgu1vl70Q/t3/w1up3vU9/LwPcfdEf15BuBbk81oae6twelLXWXgP3FcPOT+X3uPntwOO/Rw5EEfTp8BJCGIFHgNOQi9hcLIQoDWqTAvwT+IGmaROA/zmA/gwacoMUQVJsFJmJh1Gg2NEF9ZugdoNvW9XqwM8Dxe06OCPzYIvAqxBCBI+bdkJ7JVSs6Pu6DhtUrYK9S6F2Pax6qn9+2qo18s8fXcjoAqN6LVSuBGfolecAKP9a9rOtav/X9KetQgrQypXyc0eddEcE9wnCWwQVy2HfN759rh55Hp2+4jnxnppYoRSBLgSDLQJd8FqDBL4upMOl+OoKpfpbaNnj+06D94cS0Pq5OxukNeBvsfkL571Le8c8ghmIa0jHO9IPuue9X8tnvf4/8r63fxh4L98jfTqlhRBWQgt8AcSF2O7PDGCnpmm7Ped6BTgbWb1U5xLgdU3T9gFomnaA004PT+xO36jKHG0kKc7z2Mu/lqOVcWccop6FIdiF43ZLQdtXpsT+cPgtwGPvhJr1UrFMuXSAffMIsq4m6Y+Pk7Oz6bHKz0fdAMsfh5k/8v2T6/+A4RSB3q5mnW/bhlchNgWS86FgRmD77R+BMMi+6FbG3m/kP3WPn6vKnOYTHF1Nsu2uxTDTk37YVimzV5o9o2dbi/xb8STMuE7exzG3gilM0UFdWen3pwunUCPKUDECTZPbrX6Cstc1rBCTGPr6+ndRuRrqt0qFEiwge1kE+sg/SOlZw1gKwefRYwvBQlXf39MOq5+DnDJo3A5po6Fb75Mm3S7+fbTWwI5P5O/TWi2fnaaB7qatXCV/p/lHSoXbl2vIWiczs/wVaah719EVqP6q35uuSDRNprgWzwNLcehzHiT6VASapoX5BfSLPMDfKVYJzAxqU4wsaLcESAQe1DTt+QO45iHl8qdXMKfYwjXHjAzY3uC3IH1OcqwvFrB0gfRdHnaKICio6x19d4Vu369z+glhe4d0w6x9GcouAsMA1lrQR7TbF0HDVphwrvxc/pVMd3R0wdf/gLRRPnfB/iwRXajofm2Qgb3yr6QACFYEL3sM19Qin+Bf+pAUGLrgD7YMuhrl/S77J0y6AOJS4L1fwPYPwOgR9LYW2PIuLP6DHCB88zAUHgWj5vT9LHQBEy5Tx3+fv3vM3iH9+brw1Z9XwDX6sIz0fU07YfUzsPwxSMoLbBM8Om4P4wLyKu1wFoEuHF2B7UNd552fwhEXSPfR2Lkw2i+vxVoTKMRb9sKiO+V7Y4x8Ht2tcoChafDebVC7EcadDlvfA4NHZIZSBN++IL+7cAT3uXKVfNV/d/q96ffSVgkf/0ZaKpe8Ev68B4EDcQ3tj1AxhGDrwgRMA84ATgV+I4TopfqEENfr5S0aGr5/s6m/rCpvZm1Fa6/tde3SbRFjMnBEXrJvh7M7tEvjUBPsy9ctgQPK9ukMfG9rAbdD/tj9cTl7Cw+3W462wCd89FGXLrz0Vz1n29bS+x/P3gEdDdJN1dnk8+OHGoXaWuU5/AW73m+dHqvsjz6y7qz3CWd95Kgf39ngyyJp2iX7oY/0XXZfn3WlpI8Sm3cHBg+tfqNNfURurZHb9WfQ2SgHGNXfSleIfj8glWJnk3Sp+ccNeqyhFUhHne9edBw2eax+r64eX2A0eKTfWe97vi6H7xrBAn+/FkEYF1O4/RXL5X211wSes73ad9+GKCm8dYLdWZWrpJWouaRS0dyB3xXI35JePyuUSy64z/Vb5XdbuUq6IUPeq+cZ6b+B7R/K+Qh9uRcPkEgqgkqgwO9zPhCs7iuBDzVN69Q0rRH4AigLPpGmaU9omjZd07TpFoslePdhgcPlpsvuorWr95dV3y5/YAtvOJq/XzjZt8Np9/2wDieCXUP6iPqALAL/kaifQA0uIPft8zLg6e+GWvkk3F8MjTt8wsfrBmkOfG3wUwTBwsLWAg+WwZrn4KEpchQLvYVS5gR5rKNLjujdbvhLITx/jhSuOj0dUkjogtHZjXes41UgHiHa2eS71xfOhfvGQIxvDom3f7ryql4rX9e+DPeXyFFp/Rb5Xvdj68+ibjP84whY/az83FoBDx8JTxwPS/4c+LwAHp0Fi/8YuK29JvTkr9eukamP/jx1Mvy1SH6nepygIdSkKSHdKQ9OkorCWiufjzE6UDi7HFKYGqPDp3SGCzoH7Pcbe7Z6ArR6WRPd6vK3CLKPCP2b1p/D2pfA5InduYMmQernWPM8/GOS/P1VrQq892Daq+CfM+FfJ8KTJ8nfS3RC73bdrfKZVK2Wyspgkr+Zr/7eu+1BIpKKYCUwVggxUggRDVwEvB3U5i3gWCGESQhhRrqODv9peCGwdssfSpvN0WtfvVWO+rOSYwJTRF09A89j/j4Idg15LYKD6BrSR6jBiqBxpxS+/qN5PZhZ/lXv0hLBFkGrJz0xlEUAMvi5b7kU1Hr6o79QMURBxhjfvu42WegOoHJF4KhPH0HaO3q7CnqCLIKOWt85ezzbWoMK5/krL/0+Kz1B0cZtnmC95osp6NZRT5vsix5Abdsnj4+Kl1lLuqLS6aiTI90AX3l1eIugfmvgNj1pwNktXXDge+7+JOfLV5ddKnP93rInBQr8jjp5X9mTPH0JkT4ZHPfQrRj//akjeh9nrZW/g4xi6frxWgQCLnoJLnkVzgvKhtOfQ+VKKDoGkvJ7n1d/duVfyme95Z3A2ECovuhc/B953cvfgsmXhG7T1SR/azllcP1i2f99S8Of8wCJmCLQNM0J3AwsQgr3Vz0L398ghLjB02YL8CGwHlgBPKlp2sZI9SmStHsUQGuXTxE8sngn/11VQX27XJA+PT5o5rDTfpgqgmCLQI8RhHENff2gnE7fn3Pq78NZBPo/vP9oPiFTvlauohfetMGg3P1QFoGOv9UAgRZBYjaY0/0EebsUpgCJuaHN/+723oogOEZQs6639RdcQTWc8gKprPRnpY9YgzOAXEG/pePvkOd8fA5sC5rN27ynfxYBSPeO7kYLrpuTOrJ3ex19Jm1ClnTB6O6jvGny9flzpKWkZ0vlT5evL/2P7POTJwcGUKOCCh63Vcq6P7s+k662jBABVZddWlNJefK7/fZF+OJvMkaTlAvFp8LI4wKPWf4YvP9LqN8s+5o3NXC/wSRnT792LVR4MrZWPCFfdcsjucDXFiDG4xIuPRtK5snrjjq+d0xFv0drrbQ+86ZJy6XoGKj6tu903gMgkhYBmqa9r2lasaZpozVN+6Nn22Oapj3m1+ZeTdNKNU2bqGnaPyLZn0jS3q0rAt8/+72LtnH7wvXUW7vJSIjGaAgyF53dvf95DwfCxQjCZQ2teNLnlgh7znCuoT2B7fTRmP8oXbceQs061YVrsJDtag7vb9aFi96H9hpI8YzgEnN8mUggBbnuy+1p943GA/pc3/t77G6XSl6PAemjdaPfYKCtUiqXo34sR9Z9KS+rvyLQM2T2M+9g6uUw8wbpaoBAQdpWERhv0DNmYlN6n0dz+0a7/q4xgLQQikA/x7w/w1E3wen3SQWy7mWZbXXktTD+LDnCXf20rElkMMGR18HE+TIIn5AlLY+vH/TMb2iAzHHyvPp3tfpZmXq58BppIYzxzNcwBg24rNWQPgaO/onM5AKfgAZ5rSiz7/Per2HF4/K+86bBUTfCCb+W1iL4hPyG//omyNVtlG6ym5bDjB/5Avx627KLYNpVcGaQiEvKDbwn/R73LpUDL11p5k2Tll+o399BIKKKYDjRbpMjpvZuJy53YEx8VXlL6HkCumuoPzVIvk/CxQiCXUP1W6Uw8xdSOs27Zc0akD5rfeQkDFJI69ZF8HH6qNRaLa+3d6lP8O1vZqY/bRXhLRhdOHstghrfqC8pSBE4u32uCnuHHMUH+/ZbQ/Sruy0wyKq7gYpm+7ZpLsidLAWmOUP6yf3dM/ooEqSlEGwRBE8O88ecLke9p/0VTrhLbgt4HhrUeOIQUWaPRdAA6aMB4fON61hr5L2vCUrqC2UR6CPzwlkw70++rKvdS8AyXqZCXvgijDoBVj4lR+ml50iX3PlPw6Wvyr8pl8pJjSuflN+DZbw8jy4cl3vGk7Zm+fymXi77PmJW7+eXP02m5OpuIP/nLIRUxPGW3j773Kkw4miYc7u0KKD3LHb9GUy7Up7n9L9BvMeKTS3yvZ71D58i0knMCbwn/R71OQX+igAiVsdKKYKDhG4RgHQT2ey+8ru7GzsZZYnvfZDTDmi9A1GHmnAxgmDB+sol8MYNMvunq8k3ctc0ePlCeMNTl/C/V8CyR+R7c4Yvs8ScLqff+69apgu59hoZrHvmdDnaGiiNHiUUbKL7Y2uRLg9rjfxnziiRfmp/RQDS2tB94U4bWMYF7g/lH+9p6y0wYpJg/A96bwN5zXrPFJtoT9b2uDPkKDQpL1DZdvm5hswZUpDrI0p91Kr3F+AIT7rr+LMCr121GkxxYCmRWTZ1myGlUCqQeIvvXCAV0Yd3yHkP/sRn9BaeRcdIQRjnsQwSs31+dn83y6yb5H31tMtRdzAzrpfK8v1fyM+jjpfCveR0n5A/7pfyGRx1A0TFSSU0cg5kHRF4v16BOlVaLKNPCrxW/nT53eu//ZQRkDUREvySU3ShPeki+XrCXfJcx9wiv7NpV/naWsZJyyTHE/cI/k3pZBTL72DCOfI5jjlR/lb3fg2xyb7vMaNY7t9fZtJ3ZIhXOfv+aPcLErfaHETZA4X7XaeP732Q7k5w9oAxqvf+Q4V/lVC326cI3E6pvEzRss8tewKFYMseiJsiR36N2+UPt3J14CgmIcuXMjrzRpl3vfo5OeLSXQDgyfSIAjTplpg43yeE4tJ6xwSC0ZVrTpm0DuItobOI2ipk27RRcONSabFs/6D3+SzjfYLYUuIL4kLvoK85Q1oD3UGpxJMvkaPG0rPhb55RZKxHoMWl+O4pd7IMQpacJkeRb90ky2QEzxPo6YCUArh6kVw7YPEf5H00bgtUBNFm+HW9fI5b3pH3qLnld5SYC2WXwAe3e/p4qYxnRJkDg+DWGhk8nXAenHCnXPEN5GSz+AzZ1pwuBwSzbpJC0ugnXvKmyNnTukAGGHsy/GKn7E98eohnXgK/2CEtUmO0jBVNOEcKypJ58v4Ts+HY23zuIP07PPon8ve09iU5Ck/xC97+YkfvgcEZD8jX//OM2C97XSpFf5I8iuD4O2Dub2Wfjv4pmGKkcojys6IKZ8KdVdJ9BOEVQWIW3LFP/k8VnyZ/85vflnNicqeCwTNWNxjhR1/07tNBQlkEBwl/i6C1y05zp/RZ/+bMUpbfeVLohen1vODDJWDscsITJ8DG133bHF2BmTq6VdC6TwoTt1+W1FOnwru3+moS2TtkYM6fuBSfiyd/uhyZrXraVwhMP197TaDgTs6HEZ4lMeIz+n9POZ503XhL7322Fp/PNW2UFFwGQ+h/2kw/Rb4/iyClALa8LesXAd5UwiOvlW6IuFTfaDvWzyLQyffUnUrKk0ImMccnkGOSZanoBVNlNlVMomyjC6mMsb778ccU4xu5pxT6KaBU6b+OTpBW0eiTpECPTfGbAwR1iQAAIABJREFUUSykxdDZIF1b/gHOmETfs80pk0I4NiVQCUBvF4dOgiW0EtAxp8nvXk8YMEbJZxiTKO9ZCGkJ6AJT/w6NUVK4JmbLa/pn65mife11DMbAiY3po3vP6E7KlwpEf+ZCSOGvvwZjjPJ9r8EuIX/065ii5blmeGaeBz+r9NERGzAqi2AAtHc7SIg2YQgO+uKLEYC0CPQWkwuSwy84o/uqD5eAcXcrVAeZnvbOwCCxvUv+uEMtIO/q8QWNc8rkyHLHR1K47PpUbvcvVxCXKn/0/75QztrMPkJuN0RJJWD0+0c0Z8DF/5YjrMqVPtdPKBKy5Og3Ls03ggqlCFx2KVQhUHDGhfin9VcEmUGKQFdsCVnSEvAGZT2xnwtflNfShbSuDDrrAwWyfo5jb5P9zp0it+kBRYDSs6RPXVdguhtJd1ukj5Z+8FHH974H/dkn5kLedNi4UAqW2CS44DkpwA0GOPVPIIyyJHWUWT77Le/IY/OmSQsjKl4OCqIT5H6QAdVpV/ZWAiC3x6X5vuPvi7Mf8T2b/nDD1/J7CVUJ+KgbZPxhIFWCR58oA8R50/t/zIij4ex/+oLf3wPKIugn3Q4Xs//yGf9ZFTpg6W8RtHU5vBZBWnDKqI7b5ZtSPlCLoK0KPvq1dAf0N9DscsjlB0PNBdA0GbRrCpGRYO8InHTjsMksjwBfcYh/jJN/7zm3W/qMdaL9YiVxqdI9kDJCTnBa/Ee5PXO8VATtfrOO4y1SYB15jRwB9oUerEwb5RO04UZkup9cDwTq/Qq+L38rIJxFkFEs+7j3q8D9OWUw8bzAbfo19BiBngE0cb4U2NOv8o1a9XvIneqr+KmjC3ddWcSlyhIW+gjaH2OUDAIn5fhGnXrAeMxcX/pmwQwZXI1JkEI0KUcqMmOMnGwHPqtMdw2B9IeXnt37unq/pl0xMCF6MBhzEmSV7r+dTvbEwJIU/qQU9o6z7A9TTOB32R+EkIHyxKyBXesAUBZBP6lutWHtdrKyvJmLZ8hR5jmPfM2Zk3K49thRtNscJMaYsPY4ae2y4/RkDqXFhykY5i/8Bzq7eNMbvvVWyy4ODGiFY/cS+OguSM7z1efRqVkra6oEuxOgt0Xg6PQr1SsATQqd0nOkW8DtlK+Fs+SoUnPJUeQxt8pUUX9fbVyqNMePux0+/F/fKD9/ukzZ1CdgQeCI3hRGEaSPkfvGzJU+9rRRPtdL6kiPP/xCOUMzyiwVXNUa2c5fQOlBzqRcX2A7tUgKUWN04AgzOkEqS2O0FBK1G2RAct0rvsVyYoOyjPR7B5+Qn3CetJ5m/6x326JjpQL4wUOBhfFACmv9/vJnQEFwOa8giudJC61ghszaKTktfNtRJ/hG/fVbpV9ed2PEW2RsJDpBWh9dTYdXnEsxIJQi6Ce1nnpB2+vkyK3H6WJtRStrK1qparXx9rpqSrKT2FLTTqvNgd3pxmQQJMWGecT+7qCB1hvyH6GHW8Q9uGqkHrANnrBka/W5R0LVEuqsD9weUBtIk26cxByZMhdMSoFMOcyd7Murbt4DX94n3+v9m/pDKUT/6lESY0/xTeIyRktF6e9HDuWPBTli++EbsqorBFoEcanw8y3y+l/9XbpQajdIt864MwPPY4qRro+UEVIRmOKkwDVnSIvGYJQCUHNLd4q9Q7o9Zvqt3jr9arjHc+3oRHrhVQQeJZE/DW5eGea+CuAnnsl0/soRfM8wKhau7cd6xBc853t/+X7WJD7hf33vjw9aTiTek61kNMER58s/xaBFuYb6iV44bkddBy635i0pAfDM1+W4NUg1R5EUa6LV4xpKjY8Ov+qYfwGpgRaTClAEIdxKlavgLyMCXT2hFIG1Vi6MsfhP8rO/y0WfFPTifFkFU6fWL5Uza6L0e+u50sFYxkFmqU8Yg5yApAdKg0fhWRPl+6JjfXnsI44GhBzN64SzCPRJQbqbx1IiR67CIAOgIK0XY7SvnAF4cueDSMqRee3C4LNGkvN9E6hik6UQ1kfk5hABTz1lMpRbINgi6C/B1wn3LCJNcv7AgvaKwxplEfST2jYpcHucbsqbOgO84gYBbg06epxkJMTQYO3B4XKTZg7jFoJAK2CgwWJ/V00oa6Juo3TJNGyTQk7TfIrAPxOnYrm8tu7+8J+JGzyhyHtuT52ZC1+S7gV7R+CsTH/O/Htot9cvtgfOHdC55iPpHopJkH71iuVw7M/hxLsD/aXhLAI9/pA+Gq75RLpoDEa49lOfkomOl5+j4mRqIUi3VjCXvCqF9Oa3fNbIeU/43B8xSfLe9HkPoUqJ3/hV4Oxdf7wxggEqghGzZLrovm/gk3v6XiAlkhx/p5y1rBgSKIugn+gWAcD2WivtHovgjCNyePyHMsi2q76DvNQ4Klq6aO60h48PQKCAHGiw2F8RNG6Hd24JXEDcuxygR8C37vXV5PEX9sGTU/wVkuYK3Cc8qXW1GwEh/fAJmdL94h9o9ScpN7S1YE7rnXkDUkjrmTJ6lkVKoXSb+KMrqeCJTP4KqeBIXzpg3tTAVMCcSYEpkMG1ZEAqk/gMqQx0iyB1hC8oq1sE+uSu6Vf3Pkdcauj71Pfp5xkohUdBQphZrt8X8em+LCjFoEdZBP2ktq2bgrQ4KltsbK21Eh8jH92Vs4s4siiNm04YzcyR6Xy4qZaNG2pIiotiYl4f/+QHEiz2VwTbPpAplcfc4hO6ehG14AVXkvIDC6xVrZZZIDllsO29wIJi7iBFYE6X8YKWPXK2brhR+cFi8iUyUKnPDPZHd2ElZEFzhxxV97QFZiTtj6g4WRNmQghrwJ+ZN/iqaPoz7UqpOKPM0sWWNIAURZBB2o7a0C6l/jDhHFn47Pj/3X9bhWI/KEXQT2rbuylKj8dkMLCt1sqUuFrMdJMUK10Ft58qR36bqttp6XLQanNw7pS88CcMFyzuapZ/GWMC2/d0yEBh9sTAGIFe1sHlN7HLf4HwipW+uED+NNj2oXQVaW5Z877sQjjjflkSQq9vAr0tAl0RQOhCYweb7Ilw1oOh9+kWQUKWzKePS5GKIJyLKhRChA5wBzPrptDbJ1/c/2uFInuifO7flag4OPfRA+uDQuFBuYb6SV17N1lJsRRnJbCttp3Ziy/gcuNHvvWHPRSkydGqpkFpToi0QZ1wweLnzoKHp/WeH/DRXfDYbJmF47D5XDX6KN7fwtAtgS1vw1NzfTOFc6dIBWRrkdUw7VaZcgi+fHZ9EteUoMVI/FMDM0rC39f3QcoIef/6Oq56uuf+5hcoFIqQKEXQD1xujXprD1lJMZRkJ1HT3EqUy0ay6PRaBDqFab5RaWluH4rA3yLwf68XWPv/9s49yq6qvuOf38yduTOTZDKTZJJJJkOYwIQ3ATJErRCkBAxUG1Es0bY+6irLVXFJrVasVq3+YbVLihYs4qvaB5Quq1KLRevCN5UEhECEYEgCTB4kk8ljXpk7d2b3j332PfueuY95ndwZzu+zVtY59zzu7D0ns7/n9/vt/ftFF3e5RUuu0Igb/Fw+G9+95CyAXGH1X9o3euc6OrEftn7ZulTOCaZO5hZeLYYPd8PVn7Tb678UfJc3hXS6b8PTpf1S+NCecGGX87dPxjWkKEoOFYIJsOuQnTK6avE8zm5dQNrYQbdGRmmozS+83t5shaCxLkVbU4k3VP8N/pd3wPfeb/ddeoJoulkJHtVj37Rv8m7wi7qGRk6OT8g2OmynX7oA6devtVbCxX8UDp5uPnvtfBsEraq2W+cf9xOoRXOgVIK6haEryP0uJuMaUhQlh8YIJsBPn7XZHi/vXMJgZpQ6rBDMqzbj1gk0NdQwP53i3BWNxdcQQL4Q9OwMF4Y1LIbjA1YI1t4YXuP8/kO9NvGVK4WXswiC73NuoSVr8vPxNK6wrqHLP2DdSdW1NsDscK6hdGTxk1tFO3QU3viV0B0zG3AilrMIVAgUZSqoEEyAnzx7mLOWLWD5wnqyo2MsSNmpmg2p8Xl+RIQ/vXw1Zy4tUJTaJzpTaCRSLCVqEfTtD9MiDBy2q3X973FbJwRt6wIhCNJANC63fv6r/rpwe5xrKB1pt5suacbgwjeX7tOpZpxFoK4hRZkK6hoqw8mRUR7Z08uGNXYVZaq6ijWLrH7WVxeuH/q+jZ383oVlphNG1w5kT1rfv6s61fPb8JwrPt7igrRmfElBF3B2KSDO+X1YeFq40MlfmVsI5xqKVt+qqbcD7VUfL31/JVh6jg0cn3Wd3bZUOIitKHOUWIVARDaJyE4R2SUitxY4/xoROS4ijwf/PhZne6bCweMnyYyOcY43A2jN4tJCMCGiq4lHBsNVqvOWWkFwM4fcW36Llwq5PiIEziLo3Y1d8HUV/PmTcMaV9ni5ee7OIogu0gL40F6bGnm2sagDbtluk9Tdsv3UTGtVlJchsbmGRKQauBO4GugGtorI/caY30Qu/Zkx5nXjvmCWcGTADtj+KuEzmmyAuK5qGkIQtQjGsmGB8EUdds7+yKD1g7v4gP/Gm14QVpqCMFjcu9suwkoF6a9dSuZylY1cqoNojEBRlJc9cVoE64FdxpjdxpgMcC9QJFn57KWn375pL5kf1hVY1Wh/bXVVowXvmRCF0kq4AieuGPZwMP0zZxF46QpqGvLzATkLo3d3/pvx6ZfDH34LTt9Quj11RYLFiqK87IlTCNoAv4pLd3AsyqtE5AkR+b6InBdje6ZEWGAmtAjaAyFIT9Qi2H6fTQXhUyitxLHg1+XqArhiJW5dwNKIEPgVvI69AN+/1aaU9usKiEDnxvKFMYoFixVFedkT56yhQnMno9NsHgNWGWP6ReQ64DvAuExWInITcBPAaafFU7y5GEf6x7uGmmrsrKGWhgnq6C8+bxOX+UVAssP5rh0ILQL3Ru8Cx8desEHcRi/nTU19vkXw2DdDy6FQgZlyzGuxBbiLVWdSFOVlS5wWQTfgZwxbCeRVRTHGnDDG9Af7DwA1IjIuybkx5m5jTJcxpqulZQLVuKbJc4f7GQsqjB0ZyLAgnaKuJlw4JkFuoOa6EusE+g6Gi70yA/n5gcC6cqKpno9323KALpunswh6d4fF1d0UyZr6MA4A+YupphI0raqGN34pzP6pKEpiiFMItgKdItIhIrXAFuB+/wIRaZVg1ZWIrA/acyTGNpXlxd5BNt72E370jE2wdqQ/w6L5kXTSbs6/n/o5yufOgs+vDa4fHF8rODuc79oBKwQNi0M/vYsROCGA0IUTtQj8dMTRmrqKoigliE0IjDFZ4GbgQeBp4D5jzA4RebeIuIoWNwBPicgTwBeALcZMtBp7POw9MoAxcOC4Xel7ZGCYxdG6Am4V8NgIJXGrfjOD4y2C7HD+Gz3YhWJ1jfkFzUdHrGsoJwTBuZqG/Bz7LvnczY9qnnhFUSZFrCuLA3fPA5Fjd3n7dwB3RO+rJAeO27f9Y4N2kD/Sn2FVcxrueYstwN6+3rMIJlBHwBgrAiOD8NCn4Yl/sxWxRjPWDeQzcNjm9nFz+TN9VgTMaCgETiSiFoFrS6Gyi4qiKCXQlcURDgZCcHwoEIKBDO3pIdj5gM3iCaFFUMo15BjN2IE8M2jz/R97AR6+09YWSEUsjbFsmPQNrEXQu8ful3MNgf1cKr+RoihKAZIjBIefhZ/fbou+lOCAJwRjY4ajAxla6oOZPa4ymLMIyrmGIEzfPDJg00KvuNjOFHrh4fEWAVgRqKm3M4qG+4OVwoRv+r5rKBpjKFZnWFEUpQTJEYJDO+B/P25n85TgYBAbOD40Qk//MNkxw+K6IGzh/Pw5i2ACQuDuGctaEWq9ICyPuORMODuyqDq9wL7VpxdYi6BvP1TVhHVznUWQqhs/8GthFkVRpkByso9WBQVkyrzF5yyCwRHu/uluRKBrRTDAupKS5SwCP97tzxYa6rU+/td/ATbfCal6u9DrxAG4LZjp49xCtQusBVGVCiyEwOWT9oPFEYtCLQJFUaZAciwCV2qxjF//4Ak7yD/fO8A3Ht7LH6xr5/SFwa9pnEUQfNdXroZHvhx+iR9E9gu6gI0BiNgcQm61r18I3gWKnUUwOpxfJrJhMRDcrxaBoigzQIIsgqCrJSyCocxobrbQSyfsiuJNF7TCyA57QbEYwcHt1uXj8IWg/1D+DymUyydVP/58er4Vgvqm/FjAJX8My86156PBZrUIFEWZAgmyCIJBs8SUT2cN+OsG2psbPJdQIAQ5iyBj3UDZk/bN/df/Ai9uzY8d9EdiEoVy+aTS5DJypD2LINNvv8sXgvpmOHNjcJ9aBIqiTJ8ECYFzDYWD9N6eAdZ96ofs6bEze9zU0bNaw7f2lc31niUQuIb8lcXu+7IZ+O574Ksb8zOLTsQiEAkH9bRXO3i4r/AKZIfGCBRFmQGSIwS5YHEYI3ii+xhHBjLsPGhz+hw8YQf8s1oXsIxeli2otTmGokFif2WxO+cXmvH3o7OUaoukeXZxglyMoNFOHx3NlBCC4B53Xi0CRVGmQHKEoDqIEXgWQfdRO6AfHbTuIjdjaG3zMD9Pv4/N86KxgahFMBK+/Wc9l5NbBAYTswggjBNEYwSjI+NjAbk+BcedFaEWgaIoUyA5QlBg+qgTAldz4ODxkyysr6GtdogaGWV1XZD0bVyMIPiM8cTBmyb6wsPhvqs65iiW799ZBO58qs5aFqOlXEPBPbnVxioEiqJMnuQIQYHpo91H7eB9dCC0CJYvrGNhyopFi/O05CyCyBZsQBfClNEQEYLJWgTu7T5tBSdbyjUUxAjcauOUuoYURZk8yZk+mhOC0IWzL+casgP/weMnWdZYx5JaO6i3zgtm8uQsgsH8zxCmivbTQB97IdyPzhqaaIzADfKZPlvMvhBrNsGJfTZv0f5fq0WgKMqUSI5FEHENjY0Zuo9ZIdix/zjnfex/eHLfcZYvrGNRra1FfE5LMBhHXUIjQ/kZQgFOekLgUkJDmEjOUcw1FI0ROLfPcF9xi6CpHTZ+IrxWLQJFUaZAAi0CKwQ9/cNksjaZ3DMHQ7dO68K63Jt/1VhgPfgWweiIzSZaO9+6hQpZBL4QgPXhu9KU0Smfjpo6e97N/HGD/3Bf/srign1zs4bUIlAUZfIkyCJwK4ttjODFwC3UWJevhbWpKs8CCGYEjXiuIDfIuzf7TCSgDPl1iMHmBaptKF0YPlUXJpxzn8FaGsXEw+GEQC0CRVGmQHKEIGIRuEDxBSsX5i65oG0h15zbmr9yGMLPAENH7TbnGhoo/7NrG2yt4WKBYvd9dWFbckIwNlLcNeRQi0BRlGmQHCGIxAjc1NHz2+zgu25VM//13ss4c+n8AjEB723fFaT3i8f4+EXkO64I7h+yYlAsUAxw+V/YjKQO3wooJwQptQgURZk6yRGCyPTR7qNDLJlfS1uTHTxXLfYG8NzsoAIWwcmIEDjXkKO+Odx/1XvstudZ6/svZRG0rIGODeHnyQiBWgSKokyDWIVARDaJyE4R2SUit5a47lIRGRWRG2JrTFW1DcYG7p7uo4O0NTfQ1GAH0dMXzwuvdRbBaIEYQc41FFwftQh8IWi9AM57I1z9Sei8Bs68auLtzROCCQaL1SJQFGUKxDZrSESqgTuBq4FuYKuI3G+M+U2B6z4DPBhXW3JU1eS5hs5d0ciiQAjyLYJIsDg7ZIPNY9nyMYK6pnC/Og1v/vrU2uqni5hosFgtAkVRpkCcFsF6YJcxZrcxJgPcC2wucN17gW8Bhwqcm1mqa8hkMtz50C729Aywsrmei09rYsul7WzobAmvc64hFyweORm+6ediBIEQjLMIfCEo8yZfsq21hfdLXasWgaIoUyBOIWgDXvQ+dwfHcohIG3A9cFeM7QipSvHcwaP83YM7AVjZ3MC8dIq/fdOFNHs1CMIso1620fpFdj9nERSLEXhCUO5NvhS+RVBOUFJqESiKMnXiFAIpcMxEPt8OfMgYM1ryi0RuEpFtIrLt8OHDU29RdQ0nBsPA7/LGIgNnNFjsWwQuWOxiBKVcQ1XTsAjyYgS6jkBRlPiIc2VxN9DufV4J7I9c0wXcK3YR1RLgOhHJGmO+419kjLkbuBugq6srKiYTp6qG432DrG6Zx0Urm1i/elHh66LB4uwQNEQsgtz00SKzhqpSYU3iqZBnEZRxDa1+Dax7JyzqmPrPUxQlscQpBFuBThHpAPYBW4C3+hcYY3Ijl4j8E/C9qAjMJGPVNQwMDXHtulY++Nqzi1+YCxb7FoETgkiMoNj00XJv8eWYzKyhhSvh9bdP7+cpipJYYhMCY0xWRG7GzgaqBr5mjNkhIu8Ozp+auIBHZqyKFFnWrmwqfaGfZXRszFoGOYug125rywSLpxMohnwhmE6sQVEUpQyxJp0zxjwAPBA5VlAAjDHviLMtAMOmmhSjdC4rsbAL8l1DLmBc3wwIDEaEwLcIpDoMIpdz55RjMq4hRVGUaZCclcVAJhCC1tED8MXfgb6XCl+Y9VxDThRqGmyA2FkEzjXk1UAmVRdmD53uW3xVily8fbrWhaIoSgmSJQRjVdRXjVHf8yQc2gGHny58oW8RnNhn9xuXWyFwmUVrC2QSTaXDXEPTHbxFvOL06hpSFCU+EiUEJ8eqqE+NhTN/ojN+HH6wuHe33V+0OpwyCvn77s09lfbqCczA4O2sCrUIFEWJkUQJwdBoNQ3VvhD0Fb7QDxY7IWjuCAf/6tp8149fZzgnBDMweOeEQGMEiqLER6KEYHBUqKvyhCA69RNsvYKxrF0MZkbhyC6Yv8zGBHL1hOvyF4vlisfXea6hGRi8nRDorCFFUWIkMUJgjGEwW5UvBIUsAmcNuGmgh562biHItwj8N/60N1PIpXmYicE7FyNQ15CiKPGRGCE4MZQlY6qorRoNF4UVFIJguqhLFXH4mfFCMM4iWBgen6lgMahrSFGUU0JihOBQ30lGqKZWyriGchZBc/jZpW7IuYbSNn2EBL8+P0ZQXWOnfs7E4O0CzjprSFGUGEmMELx0YpgsKVJkS7uGju612+ZV4bGmYN+3CACWrLFb5xpyx2saZihGoK4hRVHiJzFCcHQwQxa7oKykEOx71G7bXxEeW7DcbnNCEAzyXe+y297nguPBm3tNvQaLFUWZMyRGCF6/dgXXd3WQMmUsgn2PwuJOmL80PNa4wm6jFsHaLXa7cr11E7kBu7EtFI/poDECRVFOAbHmGpptVFXX2MHf5Q+KxgiMge5tcMbv5vvlcxaBFyMAO230L/dY19Bj3wwF4m3fnVmLQF1DiqLESKKEgOoaGPEKyUQtghP7YOAQtF2S746pDWYC5aaPeudcVtJUOl8gZgJNMaEoyikgWUJQ5XU33Tg+xUTPs3a77LxwRpBPzjVUYGC+9rOw7NyZaadDLQJFUU4ByRICf0BtOg2OPp9/3s8rdOLA+Pv9lcVR1t44M230qU7b9QpSqOqnoijKzJCYYDGQvwis6TQbIxgbC4/17rF1f+e3hjOD3GIxKG0RxEHr+dZNpSiKEiPJEgI/gLukEzBhzGBszFoEizrsYjHnRqr36hqfaiG45G3wrh+cmp+lKEpiSZhryOuuWyQ23A/9h+Cuy+wq4rNfZ49Ltd22rw/vKeUaUhRFmaMkSwica6i+OXT5DPfB878IU0u4dBIta+Ct90HHhvD+U20RKIqinAJidQ2JyCYR2Skiu0Tk1gLnN4vIdhF5XES2ichlcbYnFyye1xKmhRjuC1cTA6S9mMCa14b1BWD8gjJFUZSXAbFZBCJSDdwJXA10A1tF5H5jzG+8y34E3G+MMSJyIXAfcHZcbcr5/ee1hIvEju2FfY/B8ouslXDBm4rfX7sAzr8BTr88tiYqiqKcauJ0Da0HdhljdgOIyL3AZiAnBMYYfyL/PMDE2B7PIlgCS8+xb/Z7fmZrF2/4IFz5V6Xvr6qCG74aaxMVRVFONXG6htqAF73P3cGxPETkehF5Bvhv4E9ibA9kgjjAvBYrCsvXwvb7bEH6lZfG+qMVRVFmK3EKQaFVUOPe+I0x3zbGnA28AfhUwS8SuSmIIWw7fPjw1Fs02GO381rstm2dnT46vxU6rpj69yqKosxh4hSCbqDd+7wS2F/sYmPMT4EzRGRJgXN3G2O6jDFdLS0tU2/RyJDdLmi127Z1dtv1znABmaIoSsKIM0awFegUkQ5gH7AFeKt/gYicCTwXBIsvAWqBI7G1aMMHbA6htUEzOq+BV/4ZrL8pth+pKIoy24lNCIwxWRG5GXgQqAa+ZozZISLvDs7fBbwJeJuIjABDwI3GmPgCxvXNcI3nfaprhE2fju3HKYqizAUkznE3Drq6usy2bdsq3QxFUZQ5hYg8aozpKnQuWbmGFEVRlHGoECiKoiQcFQJFUZSEo0KgKIqScFQIFEVREo4KgaIoSsJRIVAURUk4c24dgYgcBp4ve2FhlgA9M9icSqJ9mZ1oX2Yn2hdYZYwpmKNnzgnBdBCRbcUWVMw1tC+zE+3L7ET7Uhp1DSmKoiQcFQJFUZSEkzQhuLvSDZhBtC+zE+3L7ET7UoJExQgURVGU8STNIlAURVEiJEYIRGSTiOwUkV0icmul2zNZRGSviDwpIo+LyLbg2CIR+aGI/DbYNle6nYUQka+JyCEReco7VrTtIvLh4DntFJHXVqbVhSnSl0+IyL7g2TwuItd552ZlX0SkXUQeEpGnRWSHiLwvOD7nnkuJvszF51InIo+IyBNBX/4mOB7vczHGvOz/YQvjPAesxlZBewI4t9LtmmQf9gJLIsc+C9wa7N8KfKbS7SzS9g3AJcBT5doOnBs8nzTQETy36kr3oUxfPgF8oMC1s7YvwHLgkmB/AfBs0N4591xK9GUuPhcB5gf7NcCvgFfG/VySYhGsB3YZY3YbYzLAvcDmCrdpJtgMfCPY/wbwhgq2pSjG1qPujRwu1vbNwL3GmGFjzB5gF/b5zQqK9KUYs7YvxpgDxpjHgv0+4GmgjTn4XEr0pRiPGNM3AAADtklEQVSzuS/GGNMffKwJ/hlifi5JEYI24EXvczel/6PMRgzwAxF5VERckeVlxpgDYP8YgKUVa93kKdb2ufqsbhaR7YHryJntc6IvInI6cDH27XNOP5dIX2AOPhcRqRaRx4FDwA+NMbE/l6QIgRQ4NtemS73aGHMJcC3wHhHZUOkGxcRcfFb/CJwBXAQcAD4XHJ/1fRGR+cC3gFuMMSdKXVrg2Gzvy5x8LsaYUWPMRcBKYL2InF/i8hnpS1KEoBto9z6vBPZXqC1TwhizP9geAr6NNf9eEpHlAMH2UOVaOGmKtX3OPStjzEvBH+8Y8GVC03xW90VEarAD578aY/4zODwnn0uhvszV5+IwxhwDfgxsIubnkhQh2Ap0ikiHiNQCW4D7K9ymCSMi80RkgdsHrgGewvbh7cFlbwe+W5kWTolibb8f2CIiaRHpADqBRyrQvgnj/kADrsc+G5jFfRERAb4KPG2Muc07NeeeS7G+zNHn0iIiTcF+PbAReIa4n0ulo+SnMBp/HXY2wXPARyrdnkm2fTV2ZsATwA7XfmAx8CPgt8F2UaXbWqT992BN8xHsG8y7SrUd+EjwnHYC11a6/RPoyz8DTwLbgz/M5bO9L8BlWBfCduDx4N91c/G5lOjLXHwuFwK/Dtr8FPCx4Hisz0VXFiuKoiScpLiGFEVRlCKoECiKoiQcFQJFUZSEo0KgKIqScFQIFEVREo4KgaJMABH5SJANcnuQyfIVInKLiDRUum2KMl10+qiilEFEXgXcBrzGGDMsIkuwWWx/CXQZY3oq2kBFmSZqEShKeZYDPcaYYYBg4L8BWAE8JCIPAYjINSLysIg8JiL/EeS+cbUkPhPkmX9ERM6sVEcUpRAqBIpSnh8A7SLyrIh8UUSuMMZ8AZvT5UpjzJWBlfBRYKOxyQG3Ae/3vuOEMWY9cAdw+6nugKKUIlXpBijKbMcY0y8i64DLgSuBf5fxVe5eiS0S8gub+oZa4GHv/D3e9u/jbbGiTA4VAkWZAMaYUWwmyB+LyJOECcAcgs0d/5ZiX1FkX1EqjrqGFKUMInKWiHR6hy4Cngf6sKURAf4PeLXz/4tIg4is8e650dv6loKiVBy1CBSlPPOBfwjSA2ex5QBvAt4CfF9EDgRxgncA94hIOrjvo9iMtwBpEfkV9uWrmNWgKBVBp48qSsyIyF50mqkyi1HXkKIoSsJRi0BRFCXhqEWgKIqScFQIFEVREo4KgaIoSsJRIVAURUk4KgSKoigJR4VAURQl4fw/PLhWXYFmfzwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trying optimal CNN\n",
    "\n",
    "from tensorflow.keras.constraints import min_max_norm\n",
    "from tensorflow.keras import layers, models, metrics, optimizers, utils\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "\n",
    "batch_size=100\n",
    "steps=300\n",
    "\n",
    "Y_change=list()\n",
    "\n",
    "#35:0, 36:1 37:2\n",
    "for i in Y:\n",
    "    if i==35:\n",
    "        Y_change.append(0)\n",
    "    elif i==36:\n",
    "        Y_change.append(1)\n",
    "    elif i==37:\n",
    "        Y_change.append(2)\n",
    "\n",
    "Y_change=np.array(Y_change)\n",
    "\n",
    "unique_elements, counts_elements = np.unique(Y_change, return_counts=True)\n",
    "nb_class=max(unique_elements)+1\n",
    "        \n",
    "Y_cat=utils.to_categorical(Y_change, num_classes=nb_class)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_cat, test_size=0.2, random_state=54)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                np.unique(Y_change),\n",
    "                                                 Y_change)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "print(class_weights)\n",
    "input_shape=((1520-744),2,1)\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=52, kernel_size=(6, 2), padding='valid',activation='tanh', kernel_constraint=min_max_norm(min_value=1.0, max_value=1.0),input_shape=input_shape))\n",
    "model.add(layers.MaxPooling2D((2, 1), strides=2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(units=162, activation='tanh'))\n",
    "model.add(layers.Dropout(0.8)\n",
    ")\n",
    "\n",
    "model.add(layers.Dense(nb_class, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate = 0.001),\n",
    "          loss=tf.keras.losses.categorical_crossentropy,\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "history=model.fit(X_train, y_train, batch_size=batch_size, epochs=steps, verbose=1, validation_split=0.2,\n",
    "                 class_weight=class_weights)\n",
    "\n",
    "i = np.arange(steps)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(i, history.history['accuracy'], label='accuracy')\n",
    "plt.plot(i, history.history['val_accuracy'], label='val_accuaracy')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions= model.predict(X_test)\n",
    "\n",
    "predictions = [np.argmax(y, axis=0, out=None) for y in predictions]\n",
    "y_test_cat = [np.argmax(y, axis=0, out=None) for y in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score for each class\n",
      "[0.47191011 0.5625     0.89719626]\n",
      "Global f1_score\n",
      "0.6583702273083623\n",
      "Global accuracy\n",
      "0.6575342465753424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2c05f9c190>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "present_class_list=np.unique(y_test_cat)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat=confusion_matrix(y_test_cat, predictions, labels=present_class_list)\n",
    "\n",
    "#F1-score\n",
    "print(\"f1_score for each class\")\n",
    "f1_each=f1_score(y_test_cat, predictions, average=None, labels=present_class_list)\n",
    "print(f1_each)\n",
    "\n",
    "#F1-score\n",
    "print(\"Global f1_score\")\n",
    "print(f1_score(y_test_cat, predictions, average='weighted'))\n",
    "\n",
    "#accyracy\n",
    "print(\"Global accuracy\")\n",
    "print(accuracy_score(y_test_cat,predictions))\n",
    "\n",
    "import seaborn as sns\n",
    "cmap = sns.diverging_palette(200, 10, as_cmap=True)\n",
    "sns.heatmap(conf_mat, cmap=cmap, xticklabels=present_class_list, yticklabels=present_class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, optimizers\n",
    "from kerastuner.tuners import RandomSearch, Hyperband\n",
    "\n",
    "input_shape=((1520-744),2,1)\n",
    "\n",
    "def build_model(h):\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(filters=h.Int('filters_1', 2, 200,10, default=32), kernel_size=(6, 2), padding='valid',activation=h.Choice(\n",
    "                'activation_1',\n",
    "                values=['relu', 'tanh', 'sigmoid'],\n",
    "                default='relu'), kernel_constraint=min_max_norm(min_value=1.0, max_value=1.0),input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 1), strides=2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(units=h.Int('units_dense', 2, 200,10, default=100), activation=h.Choice(\n",
    "                'activation_2',\n",
    "                values=['relu', 'tanh', 'sigmoid'],\n",
    "                default='relu')))\n",
    "    model.add(layers.Dropout(\n",
    "            h.Float(\n",
    "                'dropout',\n",
    "                min_value=0.5,\n",
    "                max_value=0.9,\n",
    "                default=0.7,\n",
    "                step=0.1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(layers.Dense(nb_class, activation='softmax'))\n",
    "\n",
    "    hp_learning_rate = h.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4])\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "{0: 1.0, 1: 1.0, 2: 1.0}\n"
     ]
    }
   ],
   "source": [
    "#trying first simple CNN to test if our data are ok\n",
    "#using Thibaud network\n",
    "from tensorflow.keras.constraints import min_max_norm\n",
    "from tensorflow.keras import layers, models, metrics, optimizers, utils\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "batch_size=100\n",
    "steps=300\n",
    "\n",
    "Y_change=list()\n",
    "\n",
    "#36:0, 37:1\n",
    "for i in Y:\n",
    "    if i==35:\n",
    "        Y_change.append(0)\n",
    "    elif i==36:\n",
    "        Y_change.append(1)\n",
    "    elif i==37:\n",
    "        Y_change.append(2)\n",
    "\n",
    "Y_change=np.array(Y_change)\n",
    "\n",
    "unique_elements, counts_elements = np.unique(Y_change, return_counts=True)\n",
    "nb_class=max(unique_elements)+1\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "Y_cat=utils.to_categorical(Y_change, num_classes=nb_class)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_cat, test_size=0.2, random_state=54)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                np.unique(Y_change),\n",
    "                                                 Y_change)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 52s]\n",
      "val_accuracy: 0.6422413885593414\n",
      "\n",
      "Best val_accuracy So Far: 0.6637930870056152\n",
      "Total elapsed time: 00h 09m 31s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in ./untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 12\n",
      "activation_1: tanh\n",
      "units_dense: 172\n",
      "activation_2: tanh\n",
      "dropout: 0.6\n",
      "learning_rate: 0.001\n",
      "Score: 0.6637930870056152\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 62\n",
      "activation_1: tanh\n",
      "units_dense: 182\n",
      "activation_2: tanh\n",
      "dropout: 0.7\n",
      "learning_rate: 0.001\n",
      "Score: 0.6594827473163605\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 72\n",
      "activation_1: relu\n",
      "units_dense: 72\n",
      "activation_2: tanh\n",
      "dropout: 0.6\n",
      "learning_rate: 0.0001\n",
      "Score: 0.6422413885593414\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 132\n",
      "activation_1: relu\n",
      "units_dense: 82\n",
      "activation_2: tanh\n",
      "dropout: 0.8999999999999999\n",
      "learning_rate: 0.001\n",
      "Score: 0.625\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 42\n",
      "activation_1: relu\n",
      "units_dense: 82\n",
      "activation_2: tanh\n",
      "dropout: 0.7\n",
      "learning_rate: 0.0001\n",
      "Score: 0.625\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 162\n",
      "activation_1: relu\n",
      "units_dense: 52\n",
      "activation_2: sigmoid\n",
      "dropout: 0.6\n",
      "learning_rate: 0.001\n",
      "Score: 0.599137932062149\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 112\n",
      "activation_1: relu\n",
      "units_dense: 102\n",
      "activation_2: relu\n",
      "dropout: 0.8999999999999999\n",
      "learning_rate: 0.0001\n",
      "Score: 0.5775862038135529\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 182\n",
      "activation_1: relu\n",
      "units_dense: 192\n",
      "activation_2: relu\n",
      "dropout: 0.7\n",
      "learning_rate: 0.01\n",
      "Score: 0.3965517282485962\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 12\n",
      "activation_1: relu\n",
      "units_dense: 152\n",
      "activation_2: sigmoid\n",
      "dropout: 0.8999999999999999\n",
      "learning_rate: 0.001\n",
      "Score: 0.3965517282485962\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filters_1: 142\n",
      "activation_1: relu\n",
      "units_dense: 162\n",
      "activation_2: relu\n",
      "dropout: 0.7\n",
      "learning_rate: 0.01\n",
      "Score: 0.3965517282485962\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "tuner_rs = RandomSearch(\n",
    "            build_model,\n",
    "    objective='val_accuracy',\n",
    "            seed=42,\n",
    "            max_trials=10,\n",
    "            executions_per_trial=2,\n",
    "            overwrite=True)\n",
    "\n",
    "tuner_rs.search_space_summary()\n",
    "\n",
    "\n",
    "\n",
    "tuner_rs.search(X_train, y_train, epochs=300, validation_split=0.2, verbose=1, class_weight=class_weights)\n",
    "\n",
    "tuner_rs.results_summary()\n",
    "\n",
    "best_model = tuner_rs.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
